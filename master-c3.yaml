apiVersion: v1
items:
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2025-02-27T05:43:04Z"
    generateName: fleet-agent-
    labels:
      app: fleet-agent
      apps.kubernetes.io/pod-index: "0"
      controller-revision-hash: fleet-agent-6f4bc75f47
      statefulset.kubernetes.io/pod-name: fleet-agent-0
    name: fleet-agent-0
    namespace: cattle-fleet-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: StatefulSet
      name: fleet-agent
      uid: 4fb5e7d2-ecb6-4fac-b4b7-f6f571257863
    resourceVersion: "23056"
    uid: 1dedcac9-99d0-4b09-9b0d-510db5307a3d
  spec:
    affinity:
      nodeAffinity:
        preferredDuringSchedulingIgnoredDuringExecution:
        - preference:
            matchExpressions:
            - key: fleet.cattle.io/agent
              operator: In
              values:
              - "true"
          weight: 1
    containers:
    - command:
      - fleetagent
      env:
      - name: BUNDLEDEPLOYMENT_RECONCILER_WORKERS
        value: "50"
      - name: DRIFT_RECONCILER_WORKERS
        value: "50"
      - name: NAMESPACE
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.namespace
      - name: AGENT_SCOPE
      image: rancher/fleet-agent:v0.11.3
      imagePullPolicy: IfNotPresent
      name: fleet-agent
      resources: {}
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
        privileged: false
        readOnlyRootFilesystem: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /.kube
        name: kube
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-gknhw
        readOnly: true
    - command:
      - fleetagent
      - clusterstatus
      env:
      - name: NAMESPACE
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.namespace
      - name: CHECKIN_INTERVAL
        value: 15m0s
      image: rancher/fleet-agent:v0.11.3
      imagePullPolicy: IfNotPresent
      name: fleet-agent-clusterstatus
      resources: {}
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
        privileged: false
        readOnlyRootFilesystem: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-gknhw
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostname: fleet-agent-0
    initContainers:
    - command:
      - fleetagent
      - register
      env:
      - name: NAMESPACE
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.namespace
      image: rancher/fleet-agent:v0.11.3
      imagePullPolicy: IfNotPresent
      name: fleet-agent-register
      resources: {}
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
        privileged: false
        readOnlyRootFilesystem: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-gknhw
        readOnly: true
    nodeName: ip-172-31-91-35
    nodeSelector:
      kubernetes.io/os: linux
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      runAsGroup: 1000
      runAsNonRoot: true
      runAsUser: 1000
    serviceAccount: fleet-agent
    serviceAccountName: fleet-agent
    subdomain: fleet-agent
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoSchedule
      key: node.cloudprovider.kubernetes.io/uninitialized
      operator: Equal
      value: "true"
    - effect: NoSchedule
      key: cattle.io/os
      operator: Equal
      value: linux
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - emptyDir: {}
      name: kube
    - name: kube-api-access-gknhw
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-03-03T02:55:06Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-02-27T05:43:10Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2025-03-03T02:57:16Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2025-03-03T02:57:16Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-02-27T05:43:04Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://b21e5b83eb1646828e48077d5e8e5058857c0e08fe2badcef3fc4246c353b859
      image: docker.io/rancher/fleet-agent:v0.11.3
      imageID: docker.io/rancher/fleet-agent@sha256:2a229b1be5f095e5c56e06c51318cf6d0aeed19204fb3c97e15052bc92591d23
      lastState:
        terminated:
          containerID: containerd://0752be2f0133634bf1c7d1765071ae3f7cf6785823199e63f247b77616e7b4c9
          exitCode: 255
          finishedAt: "2025-03-03T02:54:52Z"
          reason: Unknown
          startedAt: "2025-03-02T07:32:36Z"
      name: fleet-agent
      ready: true
      restartCount: 5
      started: true
      state:
        running:
          startedAt: "2025-03-03T02:57:15Z"
      volumeMounts:
      - mountPath: /.kube
        name: kube
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-gknhw
        readOnly: true
        recursiveReadOnly: Disabled
    - containerID: containerd://f26dcc9e7ff98522c8ac518c517f125dd9ae8b848c5aa58c2ad3854a661a3033
      image: docker.io/rancher/fleet-agent:v0.11.3
      imageID: docker.io/rancher/fleet-agent@sha256:2a229b1be5f095e5c56e06c51318cf6d0aeed19204fb3c97e15052bc92591d23
      lastState:
        terminated:
          containerID: containerd://d237bb3230a38f398c11a6bba5f76ad1bf55eb676c4afc28774098540f344097
          exitCode: 255
          finishedAt: "2025-03-03T02:54:52Z"
          reason: Unknown
          startedAt: "2025-03-02T07:32:36Z"
      name: fleet-agent-clusterstatus
      ready: true
      restartCount: 5
      started: true
      state:
        running:
          startedAt: "2025-03-03T02:57:16Z"
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-gknhw
        readOnly: true
        recursiveReadOnly: Disabled
    hostIP: 172.31.91.35
    hostIPs:
    - ip: 172.31.91.35
    initContainerStatuses:
    - containerID: containerd://4c2195eea950d38903dddcb560ad55d32258879f822c10906218e67093edbba9
      image: docker.io/rancher/fleet-agent:v0.11.3
      imageID: docker.io/rancher/fleet-agent@sha256:2a229b1be5f095e5c56e06c51318cf6d0aeed19204fb3c97e15052bc92591d23
      lastState: {}
      name: fleet-agent-register
      ready: true
      restartCount: 6
      started: false
      state:
        terminated:
          containerID: containerd://4c2195eea950d38903dddcb560ad55d32258879f822c10906218e67093edbba9
          exitCode: 0
          finishedAt: "2025-03-03T02:57:15Z"
          reason: Completed
          startedAt: "2025-03-03T02:55:05Z"
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-gknhw
        readOnly: true
        recursiveReadOnly: Disabled
    phase: Running
    podIP: 10.42.0.115
    podIPs:
    - ip: 10.42.0.115
    qosClass: BestEffort
    startTime: "2025-02-27T05:43:04Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2025-02-27T05:42:55Z"
    generateName: cattle-cluster-agent-74975b7654-
    labels:
      app: cattle-cluster-agent
      pod-template-hash: 74975b7654
    name: cattle-cluster-agent-74975b7654-x8wtg
    namespace: cattle-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: cattle-cluster-agent-74975b7654
      uid: 4638bd7a-1754-431b-ade5-1e7ad697e3b6
    resourceVersion: "23014"
    uid: 0bcbaf21-0992-441d-b7b6-97b4f6d35408
  spec:
    affinity:
      nodeAffinity:
        preferredDuringSchedulingIgnoredDuringExecution:
        - preference:
            matchExpressions:
            - key: node-role.kubernetes.io/controlplane
              operator: In
              values:
              - "true"
          weight: 100
        - preference:
            matchExpressions:
            - key: node-role.kubernetes.io/control-plane
              operator: In
              values:
              - "true"
          weight: 100
        - preference:
            matchExpressions:
            - key: node-role.kubernetes.io/master
              operator: In
              values:
              - "true"
          weight: 100
        - preference:
            matchExpressions:
            - key: cattle.io/cluster-agent
              operator: In
              values:
              - "true"
          weight: 1
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchExpressions:
            - key: beta.kubernetes.io/os
              operator: NotIn
              values:
              - windows
      podAntiAffinity:
        preferredDuringSchedulingIgnoredDuringExecution:
        - podAffinityTerm:
            labelSelector:
              matchExpressions:
              - key: app
                operator: In
                values:
                - cattle-cluster-agent
            topologyKey: kubernetes.io/hostname
          weight: 100
    containers:
    - env:
      - name: CATTLE_FEATURES
        value: embedded-cluster-api=false,fleet=false,managed-system-upgrade-controller=true,multi-cluster-management=false,multi-cluster-management-agent=true,provisioningprebootstrap=false,provisioningv2=false,rke2=false,ui-sql-cache=false
      - name: CATTLE_IS_RKE
        value: "false"
      - name: CATTLE_SERVER
        value: https://44.220.0.65.sslip.io
      - name: CATTLE_CA_CHECKSUM
      - name: CATTLE_CLUSTER
        value: "true"
      - name: CATTLE_K8S_MANAGED
        value: "true"
      - name: CATTLE_CLUSTER_REGISTRY
      - name: CATTLE_SERVER_VERSION
        value: v2.10.2
      - name: CATTLE_INSTALL_UUID
        value: dd7c2187-12b9-4774-8ac3-7ac6e676ed0b
      - name: CATTLE_INGRESS_IP_DOMAIN
        value: sslip.io
      - name: STRICT_VERIFY
        value: "false"
      image: rancher/rancher-agent:v2.10.2
      imagePullPolicy: IfNotPresent
      name: cluster-register
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /cattle-credentials
        name: cattle-credentials
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-6zqwd
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: ip-172-31-91-35
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: cattle
    serviceAccountName: cattle
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoSchedule
      key: node-role.kubernetes.io/controlplane
      value: "true"
    - effect: NoSchedule
      key: node-role.kubernetes.io/control-plane
      operator: Exists
    - effect: NoSchedule
      key: node-role.kubernetes.io/master
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: cattle-credentials
      secret:
        defaultMode: 320
        secretName: cattle-credentials-7494863
    - name: kube-api-access-6zqwd
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-03-03T02:55:05Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-02-27T05:42:55Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2025-03-03T02:56:44Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2025-03-03T02:56:44Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-02-27T05:42:55Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://dc24c396a385fa7996fd3f2c92ca40c16155308752a013a176f2fab2ccc69545
      image: docker.io/rancher/rancher-agent:v2.10.2
      imageID: docker.io/rancher/rancher-agent@sha256:0678aacdd192768b9112272f00332fa7303ef497b058b46a0f4e1cbbdee47350
      lastState:
        terminated:
          containerID: containerd://cc163c2a334291149fcd8b60090ca228855de0dba36fc72c7edd8c4afd2989c4
          exitCode: 1
          finishedAt: "2025-03-03T02:55:55Z"
          reason: Error
          startedAt: "2025-03-03T02:55:55Z"
      name: cluster-register
      ready: true
      restartCount: 23
      started: true
      state:
        running:
          startedAt: "2025-03-03T02:56:44Z"
      volumeMounts:
      - mountPath: /cattle-credentials
        name: cattle-credentials
        readOnly: true
        recursiveReadOnly: Disabled
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-6zqwd
        readOnly: true
        recursiveReadOnly: Disabled
    hostIP: 172.31.91.35
    hostIPs:
    - ip: 172.31.91.35
    phase: Running
    podIP: 10.42.0.102
    podIPs:
    - ip: 10.42.0.102
    qosClass: BestEffort
    startTime: "2025-02-27T05:42:55Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2025-02-27T05:44:06Z"
    generateName: rancher-webhook-586f888bb-
    labels:
      app: rancher-webhook
      pod-template-hash: 586f888bb
    name: rancher-webhook-586f888bb-f5x9n
    namespace: cattle-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: rancher-webhook-586f888bb
      uid: f92df8b9-d137-4275-bed8-b3b0e52d9e4a
    resourceVersion: "22942"
    uid: e41996e8-c78b-461b-b7fe-23bb4c68baab
  spec:
    containers:
    - env:
      - name: STAMP
      - name: ENABLE_MCM
        value: "false"
      - name: CATTLE_PORT
        value: "9443"
      - name: NAMESPACE
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.namespace
      image: rancher/rancher-webhook:v0.6.3
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 3
        httpGet:
          path: /healthz
          port: https
          scheme: HTTPS
        periodSeconds: 5
        successThreshold: 1
        timeoutSeconds: 1
      name: rancher-webhook
      ports:
      - containerPort: 9443
        name: https
        protocol: TCP
      resources: {}
      startupProbe:
        failureThreshold: 60
        httpGet:
          path: /healthz
          port: https
          scheme: HTTPS
        periodSeconds: 5
        successThreshold: 1
        timeoutSeconds: 1
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-26p46
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: ip-172-31-91-35
    nodeSelector:
      kubernetes.io/os: linux
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: rancher-webhook
    serviceAccountName: rancher-webhook
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoSchedule
      key: cattle.io/os
      operator: Equal
      value: linux
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: kube-api-access-26p46
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-03-03T02:55:06Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-02-27T05:44:06Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2025-03-03T02:55:19Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2025-03-03T02:55:19Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-02-27T05:44:06Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://b93e85fbfbe8058aa600b902f6d2d39589e7697c3bd9e6b2ba75059bc51ee8df
      image: docker.io/rancher/rancher-webhook:v0.6.3
      imageID: docker.io/rancher/rancher-webhook@sha256:79a21351e4536a3e609b41bf8beade3833d100a509a74e868cef4c6a826b50b3
      lastState:
        terminated:
          containerID: containerd://08a6b01999e7f9b4057e4fa55ae524a8d9edbb7d26f3cdde87751d4d2cc43168
          exitCode: 255
          finishedAt: "2025-03-03T02:54:52Z"
          reason: Unknown
          startedAt: "2025-03-02T07:30:25Z"
      name: rancher-webhook
      ready: true
      restartCount: 6
      started: true
      state:
        running:
          startedAt: "2025-03-03T02:55:00Z"
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-26p46
        readOnly: true
        recursiveReadOnly: Disabled
    hostIP: 172.31.91.35
    hostIPs:
    - ip: 172.31.91.35
    phase: Running
    podIP: 10.42.0.100
    podIPs:
    - ip: 10.42.0.100
    qosClass: BestEffort
    startTime: "2025-02-27T05:44:06Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2025-02-27T05:44:38Z"
    generateName: system-upgrade-controller-5fb67f585d-
    labels:
      pod-template-hash: 5fb67f585d
      upgrade.cattle.io/controller: system-upgrade-controller
    name: system-upgrade-controller-5fb67f585d-8xxtn
    namespace: cattle-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: system-upgrade-controller-5fb67f585d
      uid: d2e481c1-836a-4254-b48d-44782e294944
    resourceVersion: "22881"
    uid: 58beb554-e302-4a9c-bfec-a192c53fb799
  spec:
    affinity:
      nodeAffinity:
        preferredDuringSchedulingIgnoredDuringExecution:
        - preference:
            matchExpressions:
            - key: node-role.kubernetes.io/control-plane
              operator: In
              values:
              - "true"
          weight: 100
        - preference:
            matchExpressions:
            - key: node-role.kubernetes.io/master
              operator: In
              values:
              - "true"
          weight: 100
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchExpressions:
            - key: kubernetes.io/os
              operator: NotIn
              values:
              - windows
    containers:
    - env:
      - name: SYSTEM_UPGRADE_CONTROLLER_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.labels['upgrade.cattle.io/controller']
      - name: SYSTEM_UPGRADE_CONTROLLER_NAMESPACE
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.namespace
      envFrom:
      - configMapRef:
          name: system-upgrade-controller-config
      image: rancher/system-upgrade-controller:v0.14.2
      imagePullPolicy: IfNotPresent
      name: system-upgrade-controller
      resources: {}
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
        runAsGroup: 65534
        runAsNonRoot: true
        runAsUser: 65534
        seccompProfile:
          type: RuntimeDefault
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/ssl
        name: etc-ssl
        readOnly: true
      - mountPath: /etc/pki
        name: etc-pki
        readOnly: true
      - mountPath: /etc/ca-certificates
        name: etc-ca-certificates
        readOnly: true
      - mountPath: /tmp
        name: tmp
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-6hzqr
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: ip-172-31-91-35
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: system-upgrade-controller
    serviceAccountName: system-upgrade-controller
    terminationGracePeriodSeconds: 30
    tolerations:
    - operator: Exists
    volumes:
    - hostPath:
        path: /etc/ssl
        type: DirectoryOrCreate
      name: etc-ssl
    - hostPath:
        path: /etc/pki
        type: DirectoryOrCreate
      name: etc-pki
    - hostPath:
        path: /etc/ca-certificates
        type: DirectoryOrCreate
      name: etc-ca-certificates
    - emptyDir: {}
      name: tmp
    - name: kube-api-access-6hzqr
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-03-03T02:55:09Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-02-27T05:44:38Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2025-03-03T02:55:09Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2025-03-03T02:55:09Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-02-27T05:44:38Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://31edda4b52b53a6ed51ab425084b4ec3ab1f3074dd569bcafa49016437290c1e
      image: docker.io/rancher/system-upgrade-controller:v0.14.2
      imageID: docker.io/rancher/system-upgrade-controller@sha256:3cdbfdd90f814702cefb832fc4bdb09ea93865a4d06c6bafd019d1dc6a9f34c9
      lastState:
        terminated:
          containerID: containerd://4bf287e931eaa5b0870262b3ae4dae36a6ca8f7f13c36dfdbd82ce954d304a2e
          exitCode: 255
          finishedAt: "2025-03-03T02:54:52Z"
          reason: Unknown
          startedAt: "2025-03-02T07:30:28Z"
      name: system-upgrade-controller
      ready: true
      restartCount: 6
      started: true
      state:
        running:
          startedAt: "2025-03-03T02:55:05Z"
      volumeMounts:
      - mountPath: /etc/ssl
        name: etc-ssl
        readOnly: true
        recursiveReadOnly: Disabled
      - mountPath: /etc/pki
        name: etc-pki
        readOnly: true
        recursiveReadOnly: Disabled
      - mountPath: /etc/ca-certificates
        name: etc-ca-certificates
        readOnly: true
        recursiveReadOnly: Disabled
      - mountPath: /tmp
        name: tmp
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-6hzqr
        readOnly: true
        recursiveReadOnly: Disabled
    hostIP: 172.31.91.35
    hostIPs:
    - ip: 172.31.91.35
    phase: Running
    podIP: 10.42.0.114
    podIPs:
    - ip: 10.42.0.114
    qosClass: BestEffort
    startTime: "2025-02-27T05:44:38Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2025-02-27T05:22:14Z"
    generateName: coredns-ccb96694c-
    labels:
      k8s-app: kube-dns
      pod-template-hash: ccb96694c
    name: coredns-ccb96694c-lxb9j
    namespace: kube-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: coredns-ccb96694c
      uid: 02df4866-c869-4ddb-b6e8-91c068b96fc9
    resourceVersion: "22753"
    uid: 20c98a8c-fe4b-482a-b95b-8284c49934fb
  spec:
    containers:
    - args:
      - -conf
      - /etc/coredns/Corefile
      image: rancher/mirrored-coredns-coredns:1.12.0
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 3
        httpGet:
          path: /health
          port: 8080
          scheme: HTTP
        initialDelaySeconds: 60
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      name: coredns
      ports:
      - containerPort: 53
        name: dns
        protocol: UDP
      - containerPort: 53
        name: dns-tcp
        protocol: TCP
      - containerPort: 9153
        name: metrics
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /ready
          port: 8181
          scheme: HTTP
        periodSeconds: 2
        successThreshold: 1
        timeoutSeconds: 1
      resources:
        limits:
          memory: 170Mi
        requests:
          cpu: 100m
          memory: 70Mi
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          add:
          - NET_BIND_SERVICE
          drop:
          - all
        readOnlyRootFilesystem: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/coredns
        name: config-volume
        readOnly: true
      - mountPath: /etc/coredns/custom
        name: custom-config-volume
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-gslxz
        readOnly: true
    dnsPolicy: Default
    enableServiceLinks: true
    nodeName: ip-172-31-91-35
    nodeSelector:
      kubernetes.io/os: linux
    preemptionPolicy: PreemptLowerPriority
    priority: 2000000000
    priorityClassName: system-cluster-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: coredns
    serviceAccountName: coredns
    terminationGracePeriodSeconds: 30
    tolerations:
    - key: CriticalAddonsOnly
      operator: Exists
    - effect: NoSchedule
      key: node-role.kubernetes.io/control-plane
      operator: Exists
    - effect: NoSchedule
      key: node-role.kubernetes.io/master
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    topologySpreadConstraints:
    - labelSelector:
        matchLabels:
          k8s-app: kube-dns
      maxSkew: 1
      topologyKey: kubernetes.io/hostname
      whenUnsatisfiable: DoNotSchedule
    volumes:
    - configMap:
        defaultMode: 420
        items:
        - key: Corefile
          path: Corefile
        - key: NodeHosts
          path: NodeHosts
        name: coredns
      name: config-volume
    - configMap:
        defaultMode: 420
        name: coredns-custom
        optional: true
      name: custom-config-volume
    - name: kube-api-access-gslxz
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-03-03T02:55:05Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-02-27T05:22:15Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2025-03-03T02:55:05Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2025-03-03T02:55:05Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-02-27T05:22:15Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://acf97b24abcd9d8febf99604bce216c759ad2f0a606400ed9f09ac97f1091d07
      image: docker.io/rancher/mirrored-coredns-coredns:1.12.0
      imageID: docker.io/rancher/mirrored-coredns-coredns@sha256:82979ddf442c593027a57239ad90616deb874e90c365d1a96ad508c2104bdea5
      lastState:
        terminated:
          containerID: containerd://ef09543b82d1ca922a4b22eab05088d0a3dc01806d4da2548ab42e8dfb49faff
          exitCode: 255
          finishedAt: "2025-03-03T02:54:52Z"
          reason: Unknown
          startedAt: "2025-03-02T07:30:25Z"
      name: coredns
      ready: true
      restartCount: 9
      started: true
      state:
        running:
          startedAt: "2025-03-03T02:55:01Z"
      volumeMounts:
      - mountPath: /etc/coredns
        name: config-volume
        readOnly: true
        recursiveReadOnly: Disabled
      - mountPath: /etc/coredns/custom
        name: custom-config-volume
        readOnly: true
        recursiveReadOnly: Disabled
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-gslxz
        readOnly: true
        recursiveReadOnly: Disabled
    hostIP: 172.31.91.35
    hostIPs:
    - ip: 172.31.91.35
    phase: Running
    podIP: 10.42.0.107
    podIPs:
    - ip: 10.42.0.107
    qosClass: Burstable
    startTime: "2025-02-27T05:22:15Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      helmcharts.helm.cattle.io/configHash: SHA256=E3B0C44298FC1C149AFBF4C8996FB92427AE41E4649B934CA495991B7852B855
    creationTimestamp: "2025-02-27T05:22:15Z"
    generateName: helm-install-traefik-crd-
    labels:
      batch.kubernetes.io/controller-uid: 05b08b97-af47-4ad4-b681-eb5d61c522a9
      batch.kubernetes.io/job-name: helm-install-traefik-crd
      controller-uid: 05b08b97-af47-4ad4-b681-eb5d61c522a9
      helmcharts.helm.cattle.io/chart: traefik-crd
      job-name: helm-install-traefik-crd
    name: helm-install-traefik-crd-k7h5k
    namespace: kube-system
    ownerReferences:
    - apiVersion: batch/v1
      blockOwnerDeletion: true
      controller: true
      kind: Job
      name: helm-install-traefik-crd
      uid: 05b08b97-af47-4ad4-b681-eb5d61c522a9
    resourceVersion: "1094"
    uid: c6df6e2d-6295-4606-a387-62aae0695c21
  spec:
    containers:
    - args:
      - install
      env:
      - name: NAME
        value: traefik-crd
      - name: VERSION
      - name: REPO
      - name: HELM_DRIVER
        value: secret
      - name: CHART_NAMESPACE
        value: kube-system
      - name: CHART
        value: https://%{KUBERNETES_API}%/static/charts/traefik-crd-27.0.201+up27.0.2.tgz
      - name: HELM_VERSION
      - name: TARGET_NAMESPACE
        value: kube-system
      - name: AUTH_PASS_CREDENTIALS
        value: "false"
      - name: INSECURE_SKIP_TLS_VERIFY
        value: "false"
      - name: PLAIN_HTTP
        value: "false"
      - name: NO_PROXY
        value: .svc,.cluster.local,10.42.0.0/16,10.43.0.0/16
      - name: FAILURE_POLICY
        value: reinstall
      image: rancher/klipper-helm:v0.9.3-build20241008
      imagePullPolicy: IfNotPresent
      name: helm
      resources: {}
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
        readOnlyRootFilesystem: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /home/klipper-helm/.helm
        name: klipper-helm
      - mountPath: /home/klipper-helm/.cache
        name: klipper-cache
      - mountPath: /home/klipper-helm/.config
        name: klipper-config
      - mountPath: /tmp
        name: tmp
      - mountPath: /config
        name: values
      - mountPath: /chart
        name: content
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-x8mpl
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: ip-172-31-91-35
    nodeSelector:
      kubernetes.io/os: linux
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: OnFailure
    schedulerName: default-scheduler
    securityContext:
      runAsNonRoot: true
      seccompProfile:
        type: RuntimeDefault
    serviceAccount: helm-traefik-crd
    serviceAccountName: helm-traefik-crd
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - emptyDir:
        medium: Memory
      name: klipper-helm
    - emptyDir:
        medium: Memory
      name: klipper-cache
    - emptyDir:
        medium: Memory
      name: klipper-config
    - emptyDir:
        medium: Memory
      name: tmp
    - name: values
      secret:
        defaultMode: 420
        secretName: chart-values-traefik-crd
    - configMap:
        defaultMode: 420
        name: chart-content-traefik-crd
      name: content
    - name: kube-api-access-x8mpl
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-02-27T05:22:41Z"
      status: "False"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-02-27T05:22:15Z"
      reason: PodCompleted
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2025-02-27T05:22:40Z"
      reason: PodCompleted
      status: "False"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2025-02-27T05:22:40Z"
      reason: PodCompleted
      status: "False"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-02-27T05:22:15Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://71b56024191393cd63407679da8a05d9542e8cb272050efb941c78b46567fa3b
      image: docker.io/rancher/klipper-helm:v0.9.3-build20241008
      imageID: docker.io/rancher/klipper-helm@sha256:73ff7ef399717ba8339559054557bd427bdafb47db112165a8c0c358d1ca0283
      lastState: {}
      name: helm
      ready: false
      restartCount: 0
      started: false
      state:
        terminated:
          containerID: containerd://71b56024191393cd63407679da8a05d9542e8cb272050efb941c78b46567fa3b
          exitCode: 0
          finishedAt: "2025-02-27T05:22:39Z"
          message: |
            Installing helm chart
          reason: Completed
          startedAt: "2025-02-27T05:22:33Z"
      volumeMounts:
      - mountPath: /home/klipper-helm/.helm
        name: klipper-helm
      - mountPath: /home/klipper-helm/.cache
        name: klipper-cache
      - mountPath: /home/klipper-helm/.config
        name: klipper-config
      - mountPath: /tmp
        name: tmp
      - mountPath: /config
        name: values
      - mountPath: /chart
        name: content
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-x8mpl
        readOnly: true
        recursiveReadOnly: Disabled
    hostIP: 172.31.91.35
    hostIPs:
    - ip: 172.31.91.35
    phase: Succeeded
    qosClass: BestEffort
    startTime: "2025-02-27T05:22:15Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      helmcharts.helm.cattle.io/configHash: SHA256=EF658189C81681410C1B7E28CB9F0030170938B99D1CD7EC9E40B82278BB67A8
    creationTimestamp: "2025-02-27T05:22:15Z"
    generateName: helm-install-traefik-
    labels:
      batch.kubernetes.io/controller-uid: c1641b53-c755-4d8e-a64e-b7393e622d86
      batch.kubernetes.io/job-name: helm-install-traefik
      controller-uid: c1641b53-c755-4d8e-a64e-b7393e622d86
      helmcharts.helm.cattle.io/chart: traefik
      job-name: helm-install-traefik
    name: helm-install-traefik-j765t
    namespace: kube-system
    ownerReferences:
    - apiVersion: batch/v1
      blockOwnerDeletion: true
      controller: true
      kind: Job
      name: helm-install-traefik
      uid: c1641b53-c755-4d8e-a64e-b7393e622d86
    resourceVersion: "1109"
    uid: 9d2408ed-fcc7-43ca-9401-3a656a6663c3
  spec:
    containers:
    - args:
      - install
      - --set-string
      - global.systemDefaultRegistry=
      env:
      - name: NAME
        value: traefik
      - name: VERSION
      - name: REPO
      - name: HELM_DRIVER
        value: secret
      - name: CHART_NAMESPACE
        value: kube-system
      - name: CHART
        value: https://%{KUBERNETES_API}%/static/charts/traefik-27.0.201+up27.0.2.tgz
      - name: HELM_VERSION
      - name: TARGET_NAMESPACE
        value: kube-system
      - name: AUTH_PASS_CREDENTIALS
        value: "false"
      - name: INSECURE_SKIP_TLS_VERIFY
        value: "false"
      - name: PLAIN_HTTP
        value: "false"
      - name: NO_PROXY
        value: .svc,.cluster.local,10.42.0.0/16,10.43.0.0/16
      - name: FAILURE_POLICY
        value: reinstall
      image: rancher/klipper-helm:v0.9.3-build20241008
      imagePullPolicy: IfNotPresent
      name: helm
      resources: {}
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
        readOnlyRootFilesystem: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /home/klipper-helm/.helm
        name: klipper-helm
      - mountPath: /home/klipper-helm/.cache
        name: klipper-cache
      - mountPath: /home/klipper-helm/.config
        name: klipper-config
      - mountPath: /tmp
        name: tmp
      - mountPath: /config
        name: values
      - mountPath: /chart
        name: content
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-5h46s
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: ip-172-31-91-35
    nodeSelector:
      kubernetes.io/os: linux
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: OnFailure
    schedulerName: default-scheduler
    securityContext:
      runAsNonRoot: true
      seccompProfile:
        type: RuntimeDefault
    serviceAccount: helm-traefik
    serviceAccountName: helm-traefik
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - emptyDir:
        medium: Memory
      name: klipper-helm
    - emptyDir:
        medium: Memory
      name: klipper-cache
    - emptyDir:
        medium: Memory
      name: klipper-config
    - emptyDir:
        medium: Memory
      name: tmp
    - name: values
      secret:
        defaultMode: 420
        secretName: chart-values-traefik
    - configMap:
        defaultMode: 420
        name: chart-content-traefik
      name: content
    - name: kube-api-access-5h46s
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-02-27T05:22:59Z"
      status: "False"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-02-27T05:22:15Z"
      reason: PodCompleted
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2025-02-27T05:22:57Z"
      reason: PodCompleted
      status: "False"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2025-02-27T05:22:57Z"
      reason: PodCompleted
      status: "False"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-02-27T05:22:15Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://82717f7b46299138982688b3b8458f7224fd110956df64568a8f37ac3392a74a
      image: docker.io/rancher/klipper-helm:v0.9.3-build20241008
      imageID: docker.io/rancher/klipper-helm@sha256:73ff7ef399717ba8339559054557bd427bdafb47db112165a8c0c358d1ca0283
      lastState: {}
      name: helm
      ready: false
      restartCount: 2
      started: false
      state:
        terminated:
          containerID: containerd://82717f7b46299138982688b3b8458f7224fd110956df64568a8f37ac3392a74a
          exitCode: 0
          finishedAt: "2025-02-27T05:22:57Z"
          message: |
            Installing helm chart
          reason: Completed
          startedAt: "2025-02-27T05:22:55Z"
      volumeMounts:
      - mountPath: /home/klipper-helm/.helm
        name: klipper-helm
      - mountPath: /home/klipper-helm/.cache
        name: klipper-cache
      - mountPath: /home/klipper-helm/.config
        name: klipper-config
      - mountPath: /tmp
        name: tmp
      - mountPath: /config
        name: values
      - mountPath: /chart
        name: content
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-5h46s
        readOnly: true
        recursiveReadOnly: Disabled
    hostIP: 172.31.91.35
    hostIPs:
    - ip: 172.31.91.35
    phase: Succeeded
    qosClass: BestEffort
    startTime: "2025-02-27T05:22:15Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2025-02-27T05:22:14Z"
    generateName: local-path-provisioner-5cf85fd84d-
    labels:
      app: local-path-provisioner
      pod-template-hash: 5cf85fd84d
    name: local-path-provisioner-5cf85fd84d-pswzz
    namespace: kube-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: local-path-provisioner-5cf85fd84d
      uid: 5cead904-ec0a-4418-8fab-e1e8874a8145
    resourceVersion: "22759"
    uid: ec42452b-fafb-49ed-b46a-b82107f2cf3d
  spec:
    containers:
    - command:
      - local-path-provisioner
      - start
      - --config
      - /etc/config/config.json
      env:
      - name: POD_NAMESPACE
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.namespace
      image: rancher/local-path-provisioner:v0.0.30
      imagePullPolicy: IfNotPresent
      name: local-path-provisioner
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/config/
        name: config-volume
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-pgqp8
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: ip-172-31-91-35
    preemptionPolicy: PreemptLowerPriority
    priority: 2000001000
    priorityClassName: system-node-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: local-path-provisioner-service-account
    serviceAccountName: local-path-provisioner-service-account
    terminationGracePeriodSeconds: 30
    tolerations:
    - key: CriticalAddonsOnly
      operator: Exists
    - effect: NoSchedule
      key: node-role.kubernetes.io/control-plane
      operator: Exists
    - effect: NoSchedule
      key: node-role.kubernetes.io/master
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - configMap:
        defaultMode: 420
        name: local-path-config
      name: config-volume
    - name: kube-api-access-pgqp8
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-03-03T02:55:05Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-02-27T05:22:15Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2025-03-03T02:55:05Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2025-03-03T02:55:05Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-02-27T05:22:15Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://896fee8596215f7938601d5a412859566971cdf5cf8fd75a5c94435412638561
      image: docker.io/rancher/local-path-provisioner:v0.0.30
      imageID: docker.io/rancher/local-path-provisioner@sha256:9b914881170048f80ae9302f36e5b99b4a6b18af73a38adc1c66d12f65d360be
      lastState:
        terminated:
          containerID: containerd://10df81fd238a27232ba67436fd9eb004d91176d7f1ca2adfa1b1a1936bce6cb6
          exitCode: 255
          finishedAt: "2025-03-03T02:54:52Z"
          reason: Unknown
          startedAt: "2025-03-02T07:30:26Z"
      name: local-path-provisioner
      ready: true
      restartCount: 7
      started: true
      state:
        running:
          startedAt: "2025-03-03T02:55:01Z"
      volumeMounts:
      - mountPath: /etc/config/
        name: config-volume
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-pgqp8
        readOnly: true
        recursiveReadOnly: Disabled
    hostIP: 172.31.91.35
    hostIPs:
    - ip: 172.31.91.35
    phase: Running
    podIP: 10.42.0.106
    podIPs:
    - ip: 10.42.0.106
    qosClass: BestEffort
    startTime: "2025-02-27T05:22:15Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2025-02-27T05:22:14Z"
    generateName: metrics-server-5985cbc9d7-
    labels:
      k8s-app: metrics-server
      pod-template-hash: 5985cbc9d7
    name: metrics-server-5985cbc9d7-794hc
    namespace: kube-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: metrics-server-5985cbc9d7
      uid: c648ac59-2e3f-4b68-9907-c71953d6db72
    resourceVersion: "22949"
    uid: a2738292-c3d5-4ea5-8143-6e01049b3e43
  spec:
    containers:
    - args:
      - --cert-dir=/tmp
      - --secure-port=10250
      - --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname
      - --kubelet-use-node-status-port
      - --metric-resolution=15s
      - --tls-cipher-suites=TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305
      image: rancher/mirrored-metrics-server:v0.7.2
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 3
        httpGet:
          path: /livez
          port: https
          scheme: HTTPS
        initialDelaySeconds: 60
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      name: metrics-server
      ports:
      - containerPort: 10250
        name: https
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /readyz
          port: https
          scheme: HTTPS
        periodSeconds: 2
        successThreshold: 1
        timeoutSeconds: 1
      resources:
        requests:
          cpu: 100m
          memory: 70Mi
      securityContext:
        allowPrivilegeEscalation: false
        readOnlyRootFilesystem: true
        runAsNonRoot: true
        runAsUser: 1000
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /tmp
        name: tmp-dir
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-5d9ck
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: ip-172-31-91-35
    preemptionPolicy: PreemptLowerPriority
    priority: 2000001000
    priorityClassName: system-node-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: metrics-server
    serviceAccountName: metrics-server
    terminationGracePeriodSeconds: 30
    tolerations:
    - key: CriticalAddonsOnly
      operator: Exists
    - effect: NoSchedule
      key: node-role.kubernetes.io/control-plane
      operator: Exists
    - effect: NoSchedule
      key: node-role.kubernetes.io/master
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - emptyDir: {}
      name: tmp-dir
    - name: kube-api-access-5d9ck
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-03-03T02:55:06Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-02-27T05:22:15Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2025-03-03T02:55:23Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2025-03-03T02:55:23Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-02-27T05:22:15Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://e801e2843d5bcc4f1003a65a1b2a8fb2b3121f0095663526c010bc22ef27b9ea
      image: docker.io/rancher/mirrored-metrics-server:v0.7.2
      imageID: docker.io/rancher/mirrored-metrics-server@sha256:dccf8474fb910fef261d31d9483d7e4c1df7b86cf4d638fb6a7d7c88bd51600a
      lastState:
        terminated:
          containerID: containerd://aea8291a13810c1497e3a3bf6d578445667fabe24ddde4b02ca83e3339ade671
          exitCode: 255
          finishedAt: "2025-03-03T02:54:52Z"
          reason: Unknown
          startedAt: "2025-03-02T07:30:28Z"
      name: metrics-server
      ready: true
      restartCount: 10
      started: true
      state:
        running:
          startedAt: "2025-03-03T02:55:05Z"
      volumeMounts:
      - mountPath: /tmp
        name: tmp-dir
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-5d9ck
        readOnly: true
        recursiveReadOnly: Disabled
    hostIP: 172.31.91.35
    hostIPs:
    - ip: 172.31.91.35
    phase: Running
    podIP: 10.42.0.112
    podIPs:
    - ip: 10.42.0.112
    qosClass: Burstable
    startTime: "2025-02-27T05:22:15Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2025-02-27T05:22:57Z"
    generateName: svclb-traefik-b6c4d7d5-
    labels:
      app: svclb-traefik-b6c4d7d5
      controller-revision-hash: 5d757bd75b
      pod-template-generation: "1"
      svccontroller.k3s.cattle.io/svcname: traefik
      svccontroller.k3s.cattle.io/svcnamespace: kube-system
    name: svclb-traefik-b6c4d7d5-j2296
    namespace: kube-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: svclb-traefik-b6c4d7d5
      uid: b1de4a8b-ed89-4722-9f80-654f6a4c6691
    resourceVersion: "22734"
    uid: f5f4979c-4e92-4149-96cd-3145fc23f190
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - ip-172-31-91-35
    automountServiceAccountToken: false
    containers:
    - env:
      - name: SRC_PORT
        value: "80"
      - name: SRC_RANGES
        value: 0.0.0.0/0
      - name: DEST_PROTO
        value: TCP
      - name: DEST_PORT
        value: "80"
      - name: DEST_IPS
        value: 10.43.227.11
      image: rancher/klipper-lb:v0.4.9
      imagePullPolicy: IfNotPresent
      name: lb-tcp-80
      ports:
      - containerPort: 80
        hostPort: 80
        name: lb-tcp-80
        protocol: TCP
      resources: {}
      securityContext:
        capabilities:
          add:
          - NET_ADMIN
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
    - env:
      - name: SRC_PORT
        value: "443"
      - name: SRC_RANGES
        value: 0.0.0.0/0
      - name: DEST_PROTO
        value: TCP
      - name: DEST_PORT
        value: "443"
      - name: DEST_IPS
        value: 10.43.227.11
      image: rancher/klipper-lb:v0.4.9
      imagePullPolicy: IfNotPresent
      name: lb-tcp-443
      ports:
      - containerPort: 443
        hostPort: 443
        name: lb-tcp-443
        protocol: TCP
      resources: {}
      securityContext:
        capabilities:
          add:
          - NET_ADMIN
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: ip-172-31-91-35
    preemptionPolicy: PreemptLowerPriority
    priority: 2000001000
    priorityClassName: system-node-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      sysctls:
      - name: net.ipv4.ip_forward
        value: "1"
    serviceAccount: svclb
    serviceAccountName: svclb
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoSchedule
      key: node-role.kubernetes.io/master
      operator: Exists
    - effect: NoSchedule
      key: node-role.kubernetes.io/control-plane
      operator: Exists
    - key: CriticalAddonsOnly
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-03-03T02:55:05Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-02-27T05:22:57Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2025-03-03T02:55:05Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2025-03-03T02:55:05Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-02-27T05:22:57Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://59bc7c5f6b57849559ea6609492567f4075dfa534608b129b5287f305ccaee0b
      image: docker.io/rancher/klipper-lb:v0.4.9
      imageID: docker.io/rancher/klipper-lb@sha256:dd380f5d89a52f2a07853ff17a6048f805c1f8113b50578f3efc3efb9bcf670a
      lastState:
        terminated:
          containerID: containerd://297ebb1b2b6ea32815c568034fbc174f30b37f07397aa5e358b5133a0544f8f7
          exitCode: 255
          finishedAt: "2025-03-03T02:54:52Z"
          reason: Unknown
          startedAt: "2025-03-02T07:30:28Z"
      name: lb-tcp-443
      ready: true
      restartCount: 7
      started: true
      state:
        running:
          startedAt: "2025-03-03T02:55:04Z"
    - containerID: containerd://b2049d07af978cdfed89ca54e23feadbde746088cd47432afaa038f6b34eb20b
      image: docker.io/rancher/klipper-lb:v0.4.9
      imageID: docker.io/rancher/klipper-lb@sha256:dd380f5d89a52f2a07853ff17a6048f805c1f8113b50578f3efc3efb9bcf670a
      lastState:
        terminated:
          containerID: containerd://21bc4c1d128fac89050f8d8cdfe58671ff391d75e77e26825d31503d0293f834
          exitCode: 255
          finishedAt: "2025-03-03T02:54:52Z"
          reason: Unknown
          startedAt: "2025-03-02T07:30:27Z"
      name: lb-tcp-80
      ready: true
      restartCount: 7
      started: true
      state:
        running:
          startedAt: "2025-03-03T02:55:03Z"
    hostIP: 172.31.91.35
    hostIPs:
    - ip: 172.31.91.35
    phase: Running
    podIP: 10.42.0.110
    podIPs:
    - ip: 10.42.0.110
    qosClass: BestEffort
    startTime: "2025-02-27T05:22:57Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      prometheus.io/path: /metrics
      prometheus.io/port: "9100"
      prometheus.io/scrape: "true"
    creationTimestamp: "2025-02-27T05:22:56Z"
    generateName: traefik-5d45fc8cc9-
    labels:
      app.kubernetes.io/instance: traefik-kube-system
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: traefik
      helm.sh/chart: traefik-27.0.201_up27.0.2
      pod-template-hash: 5d45fc8cc9
    name: traefik-5d45fc8cc9-9ntrv
    namespace: kube-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: traefik-5d45fc8cc9
      uid: bc91c3f6-9c21-4970-b44f-59dbc13d7b1b
    resourceVersion: "22887"
    uid: ecee522e-0852-44fe-bf25-3d546bc2d7b9
  spec:
    containers:
    - args:
      - --global.checknewversion
      - --global.sendanonymoususage
      - --entrypoints.metrics.address=:9100/tcp
      - --entrypoints.traefik.address=:9000/tcp
      - --entrypoints.web.address=:8000/tcp
      - --entrypoints.websecure.address=:8443/tcp
      - --api.dashboard=true
      - --ping=true
      - --metrics.prometheus=true
      - --metrics.prometheus.entrypoint=metrics
      - --providers.kubernetescrd
      - --providers.kubernetesingress
      - --providers.kubernetesingress.ingressendpoint.publishedservice=kube-system/traefik
      - --entrypoints.websecure.http.tls=true
      env:
      - name: POD_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.name
      - name: POD_NAMESPACE
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.namespace
      image: rancher/mirrored-library-traefik:2.11.18
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 3
        httpGet:
          path: /ping
          port: 9000
          scheme: HTTP
        initialDelaySeconds: 2
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 2
      name: traefik
      ports:
      - containerPort: 9100
        name: metrics
        protocol: TCP
      - containerPort: 9000
        name: traefik
        protocol: TCP
      - containerPort: 8000
        name: web
        protocol: TCP
      - containerPort: 8443
        name: websecure
        protocol: TCP
      readinessProbe:
        failureThreshold: 1
        httpGet:
          path: /ping
          port: 9000
          scheme: HTTP
        initialDelaySeconds: 2
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 2
      resources: {}
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
        readOnlyRootFilesystem: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /data
        name: data
      - mountPath: /tmp
        name: tmp
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-ccm4x
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: ip-172-31-91-35
    preemptionPolicy: PreemptLowerPriority
    priority: 2000000000
    priorityClassName: system-cluster-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      fsGroupChangePolicy: OnRootMismatch
      runAsGroup: 65532
      runAsNonRoot: true
      runAsUser: 65532
    serviceAccount: traefik
    serviceAccountName: traefik
    terminationGracePeriodSeconds: 60
    tolerations:
    - key: CriticalAddonsOnly
      operator: Exists
    - effect: NoSchedule
      key: node-role.kubernetes.io/control-plane
      operator: Exists
    - effect: NoSchedule
      key: node-role.kubernetes.io/master
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - emptyDir: {}
      name: data
    - emptyDir: {}
      name: tmp
    - name: kube-api-access-ccm4x
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-03-03T02:55:05Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-02-27T05:22:57Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2025-03-03T02:55:09Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2025-03-03T02:55:09Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-02-27T05:22:57Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://5aba2096b4759a030dc76507dd2ad14128932d89bb8492a6a7ec7b8b49b8095b
      image: docker.io/rancher/mirrored-library-traefik:2.11.18
      imageID: docker.io/rancher/mirrored-library-traefik@sha256:25df7bff0b414867f16a74c571c0dc84920404e45cc7780976cec77809230e09
      lastState:
        terminated:
          containerID: containerd://b6955ce375a42afa00c4d9b4a5447d42d48deedb1cdc25405b26c2d31bd272b9
          exitCode: 255
          finishedAt: "2025-03-03T02:54:52Z"
          reason: Unknown
          startedAt: "2025-03-02T07:30:27Z"
      name: traefik
      ready: true
      restartCount: 10
      started: true
      state:
        running:
          startedAt: "2025-03-03T02:55:04Z"
      volumeMounts:
      - mountPath: /data
        name: data
      - mountPath: /tmp
        name: tmp
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-ccm4x
        readOnly: true
        recursiveReadOnly: Disabled
    hostIP: 172.31.91.35
    hostIPs:
    - ip: 172.31.91.35
    phase: Running
    podIP: 10.42.0.111
    podIPs:
    - ip: 10.42.0.111
    qosClass: BestEffort
    startTime: "2025-02-27T05:22:57Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      checksum/config: c6322136dbc9671149df2fefe7d70ceb2a5f680d5aa29ba005d080f37bee0106
    creationTimestamp: "2025-02-27T05:45:43Z"
    generateName: prometheus-alertmanager-
    labels:
      app.kubernetes.io/instance: prometheus
      app.kubernetes.io/name: alertmanager
      apps.kubernetes.io/pod-index: "0"
      controller-revision-hash: prometheus-alertmanager-769db6f968
      statefulset.kubernetes.io/pod-name: prometheus-alertmanager-0
    name: prometheus-alertmanager-0
    namespace: monitoring
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: StatefulSet
      name: prometheus-alertmanager
      uid: 89704cf2-5cbd-4469-aec7-84eef9e23347
    resourceVersion: "22762"
    uid: 70f26c00-736e-4501-8138-8a9b4c84934c
  spec:
    automountServiceAccountToken: true
    containers:
    - args:
      - --storage.path=/alertmanager
      - --config.file=/etc/alertmanager/alertmanager.yml
      env:
      - name: POD_IP
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: status.podIP
      image: quay.io/prometheus/alertmanager:v0.28.0
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 3
        httpGet:
          path: /
          port: http
          scheme: HTTP
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      name: alertmanager
      ports:
      - containerPort: 9093
        name: http
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /
          port: http
          scheme: HTTP
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      resources: {}
      securityContext:
        runAsGroup: 65534
        runAsNonRoot: true
        runAsUser: 65534
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/alertmanager
        name: config
      - mountPath: /alertmanager
        name: storage
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-h6ftv
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostname: prometheus-alertmanager-0
    nodeName: ip-172-31-91-35
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      fsGroup: 65534
      runAsGroup: 65534
      runAsNonRoot: true
      runAsUser: 65534
    serviceAccount: prometheus-alertmanager
    serviceAccountName: prometheus-alertmanager
    subdomain: prometheus-alertmanager-headless
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: storage
      persistentVolumeClaim:
        claimName: storage-prometheus-alertmanager-0
    - configMap:
        defaultMode: 420
        name: prometheus-alertmanager
      name: config
    - name: kube-api-access-h6ftv
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-03-03T02:55:05Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-02-27T05:45:48Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2025-03-03T02:55:05Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2025-03-03T02:55:05Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-02-27T05:45:48Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://f7fddc381bf6645fa8ddde3d5b5be2e861561c1828fe859b137005d285c3ed3e
      image: quay.io/prometheus/alertmanager:v0.28.0
      imageID: quay.io/prometheus/alertmanager@sha256:d5155cfac40a6d9250ffc97c19db2c5e190c7bc57c6b67125c94903358f8c7d8
      lastState:
        terminated:
          containerID: containerd://b93a32debbeb462db01c9545f561548bd6252801352269ea16018f14b8e6f64e
          exitCode: 255
          finishedAt: "2025-03-03T02:54:52Z"
          reason: Unknown
          startedAt: "2025-03-02T07:30:26Z"
      name: alertmanager
      ready: true
      restartCount: 6
      started: true
      state:
        running:
          startedAt: "2025-03-03T02:55:03Z"
      volumeMounts:
      - mountPath: /etc/alertmanager
        name: config
      - mountPath: /alertmanager
        name: storage
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-h6ftv
        readOnly: true
        recursiveReadOnly: Disabled
    hostIP: 172.31.91.35
    hostIPs:
    - ip: 172.31.91.35
    phase: Running
    podIP: 10.42.0.108
    podIPs:
    - ip: 10.42.0.108
    qosClass: BestEffort
    startTime: "2025-02-27T05:45:48Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2025-02-27T05:45:43Z"
    generateName: prometheus-kube-state-metrics-65846b5c64-
    labels:
      app.kubernetes.io/component: metrics
      app.kubernetes.io/instance: prometheus
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: kube-state-metrics
      app.kubernetes.io/part-of: kube-state-metrics
      app.kubernetes.io/version: 2.15.0
      helm.sh/chart: kube-state-metrics-5.30.0
      pod-template-hash: 65846b5c64
    name: prometheus-kube-state-metrics-65846b5c64-8pcjk
    namespace: monitoring
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: prometheus-kube-state-metrics-65846b5c64
      uid: 8df3a5da-62fd-45a8-8c54-7e95b0438fda
    resourceVersion: "22865"
    uid: 6590761b-0001-4ec1-80a4-15bc6a275f73
  spec:
    automountServiceAccountToken: true
    containers:
    - args:
      - --port=8080
      - --resources=certificatesigningrequests,configmaps,cronjobs,daemonsets,deployments,endpoints,horizontalpodautoscalers,ingresses,jobs,leases,limitranges,mutatingwebhookconfigurations,namespaces,networkpolicies,nodes,persistentvolumeclaims,persistentvolumes,poddisruptionbudgets,pods,replicasets,replicationcontrollers,resourcequotas,secrets,services,statefulsets,storageclasses,validatingwebhookconfigurations,volumeattachments
      image: registry.k8s.io/kube-state-metrics/kube-state-metrics:v2.15.0
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 3
        httpGet:
          path: /livez
          port: 8080
          scheme: HTTP
        initialDelaySeconds: 5
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 5
      name: kube-state-metrics
      ports:
      - containerPort: 8080
        name: http
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /readyz
          port: 8081
          scheme: HTTP
        initialDelaySeconds: 5
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 5
      resources: {}
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
        readOnlyRootFilesystem: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-tj7fm
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: ip-172-31-91-35
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      fsGroup: 65534
      runAsGroup: 65534
      runAsNonRoot: true
      runAsUser: 65534
      seccompProfile:
        type: RuntimeDefault
    serviceAccount: prometheus-kube-state-metrics
    serviceAccountName: prometheus-kube-state-metrics
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: kube-api-access-tj7fm
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-03-03T02:55:05Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-02-27T05:45:43Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2025-03-03T02:55:07Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2025-03-03T02:55:07Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-02-27T05:45:43Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://f496c980fbc284da51f87fb5ce4c87940b580554970283dab431181898e029a1
      image: registry.k8s.io/kube-state-metrics/kube-state-metrics:v2.15.0
      imageID: registry.k8s.io/kube-state-metrics/kube-state-metrics@sha256:db384bf43222b066c378e77027a675d4cd9911107adba46c2922b3a55e10d6fb
      lastState:
        terminated:
          containerID: containerd://4aa721af55208787544cbbd91269ef188f4d9c4f0bc542a5d11992fc422b440d
          exitCode: 255
          finishedAt: "2025-03-03T02:54:52Z"
          reason: Unknown
          startedAt: "2025-03-02T07:30:26Z"
      name: kube-state-metrics
      ready: true
      restartCount: 6
      started: true
      state:
        running:
          startedAt: "2025-03-03T02:55:01Z"
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-tj7fm
        readOnly: true
        recursiveReadOnly: Disabled
    hostIP: 172.31.91.35
    hostIPs:
    - ip: 172.31.91.35
    phase: Running
    podIP: 10.42.0.104
    podIPs:
    - ip: 10.42.0.104
    qosClass: BestEffort
    startTime: "2025-02-27T05:45:43Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      cluster-autoscaler.kubernetes.io/safe-to-evict: "true"
    creationTimestamp: "2025-02-27T05:45:43Z"
    generateName: prometheus-prometheus-node-exporter-
    labels:
      app.kubernetes.io/component: metrics
      app.kubernetes.io/instance: prometheus
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: prometheus-node-exporter
      app.kubernetes.io/part-of: prometheus-node-exporter
      app.kubernetes.io/version: 1.9.0
      controller-revision-hash: 6d7bf84f58
      helm.sh/chart: prometheus-node-exporter-4.44.0
      pod-template-generation: "1"
    name: prometheus-prometheus-node-exporter-97kvd
    namespace: monitoring
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: prometheus-prometheus-node-exporter
      uid: 37694fd5-b2fd-410f-a3f5-dcb878e1fb77
    resourceVersion: "22801"
    uid: 73d260ea-775a-409a-8646-8c4343c5c152
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - ip-172-31-91-35
    automountServiceAccountToken: false
    containers:
    - args:
      - --path.procfs=/host/proc
      - --path.sysfs=/host/sys
      - --path.rootfs=/host/root
      - --path.udev.data=/host/root/run/udev/data
      - --web.listen-address=[$(HOST_IP)]:9100
      env:
      - name: HOST_IP
        value: 0.0.0.0
      image: quay.io/prometheus/node-exporter:v1.9.0
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 3
        httpGet:
          path: /
          port: 9100
          scheme: HTTP
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      name: node-exporter
      ports:
      - containerPort: 9100
        hostPort: 9100
        name: metrics
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /
          port: 9100
          scheme: HTTP
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      resources: {}
      securityContext:
        allowPrivilegeEscalation: false
        readOnlyRootFilesystem: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /host/proc
        name: proc
        readOnly: true
      - mountPath: /host/sys
        name: sys
        readOnly: true
      - mountPath: /host/root
        mountPropagation: HostToContainer
        name: root
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostNetwork: true
    hostPID: true
    nodeName: ip-172-31-91-35
    nodeSelector:
      kubernetes.io/os: linux
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      fsGroup: 65534
      runAsGroup: 65534
      runAsNonRoot: true
      runAsUser: 65534
    serviceAccount: prometheus-prometheus-node-exporter
    serviceAccountName: prometheus-prometheus-node-exporter
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoSchedule
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
    volumes:
    - hostPath:
        path: /proc
        type: ""
      name: proc
    - hostPath:
        path: /sys
        type: ""
      name: sys
    - hostPath:
        path: /
        type: ""
      name: root
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-03-03T02:55:06Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-02-27T05:45:43Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2025-03-03T02:55:06Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2025-03-03T02:55:06Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-02-27T05:45:43Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://5c3abffb7236ffe4cf27b326224b94f8430a357bdc36a14f7606e9751d1c283e
      image: quay.io/prometheus/node-exporter:v1.9.0
      imageID: quay.io/prometheus/node-exporter@sha256:c99d7ee4d12a38661788f60d9eca493f08584e2e544bbd3b3fca64749f86b848
      lastState:
        terminated:
          containerID: containerd://5751d9e1e4abd191a225a187c5ca0449c284f5468d926e5fbb6a6c5e43a13f72
          exitCode: 255
          finishedAt: "2025-03-03T02:54:52Z"
          reason: Unknown
          startedAt: "2025-03-02T07:30:25Z"
      name: node-exporter
      ready: true
      restartCount: 6
      started: true
      state:
        running:
          startedAt: "2025-03-03T02:55:02Z"
      volumeMounts:
      - mountPath: /host/proc
        name: proc
        readOnly: true
        recursiveReadOnly: Disabled
      - mountPath: /host/sys
        name: sys
        readOnly: true
        recursiveReadOnly: Disabled
      - mountPath: /host/root
        name: root
        readOnly: true
        recursiveReadOnly: Disabled
    hostIP: 172.31.91.35
    hostIPs:
    - ip: 172.31.91.35
    phase: Running
    podIP: 172.31.91.35
    podIPs:
    - ip: 172.31.91.35
    qosClass: BestEffort
    startTime: "2025-02-27T05:45:43Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2025-02-27T05:45:43Z"
    generateName: prometheus-prometheus-pushgateway-64bc8bff6f-
    labels:
      app.kubernetes.io/instance: prometheus
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: prometheus-pushgateway
      app.kubernetes.io/version: v1.11.0
      helm.sh/chart: prometheus-pushgateway-3.0.0
      pod-template-hash: 64bc8bff6f
    name: prometheus-prometheus-pushgateway-64bc8bff6f-czs2w
    namespace: monitoring
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: prometheus-prometheus-pushgateway-64bc8bff6f
      uid: 23984fcf-969a-4b71-a1be-107f84e38ca2
    resourceVersion: "22934"
    uid: be78bcb0-af2a-4a68-a7e3-1e49f105d0a7
  spec:
    automountServiceAccountToken: true
    containers:
    - image: quay.io/prometheus/pushgateway:v1.11.0
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 3
        httpGet:
          path: /-/healthy
          port: 9091
          scheme: HTTP
        initialDelaySeconds: 10
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 10
      name: pushgateway
      ports:
      - containerPort: 9091
        name: metrics
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /-/ready
          port: 9091
          scheme: HTTP
        initialDelaySeconds: 10
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 10
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /data
        name: storage-volume
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-pthg7
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: ip-172-31-91-35
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      fsGroup: 65534
      runAsNonRoot: true
      runAsUser: 65534
    serviceAccount: prometheus-prometheus-pushgateway
    serviceAccountName: prometheus-prometheus-pushgateway
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - emptyDir: {}
      name: storage-volume
    - name: kube-api-access-pthg7
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-03-03T02:55:05Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-02-27T05:45:43Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2025-03-03T02:55:16Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2025-03-03T02:55:16Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-02-27T05:45:43Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://6fe9c840eb51a5469c1d8e50ad8db5bc0163fa853d9885edf7b9692373ee7653
      image: quay.io/prometheus/pushgateway:v1.11.0
      imageID: quay.io/prometheus/pushgateway@sha256:99392035ae99754b40e579088710df184b6a730b77670148f44102ba9ee01d2f
      lastState:
        terminated:
          containerID: containerd://026a91b9d7aff59ad918da60220ca2e899be60d87d3724ef2895df718ce4dab8
          exitCode: 255
          finishedAt: "2025-03-03T02:54:52Z"
          reason: Unknown
          startedAt: "2025-03-02T07:30:28Z"
      name: pushgateway
      ready: true
      restartCount: 6
      started: true
      state:
        running:
          startedAt: "2025-03-03T02:55:05Z"
      volumeMounts:
      - mountPath: /data
        name: storage-volume
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-pthg7
        readOnly: true
        recursiveReadOnly: Disabled
    hostIP: 172.31.91.35
    hostIPs:
    - ip: 172.31.91.35
    phase: Running
    podIP: 10.42.0.113
    podIPs:
    - ip: 10.42.0.113
    qosClass: BestEffort
    startTime: "2025-02-27T05:45:43Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2025-02-27T05:45:43Z"
    generateName: prometheus-server-847fc44949-
    labels:
      app.kubernetes.io/component: server
      app.kubernetes.io/instance: prometheus
      app.kubernetes.io/name: prometheus
      app.kubernetes.io/part-of: prometheus
      app.kubernetes.io/version: v3.2.0
      helm.sh/chart: prometheus-27.5.0
      pod-template-hash: 847fc44949
    name: prometheus-server-847fc44949-hdkxd
    namespace: monitoring
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: prometheus-server-847fc44949
      uid: 44195ab6-3515-455a-937f-4506cec56146
    resourceVersion: "22973"
    uid: 7db3028c-adeb-4247-920e-5fbb4216d78e
  spec:
    containers:
    - args:
      - --watched-dir=/etc/config
      - --listen-address=0.0.0.0:8080
      - --reload-url=http://127.0.0.1:9090/-/reload
      image: quay.io/prometheus-operator/prometheus-config-reloader:v0.80.1
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 3
        httpGet:
          path: /healthz
          port: metrics
          scheme: HTTP
        initialDelaySeconds: 2
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      name: prometheus-server-configmap-reload
      ports:
      - containerPort: 8080
        name: metrics
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /healthz
          port: metrics
          scheme: HTTP
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/config
        name: config-volume
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-kpdqq
        readOnly: true
    - args:
      - --storage.tsdb.retention.time=15d
      - --config.file=/etc/config/prometheus.yml
      - --storage.tsdb.path=/data
      - --web.console.libraries=/etc/prometheus/console_libraries
      - --web.console.templates=/etc/prometheus/consoles
      - --web.enable-lifecycle
      image: quay.io/prometheus/prometheus:v3.2.0
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 3
        httpGet:
          path: /-/healthy
          port: 9090
          scheme: HTTP
        initialDelaySeconds: 30
        periodSeconds: 15
        successThreshold: 1
        timeoutSeconds: 10
      name: prometheus-server
      ports:
      - containerPort: 9090
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /-/ready
          port: 9090
          scheme: HTTP
        initialDelaySeconds: 30
        periodSeconds: 5
        successThreshold: 1
        timeoutSeconds: 4
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/config
        name: config-volume
      - mountPath: /data
        name: storage-volume
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-kpdqq
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: ip-172-31-91-35
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      fsGroup: 65534
      runAsGroup: 65534
      runAsNonRoot: true
      runAsUser: 65534
    serviceAccount: prometheus-server
    serviceAccountName: prometheus-server
    terminationGracePeriodSeconds: 300
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - configMap:
        defaultMode: 420
        name: prometheus-server
      name: config-volume
    - name: storage-volume
      persistentVolumeClaim:
        claimName: prometheus-server
    - name: kube-api-access-kpdqq
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-03-03T02:55:06Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-02-27T05:45:48Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2025-03-03T02:55:35Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2025-03-03T02:55:35Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-02-27T05:45:48Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://eacb740af2606e569016fa7d9803af729d2f27849ab5f6814fc11844c1f1cd5d
      image: quay.io/prometheus/prometheus:v3.2.0
      imageID: quay.io/prometheus/prometheus@sha256:5888c188cf09e3f7eebc97369c3b2ce713e844cdbd88ccf36f5047c958aea120
      lastState:
        terminated:
          containerID: containerd://86e754992a4901b0743382ab3f87c34ec3ca99f4ea19616567f98dfccc8f4b77
          exitCode: 255
          finishedAt: "2025-03-03T02:54:52Z"
          reason: Unknown
          startedAt: "2025-03-02T07:30:28Z"
      name: prometheus-server
      ready: true
      restartCount: 6
      started: true
      state:
        running:
          startedAt: "2025-03-03T02:55:02Z"
      volumeMounts:
      - mountPath: /etc/config
        name: config-volume
      - mountPath: /data
        name: storage-volume
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-kpdqq
        readOnly: true
        recursiveReadOnly: Disabled
    - containerID: containerd://f08bfca3ab15401b10af784eab75b685fd1952d9e8df72ca6a155dcb64e667b6
      image: quay.io/prometheus-operator/prometheus-config-reloader:v0.80.1
      imageID: quay.io/prometheus-operator/prometheus-config-reloader@sha256:125886af6555c1d9726bef0ebc845e922e0f227bba68e0c70fb661295da96bfe
      lastState:
        terminated:
          containerID: containerd://9bf02b698e466c8d77a4fe133cd8feb8f9acddc4cef886f4af7f9a2bdb56b917
          exitCode: 255
          finishedAt: "2025-03-03T02:54:52Z"
          reason: Unknown
          startedAt: "2025-03-02T07:30:27Z"
      name: prometheus-server-configmap-reload
      ready: true
      restartCount: 6
      started: true
      state:
        running:
          startedAt: "2025-03-03T02:55:01Z"
      volumeMounts:
      - mountPath: /etc/config
        name: config-volume
        readOnly: true
        recursiveReadOnly: Disabled
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-kpdqq
        readOnly: true
        recursiveReadOnly: Disabled
    hostIP: 172.31.91.35
    hostIPs:
    - ip: 172.31.91.35
    phase: Running
    podIP: 10.42.0.105
    podIPs:
    - ip: 10.42.0.105
    qosClass: BestEffort
    startTime: "2025-02-27T05:45:48Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2025-03-02T08:10:23Z"
    generateName: webhook-deployment-64b5cc8c76-
    labels:
      app: webhook
      pod-template-hash: 64b5cc8c76
    name: webhook-deployment-64b5cc8c76-xqjsf
    namespace: monitoring
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: webhook-deployment-64b5cc8c76
      uid: 7fbc2e2c-3474-418d-8ac9-f77231f9197c
    resourceVersion: "22841"
    uid: fdf2c7a3-e87f-4a7e-b0ff-ff9a1e00913d
  spec:
    containers:
    - image: gnh374/webhook-express:latest
      imagePullPolicy: Always
      name: webhook
      ports:
      - containerPort: 3000
        protocol: TCP
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-pfqlz
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: ip-172-31-91-35
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: default
    serviceAccountName: default
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: kube-api-access-pfqlz
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-03-03T02:55:06Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-03-02T08:10:23Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2025-03-03T02:55:06Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2025-03-03T02:55:06Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-03-02T08:10:23Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://ed95917d82a9e76b68b73d41e0000e50cb75e5769eb661dd0f5a90d01b898235
      image: docker.io/gnh374/webhook-express:latest
      imageID: docker.io/gnh374/webhook-express@sha256:7f3d4f924c93d35298d7dfb4aba3bd45e54bec772f7e8a802f72803dcdf5dea7
      lastState:
        terminated:
          containerID: containerd://499409870438af5df6ad244528e2511f1704851bf1e495120fda5ab69e9e276c
          exitCode: 255
          finishedAt: "2025-03-03T02:54:52Z"
          reason: Unknown
          startedAt: "2025-03-02T08:10:45Z"
      name: webhook
      ready: true
      restartCount: 1
      started: true
      state:
        running:
          startedAt: "2025-03-03T02:55:05Z"
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-pfqlz
        readOnly: true
        recursiveReadOnly: Disabled
    hostIP: 172.31.91.35
    hostIPs:
    - ip: 172.31.91.35
    phase: Running
    podIP: 10.42.0.101
    podIPs:
    - ip: 10.42.0.101
    qosClass: BestEffort
    startTime: "2025-03-02T08:10:23Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2025-03-02T07:49:24Z"
    generateName: node-agent-
    labels:
      component: velero
      controller-revision-hash: 9dcbb9b8d
      name: node-agent
      pod-template-generation: "1"
    name: node-agent-wbj9d
    namespace: velero
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: node-agent
      uid: aa3bf221-e9cd-4d40-bb45-78370b029a41
    resourceVersion: "22796"
    uid: 97864039-68e3-4e20-9b7b-3d70df3b5bf6
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - ip-172-31-91-35
    containers:
    - args:
      - node-agent
      - server
      - --features=
      command:
      - /velero
      env:
      - name: NODE_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      - name: VELERO_NAMESPACE
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.namespace
      - name: VELERO_SCRATCH_DIR
        value: /scratch
      image: velero/velero:v1.15.2
      imagePullPolicy: IfNotPresent
      name: node-agent
      ports:
      - containerPort: 8085
        name: metrics
        protocol: TCP
      resources: {}
      securityContext:
        privileged: false
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /host_pods
        mountPropagation: HostToContainer
        name: host-pods
      - mountPath: /var/lib/kubelet/plugins
        mountPropagation: HostToContainer
        name: host-plugins
      - mountPath: /scratch
        name: scratch
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-cznlg
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: ip-172-31-91-35
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      runAsUser: 0
    serviceAccount: velero
    serviceAccountName: velero
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    volumes:
    - hostPath:
        path: /var/lib/kubelet/pods
        type: ""
      name: host-pods
    - hostPath:
        path: /var/lib/kubelet/plugins
        type: ""
      name: host-plugins
    - emptyDir: {}
      name: scratch
    - name: kube-api-access-cznlg
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-03-03T02:55:06Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-03-02T07:49:24Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2025-03-03T02:55:06Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2025-03-03T02:55:06Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-03-02T07:49:24Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://a162b24bd4352c0699d6e4538496cbf74e78525f94518c0f57982c3542ce214e
      image: docker.io/velero/velero:v1.15.2
      imageID: docker.io/velero/velero@sha256:668467fdf39f3a610ed6f27431f38a6fbb6143a2ab302ad3e839b0074aaeba39
      lastState:
        terminated:
          containerID: containerd://5666631a6e7c80869e62da3da569216e5f0532b23596b2fa9d5b30e8aa2d5bfc
          exitCode: 255
          finishedAt: "2025-03-03T02:54:52Z"
          reason: Unknown
          startedAt: "2025-03-02T07:49:29Z"
      name: node-agent
      ready: true
      restartCount: 1
      started: true
      state:
        running:
          startedAt: "2025-03-03T02:55:01Z"
      volumeMounts:
      - mountPath: /host_pods
        name: host-pods
      - mountPath: /var/lib/kubelet/plugins
        name: host-plugins
      - mountPath: /scratch
        name: scratch
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-cznlg
        readOnly: true
        recursiveReadOnly: Disabled
    hostIP: 172.31.91.35
    hostIPs:
    - ip: 172.31.91.35
    phase: Running
    podIP: 10.42.0.103
    podIPs:
    - ip: 10.42.0.103
    qosClass: BestEffort
    startTime: "2025-03-02T07:49:24Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      prometheus.io/path: /metrics
      prometheus.io/port: "8085"
      prometheus.io/scrape: "true"
    creationTimestamp: "2025-03-02T07:49:24Z"
    generateName: velero-57fdbd48db-
    labels:
      component: velero
      deploy: velero
      pod-template-hash: 57fdbd48db
    name: velero-57fdbd48db-drzkw
    namespace: velero
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: velero-57fdbd48db
      uid: 3edacc2d-151c-44fe-bc8f-8ea46daa0056
    resourceVersion: "22869"
    uid: b3aae5d7-5308-43f1-95cb-712a3b13df1b
  spec:
    containers:
    - args:
      - server
      - --features=
      - --uploader-type=kopia
      command:
      - /velero
      env:
      - name: VELERO_SCRATCH_DIR
        value: /scratch
      - name: VELERO_NAMESPACE
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.namespace
      - name: LD_LIBRARY_PATH
        value: /plugins
      image: velero/velero:v1.15.2
      imagePullPolicy: IfNotPresent
      name: velero
      ports:
      - containerPort: 8085
        name: metrics
        protocol: TCP
      resources:
        limits:
          cpu: "1"
          memory: 512Mi
        requests:
          cpu: 500m
          memory: 128Mi
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /plugins
        name: plugins
      - mountPath: /scratch
        name: scratch
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-cklxd
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    initContainers:
    - image: velero/velero-plugin-for-aws:v1.10.0
      imagePullPolicy: IfNotPresent
      name: velero-velero-plugin-for-aws
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /target
        name: plugins
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-cklxd
        readOnly: true
    nodeName: ip-172-31-91-35
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: velero
    serviceAccountName: velero
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - emptyDir: {}
      name: plugins
    - emptyDir: {}
      name: scratch
    - name: kube-api-access-cklxd
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-03-03T02:55:06Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-03-02T07:49:29Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2025-03-03T02:55:08Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2025-03-03T02:55:08Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-03-02T07:49:24Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://2f425a4554e40857871607d4954bde41627366ef757b7cbb1dff3edd311d9c6f
      image: docker.io/velero/velero:v1.15.2
      imageID: docker.io/velero/velero@sha256:668467fdf39f3a610ed6f27431f38a6fbb6143a2ab302ad3e839b0074aaeba39
      lastState:
        terminated:
          containerID: containerd://bbbd93459f4a1281c3f99f6c651203ccf65301562775772f2bf84323b989ddf9
          exitCode: 255
          finishedAt: "2025-03-03T02:54:52Z"
          reason: Unknown
          startedAt: "2025-03-02T07:49:30Z"
      name: velero
      ready: true
      restartCount: 1
      started: true
      state:
        running:
          startedAt: "2025-03-03T02:55:06Z"
      volumeMounts:
      - mountPath: /plugins
        name: plugins
      - mountPath: /scratch
        name: scratch
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-cklxd
        readOnly: true
        recursiveReadOnly: Disabled
    hostIP: 172.31.91.35
    hostIPs:
    - ip: 172.31.91.35
    initContainerStatuses:
    - containerID: containerd://ac59ce90e70a857bb19cf70c79abdb42769cb41cc3a1f49af46eb835dd6a7030
      image: docker.io/velero/velero-plugin-for-aws:v1.10.0
      imageID: docker.io/velero/velero-plugin-for-aws@sha256:80f84902d9f644aecce043908867a6629cc89b7ddb8c6633ad0e8bbe2c364e7e
      lastState: {}
      name: velero-velero-plugin-for-aws
      ready: true
      restartCount: 1
      started: false
      state:
        terminated:
          containerID: containerd://ac59ce90e70a857bb19cf70c79abdb42769cb41cc3a1f49af46eb835dd6a7030
          exitCode: 0
          finishedAt: "2025-03-03T02:55:03Z"
          reason: Completed
          startedAt: "2025-03-03T02:55:02Z"
      volumeMounts:
      - mountPath: /target
        name: plugins
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-cklxd
        readOnly: true
        recursiveReadOnly: Disabled
    phase: Running
    podIP: 10.42.0.109
    podIPs:
    - ip: 10.42.0.109
    qosClass: Burstable
    startTime: "2025-03-02T07:49:24Z"
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      meta.helm.sh/release-name: fleet-agent-cluster-3
      meta.helm.sh/release-namespace: cattle-fleet-system
      objectset.rio.cattle.io/applied: H4sIAAAAAAAA/3zQz06EMBAG8HeZM4u7oLvAUU9ejAdfYFo+XEzpNMxw2Gx4d1NITPTgsX++X7/OnSYY92xM3Z04RjG2UaLmpbgveFNYOY9SejYLKEd5GHvqaAiAHfgT0Q5OxNRmTrQW5GdsxMc4QY2nRF1cQigosEP4F76yXjNdt21/dPXpOLjBVQ3a8/lSn6qh9xeu6/bpEZVrfJNfizzhdxvaNzWxzye7ftgv6E0NU85pgs9VfFjUML++U0dvEkEFKQK8ybxNJKU/+lqQ3VKWX36i2TO2ZftcEO6fOXD0yMS6rt8BAAD//1IdqY9lAQAA
      objectset.rio.cattle.io/id: fleet-agent-bootstrap
    creationTimestamp: "2025-02-27T05:42:59Z"
    labels:
      app.kubernetes.io/managed-by: Helm
      objectset.rio.cattle.io/hash: f399d0b310fbfb28e9667312fdc7a33954e2b8c8
    name: fleet-agent
    namespace: cattle-fleet-system
    resourceVersion: "1721"
    uid: b6f0625c-e349-41fb-9979-cc37e65d54d8
  spec:
    clusterIP: None
    clusterIPs:
    - None
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    selector:
      app: fleet-agent
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"v1","kind":"Service","metadata":{"annotations":{},"name":"cattle-cluster-agent","namespace":"cattle-system"},"spec":{"ports":[{"name":"http","port":80,"protocol":"TCP","targetPort":80},{"name":"https-internal","port":443,"protocol":"TCP","targetPort":444}],"selector":{"app":"cattle-cluster-agent"}}}
    creationTimestamp: "2025-02-27T05:26:29Z"
    name: cattle-cluster-agent
    namespace: cattle-system
    resourceVersion: "778"
    uid: 1e9ea528-7241-4de0-a8dc-23b05482be5c
  spec:
    clusterIP: 10.43.131.115
    clusterIPs:
    - 10.43.131.115
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: http
      port: 80
      protocol: TCP
      targetPort: 80
    - name: https-internal
      port: 443
      protocol: TCP
      targetPort: 444
    selector:
      app: cattle-cluster-agent
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      meta.helm.sh/release-name: rancher-webhook
      meta.helm.sh/release-namespace: cattle-system
    creationTimestamp: "2025-02-27T05:44:06Z"
    labels:
      app.kubernetes.io/managed-by: Helm
    name: rancher-webhook
    namespace: cattle-system
    resourceVersion: "1823"
    uid: dcf7ed19-b52c-4b98-8067-6d03d220b377
  spec:
    clusterIP: 10.43.37.244
    clusterIPs:
    - 10.43.37.244
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: https
      port: 443
      protocol: TCP
      targetPort: 9443
    selector:
      app: rancher-webhook
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    creationTimestamp: "2025-02-27T05:22:05Z"
    labels:
      component: apiserver
      provider: kubernetes
    name: kubernetes
    namespace: default
    resourceVersion: "195"
    uid: ede8341e-f3f5-48bb-9f1e-7611d07419ab
  spec:
    clusterIP: 10.43.0.1
    clusterIPs:
    - 10.43.0.1
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: https
      port: 443
      protocol: TCP
      targetPort: 6443
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      objectset.rio.cattle.io/applied: H4sIAAAAAAAA/4ySQYvbMBCF/0p5Z9m142TjFfRQdimUQgmk7aXsQZYnG9W2JKRJSgj+70WJl00b0vZm8958vHmjI5Q33yhE4ywk9iUEOmNbSKwp7I0mCAzEqlWsII9Q1jpWbJyN6dc1P0hzJM6DcblWzD3lxr01iQBxU3c/LYXsed9BoqvihbIvxZtPxrbv3rets/9EWDUQJLQL1Nr4X/bolU4z3a6hLB4i0wABH9xAvKVdTG7vAkPivlxUV1rUQfkE4LAjjAK9aqg/1dHVMVPev8DPidJnsMR0mtb9LjKFLE71Tpg/bdNeDy7Q4+f1X/baqriFRKNpVlez+7ouy+W8UkVV36lmURab2eZuSZvlfDYv9GKZ8k7si4i3ahkFoiedVptyf1xBoizyeZUXeVlAvAoR8vul9CRg/Ac1mP6wcr3Rh/SojH3uac1Kd6lXFzhNHV8indOcy19Up+LZaddD4uvjCqO4dGas/S33l4ff3ANxMPqVne567X8SiNSTZhduHHMcx18BAAD//5X9LCMyAwAA
      objectset.rio.cattle.io/id: ""
      objectset.rio.cattle.io/owner-gvk: k3s.cattle.io/v1, Kind=Addon
      objectset.rio.cattle.io/owner-name: coredns
      objectset.rio.cattle.io/owner-namespace: kube-system
      prometheus.io/port: "9153"
      prometheus.io/scrape: "true"
    creationTimestamp: "2025-02-27T05:22:10Z"
    labels:
      k8s-app: kube-dns
      kubernetes.io/cluster-service: "true"
      kubernetes.io/name: CoreDNS
      objectset.rio.cattle.io/hash: bce283298811743a0386ab510f2f67ef74240c57
    name: kube-dns
    namespace: kube-system
    resourceVersion: "269"
    uid: 9b369255-c1ea-4299-b073-2291dd97223c
  spec:
    clusterIP: 10.43.0.10
    clusterIPs:
    - 10.43.0.10
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: dns
      port: 53
      protocol: UDP
      targetPort: 53
    - name: dns-tcp
      port: 53
      protocol: TCP
      targetPort: 53
    - name: metrics
      port: 9153
      protocol: TCP
      targetPort: 9153
    selector:
      k8s-app: kube-dns
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      objectset.rio.cattle.io/applied: H4sIAAAAAAAA/4SQQWsbMRCF/0p5Z9nNep04FfRQWnopBUNKL6WHWe04VleWhGa8xZj970UbFxLaJCchvZn3vqczKPvvXMSnCIuxgcHgYw+LOy6jdwyDAyv1pAR7BsWYlNSnKPWaul/sVFiXxaelI9XAS5/e+uoA86yefkcui/txgMXQyiNlbMybLz727z/0fYqvWkQ6MGxFLN7JQriMXObjgf31bcnkqsVw7HghJ1E+YDII1HGYO1ahRFaWuujCUfRRhIWWY016Onbh+vqE6wWePckeFnTdt527uWrc7abhZtXuqF11q83uev2uu2HabK46t1tTJfxvdTy8P1NKMrtayefPdPDhtE3BuxMstoV3XD4dKdwpuQEGORUV2B/nvzl71SwXAXa9bg1ySZpcCrD49nELA6Vyz7qdJy4L008D4cBOU5l/81YWlPO/4NM0/QkAAP//sKxN444CAAA
      objectset.rio.cattle.io/id: ""
      objectset.rio.cattle.io/owner-gvk: k3s.cattle.io/v1, Kind=Addon
      objectset.rio.cattle.io/owner-name: metrics-server-service
      objectset.rio.cattle.io/owner-namespace: kube-system
    creationTimestamp: "2025-02-27T05:22:10Z"
    labels:
      kubernetes.io/cluster-service: "true"
      kubernetes.io/name: Metrics-server
      objectset.rio.cattle.io/hash: a5d3bc601c871e123fa32b27f549b6ea770bcf4a
    name: metrics-server
    namespace: kube-system
    resourceVersion: "314"
    uid: 9d88be24-b5ff-4f49-881b-124b3849ebd0
  spec:
    clusterIP: 10.43.88.164
    clusterIPs:
    - 10.43.88.164
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: PreferDualStack
    ports:
    - name: https
      port: 443
      protocol: TCP
      targetPort: https
    selector:
      k8s-app: metrics-server
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      field.cattle.io/publicEndpoints: '[{"addresses":["172.31.91.35"],"port":80,"protocol":"TCP","serviceName":"kube-system:traefik","allNodes":false},{"addresses":["172.31.91.35"],"port":443,"protocol":"TCP","serviceName":"kube-system:traefik","allNodes":false}]'
      meta.helm.sh/release-name: traefik
      meta.helm.sh/release-namespace: kube-system
    creationTimestamp: "2025-02-27T05:22:56Z"
    finalizers:
    - service.kubernetes.io/load-balancer-cleanup
    labels:
      app.kubernetes.io/instance: traefik-kube-system
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: traefik
      helm.sh/chart: traefik-27.0.201_up27.0.2
    name: traefik
    namespace: kube-system
    resourceVersion: "10489"
    uid: b6c4d7d5-f168-47e4-bd03-c74dad63c618
  spec:
    allocateLoadBalancerNodePorts: true
    clusterIP: 10.43.227.11
    clusterIPs:
    - 10.43.227.11
    externalTrafficPolicy: Cluster
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: PreferDualStack
    ports:
    - name: web
      nodePort: 30563
      port: 80
      protocol: TCP
      targetPort: web
    - name: websecure
      nodePort: 30695
      port: 443
      protocol: TCP
      targetPort: websecure
    selector:
      app.kubernetes.io/instance: traefik-kube-system
      app.kubernetes.io/name: traefik
    sessionAffinity: None
    type: LoadBalancer
  status:
    loadBalancer:
      ingress:
      - ip: 172.31.91.35
        ipMode: VIP
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      meta.helm.sh/release-name: prometheus
      meta.helm.sh/release-namespace: monitoring
    creationTimestamp: "2025-02-27T05:45:42Z"
    labels:
      app.kubernetes.io/instance: prometheus
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: alertmanager
      app.kubernetes.io/version: v0.28.0
      helm.sh/chart: alertmanager-1.15.0
    name: prometheus-alertmanager
    namespace: monitoring
    resourceVersion: "2089"
    uid: 6d4c5218-e950-4a00-b2a8-50bad7950c01
  spec:
    clusterIP: 10.43.120.244
    clusterIPs:
    - 10.43.120.244
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: http
      port: 9093
      protocol: TCP
      targetPort: http
    selector:
      app.kubernetes.io/instance: prometheus
      app.kubernetes.io/name: alertmanager
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      meta.helm.sh/release-name: prometheus
      meta.helm.sh/release-namespace: monitoring
    creationTimestamp: "2025-02-27T05:45:42Z"
    labels:
      app.kubernetes.io/instance: prometheus
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: alertmanager
      app.kubernetes.io/version: v0.28.0
      helm.sh/chart: alertmanager-1.15.0
    name: prometheus-alertmanager-headless
    namespace: monitoring
    resourceVersion: "2083"
    uid: 2ee2191c-fed6-4f26-b240-906c99260415
  spec:
    clusterIP: None
    clusterIPs:
    - None
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: http
      port: 9093
      protocol: TCP
      targetPort: http
    selector:
      app.kubernetes.io/instance: prometheus
      app.kubernetes.io/name: alertmanager
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      meta.helm.sh/release-name: prometheus
      meta.helm.sh/release-namespace: monitoring
      prometheus.io/scrape: "true"
    creationTimestamp: "2025-02-27T05:45:42Z"
    labels:
      app.kubernetes.io/component: metrics
      app.kubernetes.io/instance: prometheus
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: kube-state-metrics
      app.kubernetes.io/part-of: kube-state-metrics
      app.kubernetes.io/version: 2.15.0
      helm.sh/chart: kube-state-metrics-5.30.0
    name: prometheus-kube-state-metrics
    namespace: monitoring
    resourceVersion: "2098"
    uid: 4a992348-8a28-429b-8c16-29ca4d61afc8
  spec:
    clusterIP: 10.43.76.38
    clusterIPs:
    - 10.43.76.38
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: http
      port: 8080
      protocol: TCP
      targetPort: 8080
    selector:
      app.kubernetes.io/instance: prometheus
      app.kubernetes.io/name: kube-state-metrics
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      meta.helm.sh/release-name: prometheus
      meta.helm.sh/release-namespace: monitoring
      prometheus.io/scrape: "true"
    creationTimestamp: "2025-02-27T05:45:42Z"
    labels:
      app.kubernetes.io/component: metrics
      app.kubernetes.io/instance: prometheus
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: prometheus-node-exporter
      app.kubernetes.io/part-of: prometheus-node-exporter
      app.kubernetes.io/version: 1.9.0
      helm.sh/chart: prometheus-node-exporter-4.44.0
    name: prometheus-prometheus-node-exporter
    namespace: monitoring
    resourceVersion: "2102"
    uid: acbe31ed-11c1-4030-867d-ea56dd0efc05
  spec:
    clusterIP: 10.43.54.225
    clusterIPs:
    - 10.43.54.225
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: metrics
      port: 9100
      protocol: TCP
      targetPort: 9100
    selector:
      app.kubernetes.io/instance: prometheus
      app.kubernetes.io/name: prometheus-node-exporter
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      meta.helm.sh/release-name: prometheus
      meta.helm.sh/release-namespace: monitoring
      prometheus.io/probe: pushgateway
    creationTimestamp: "2025-02-27T05:45:42Z"
    labels:
      app.kubernetes.io/instance: prometheus
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: prometheus-pushgateway
      app.kubernetes.io/version: v1.11.0
      helm.sh/chart: prometheus-pushgateway-3.0.0
    name: prometheus-prometheus-pushgateway
    namespace: monitoring
    resourceVersion: "2100"
    uid: 3dde41c2-3bef-460b-ac05-6661bbcfff3f
  spec:
    clusterIP: 10.43.139.15
    clusterIPs:
    - 10.43.139.15
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: http
      port: 9091
      protocol: TCP
      targetPort: 9091
    selector:
      app.kubernetes.io/instance: prometheus
      app.kubernetes.io/name: prometheus-pushgateway
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      field.cattle.io/publicEndpoints: '[{"port":30882,"protocol":"TCP","serviceName":"monitoring:prometheus-server","allNodes":true}]'
      meta.helm.sh/release-name: prometheus
      meta.helm.sh/release-namespace: monitoring
    creationTimestamp: "2025-02-27T05:45:42Z"
    labels:
      app.kubernetes.io/component: server
      app.kubernetes.io/instance: prometheus
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: prometheus
      app.kubernetes.io/part-of: prometheus
      app.kubernetes.io/version: v3.2.0
      helm.sh/chart: prometheus-27.5.0
    name: prometheus-server
    namespace: monitoring
    resourceVersion: "6152"
    uid: 685056a4-1745-4098-a3dc-baed944b873d
  spec:
    clusterIP: 10.43.203.162
    clusterIPs:
    - 10.43.203.162
    externalTrafficPolicy: Cluster
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: http
      nodePort: 30882
      port: 80
      protocol: TCP
      targetPort: 9090
    selector:
      app.kubernetes.io/component: server
      app.kubernetes.io/instance: prometheus
      app.kubernetes.io/name: prometheus
    sessionAffinity: None
    type: NodePort
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      field.cattle.io/publicEndpoints: '[{"port":30080,"protocol":"TCP","serviceName":"monitoring:webhook-service","allNodes":true}]'
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"v1","kind":"Service","metadata":{"annotations":{},"name":"webhook-service","namespace":"monitoring"},"spec":{"ports":[{"nodePort":30080,"port":80,"protocol":"TCP","targetPort":3000}],"selector":{"app":"webhook"},"type":"NodePort"}}
    creationTimestamp: "2025-03-02T08:10:23Z"
    name: webhook-service
    namespace: monitoring
    resourceVersion: "12625"
    uid: c4af2650-257d-4bc6-b81a-5e6f5844c24d
  spec:
    clusterIP: 10.43.236.44
    clusterIPs:
    - 10.43.236.44
    externalTrafficPolicy: Cluster
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - nodePort: 30080
      port: 80
      protocol: TCP
      targetPort: 3000
    selector:
      app: webhook
    sessionAffinity: None
    type: NodePort
  status:
    loadBalancer: {}
- apiVersion: apps/v1
  kind: DaemonSet
  metadata:
    annotations:
      deprecated.daemonset.template.generation: "1"
      field.cattle.io/publicEndpoints: '[{"nodeName":":ip-172-31-91-35","addresses":["172.31.91.35"],"port":80,"protocol":"TCP","podName":"kube-system:svclb-traefik-b6c4d7d5-j2296","allNodes":false},{"nodeName":":ip-172-31-91-35","addresses":["172.31.91.35"],"port":443,"protocol":"TCP","podName":"kube-system:svclb-traefik-b6c4d7d5-j2296","allNodes":false}]'
      objectset.rio.cattle.io/applied: H4sIAAAAAAAA/8RVUU8jNxD+K9U8bzYJCRBW6gMCVKH2QpTk+nKK0Kw9S9x4bcue3SNC+98rbwK39BJAlUqVh8j2zOfx981+8wTo1J/kg7IGMkDnQr8eQgIbZSRkcI1UWrMghgRKYpTICNkToDGWkZU1IS5t/hcJDsSpVzYVyKwpVbavIgYkR8/td0O+91BvIIN+PUx++V0Z+euCfK0EvZtnsCTIgD1SoTYfCg8ORczZVDn1wjYwldAkIDy1j1mqkgJj6SAzldYJaMxJv/nENYY1ZDAZnRUiFzQ4OZGnEuk0H52dXYwLGhIWo8mFuBhhcS4FJBBqIaxhb7Umn25GoYNmrKRAmgRbDxkUqAO9kxJq8RMRH4g/wsQeKtRC5709YC8/E2N5Lk9hd34kNTgSkakf9T9BiSzWf7yQiM4dB2+aBJhKp5Gpze302wcEehP78yjsEIEV29JWhvcNfSlEXC3thgxkrbYJxEtQGfIBsm9PQKZu//f1LOZX97O7+RISqFFXcWsygCZ5FTC/nP52s+iEDNL2138VeX2zWN7P5nfLu07k8mr2c8xb97URt7PubcNBOh6lJyfn6XAIzSoBVeJDPPBoxJp8f6OVc+R7Os/qQTpOL2AfM6u0nlmtxBYyuC2mlmeeAhmGl0aMYgrXmwwgAWc971h6IW1mPUM2GSSwtoF/rA5le8tWWP386lUCnoKtvKDYP1E3EpVXvL2yhumR275Dh7nSihXtmkxKyL7B9GZ5f3n95XYKq6aJ7Lwv23g8+lzd/nHh/yRcrOIN5cbjUVe6dnkQ4D8TbxXBlW1TNYYw3Rtg+zn3oh/3hFesBGo4eEvYBsE6dOU3xKly9ThV7r6w/jt62aUdmlVbcNcUph3fhQTYavLP8zXaQlGQYMhgahdiTbLScSxsKPLf1uitpjQakTfEFKJLlRiYfByLLmK1A+XmUQUObV/8G8i9I/acRkNHkXcYV3vWLqW0JtwZvT2csIqWWTmJTAv2yPSwjbRG41Xm4Wt7sBslj18N1qg05pogG8ZxsXWRtfmr2NaCGblqRReV92R4WpU5+eeHSsgGCUgKypM8dGTavS8qhAPbc0K5hWzQNH8HAAD//x/w1Xk/CQAA
      objectset.rio.cattle.io/id: ""
      objectset.rio.cattle.io/owner-gvk: /v1, Kind=Service
      objectset.rio.cattle.io/owner-name: traefik
      objectset.rio.cattle.io/owner-namespace: kube-system
    creationTimestamp: "2025-02-27T05:22:56Z"
    generation: 1
    labels:
      objectset.rio.cattle.io/hash: 836fcbce022d5dae5b36694fe1eaf389c93af7dc
      svccontroller.k3s.cattle.io/nodeselector: "false"
      svccontroller.k3s.cattle.io/svcname: traefik
      svccontroller.k3s.cattle.io/svcnamespace: kube-system
    name: svclb-traefik-b6c4d7d5
    namespace: kube-system
    resourceVersion: "10505"
    uid: b1de4a8b-ed89-4722-9f80-654f6a4c6691
  spec:
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app: svclb-traefik-b6c4d7d5
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: svclb-traefik-b6c4d7d5
          svccontroller.k3s.cattle.io/svcname: traefik
          svccontroller.k3s.cattle.io/svcnamespace: kube-system
      spec:
        automountServiceAccountToken: false
        containers:
        - env:
          - name: SRC_PORT
            value: "80"
          - name: SRC_RANGES
            value: 0.0.0.0/0
          - name: DEST_PROTO
            value: TCP
          - name: DEST_PORT
            value: "80"
          - name: DEST_IPS
            value: 10.43.227.11
          image: rancher/klipper-lb:v0.4.9
          imagePullPolicy: IfNotPresent
          name: lb-tcp-80
          ports:
          - containerPort: 80
            hostPort: 80
            name: lb-tcp-80
            protocol: TCP
          resources: {}
          securityContext:
            capabilities:
              add:
              - NET_ADMIN
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        - env:
          - name: SRC_PORT
            value: "443"
          - name: SRC_RANGES
            value: 0.0.0.0/0
          - name: DEST_PROTO
            value: TCP
          - name: DEST_PORT
            value: "443"
          - name: DEST_IPS
            value: 10.43.227.11
          image: rancher/klipper-lb:v0.4.9
          imagePullPolicy: IfNotPresent
          name: lb-tcp-443
          ports:
          - containerPort: 443
            hostPort: 443
            name: lb-tcp-443
            protocol: TCP
          resources: {}
          securityContext:
            capabilities:
              add:
              - NET_ADMIN
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        priorityClassName: system-node-critical
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          sysctls:
          - name: net.ipv4.ip_forward
            value: "1"
        serviceAccount: svclb
        serviceAccountName: svclb
        terminationGracePeriodSeconds: 30
        tolerations:
        - effect: NoSchedule
          key: node-role.kubernetes.io/master
          operator: Exists
        - effect: NoSchedule
          key: node-role.kubernetes.io/control-plane
          operator: Exists
        - key: CriticalAddonsOnly
          operator: Exists
    updateStrategy:
      rollingUpdate:
        maxSurge: 0
        maxUnavailable: 1
      type: RollingUpdate
  status:
    currentNumberScheduled: 1
    desiredNumberScheduled: 1
    numberAvailable: 1
    numberMisscheduled: 0
    numberReady: 1
    observedGeneration: 1
    updatedNumberScheduled: 1
- apiVersion: apps/v1
  kind: DaemonSet
  metadata:
    annotations:
      deprecated.daemonset.template.generation: "1"
      field.cattle.io/publicEndpoints: '[{"nodeName":":ip-172-31-91-35","addresses":["172.31.91.35"],"port":9100,"protocol":"TCP","podName":"monitoring:prometheus-prometheus-node-exporter-97kvd","allNodes":false}]'
      meta.helm.sh/release-name: prometheus
      meta.helm.sh/release-namespace: monitoring
    creationTimestamp: "2025-02-27T05:45:42Z"
    generation: 1
    labels:
      app.kubernetes.io/component: metrics
      app.kubernetes.io/instance: prometheus
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: prometheus-node-exporter
      app.kubernetes.io/part-of: prometheus-node-exporter
      app.kubernetes.io/version: 1.9.0
      helm.sh/chart: prometheus-node-exporter-4.44.0
    name: prometheus-prometheus-node-exporter
    namespace: monitoring
    resourceVersion: "22838"
    uid: 37694fd5-b2fd-410f-a3f5-dcb878e1fb77
  spec:
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app.kubernetes.io/instance: prometheus
        app.kubernetes.io/name: prometheus-node-exporter
    template:
      metadata:
        annotations:
          cluster-autoscaler.kubernetes.io/safe-to-evict: "true"
        creationTimestamp: null
        labels:
          app.kubernetes.io/component: metrics
          app.kubernetes.io/instance: prometheus
          app.kubernetes.io/managed-by: Helm
          app.kubernetes.io/name: prometheus-node-exporter
          app.kubernetes.io/part-of: prometheus-node-exporter
          app.kubernetes.io/version: 1.9.0
          helm.sh/chart: prometheus-node-exporter-4.44.0
      spec:
        affinity:
          nodeAffinity:
            requiredDuringSchedulingIgnoredDuringExecution:
              nodeSelectorTerms:
              - matchExpressions:
                - key: eks.amazonaws.com/compute-type
                  operator: NotIn
                  values:
                  - fargate
                - key: type
                  operator: NotIn
                  values:
                  - virtual-kubelet
        automountServiceAccountToken: false
        containers:
        - args:
          - --path.procfs=/host/proc
          - --path.sysfs=/host/sys
          - --path.rootfs=/host/root
          - --path.udev.data=/host/root/run/udev/data
          - --web.listen-address=[$(HOST_IP)]:9100
          env:
          - name: HOST_IP
            value: 0.0.0.0
          image: quay.io/prometheus/node-exporter:v1.9.0
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /
              port: 9100
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          name: node-exporter
          ports:
          - containerPort: 9100
            name: metrics
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /
              port: 9100
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          resources: {}
          securityContext:
            allowPrivilegeEscalation: false
            readOnlyRootFilesystem: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /host/proc
            name: proc
            readOnly: true
          - mountPath: /host/sys
            name: sys
            readOnly: true
          - mountPath: /host/root
            mountPropagation: HostToContainer
            name: root
            readOnly: true
        dnsPolicy: ClusterFirst
        hostNetwork: true
        hostPID: true
        nodeSelector:
          kubernetes.io/os: linux
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 65534
          runAsGroup: 65534
          runAsNonRoot: true
          runAsUser: 65534
        serviceAccount: prometheus-prometheus-node-exporter
        serviceAccountName: prometheus-prometheus-node-exporter
        terminationGracePeriodSeconds: 30
        tolerations:
        - effect: NoSchedule
          operator: Exists
        volumes:
        - hostPath:
            path: /proc
            type: ""
          name: proc
        - hostPath:
            path: /sys
            type: ""
          name: sys
        - hostPath:
            path: /
            type: ""
          name: root
    updateStrategy:
      rollingUpdate:
        maxSurge: 0
        maxUnavailable: 1
      type: RollingUpdate
  status:
    currentNumberScheduled: 1
    desiredNumberScheduled: 1
    numberAvailable: 1
    numberMisscheduled: 0
    numberReady: 1
    observedGeneration: 1
    updatedNumberScheduled: 1
- apiVersion: apps/v1
  kind: DaemonSet
  metadata:
    annotations:
      deprecated.daemonset.template.generation: "1"
    creationTimestamp: "2025-03-02T07:49:24Z"
    generation: 1
    labels:
      component: velero
    name: node-agent
    namespace: velero
    resourceVersion: "22837"
    uid: aa3bf221-e9cd-4d40-bb45-78370b029a41
  spec:
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        name: node-agent
    template:
      metadata:
        creationTimestamp: null
        labels:
          component: velero
          name: node-agent
      spec:
        containers:
        - args:
          - node-agent
          - server
          - --features=
          command:
          - /velero
          env:
          - name: NODE_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: spec.nodeName
          - name: VELERO_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          - name: VELERO_SCRATCH_DIR
            value: /scratch
          image: velero/velero:v1.15.2
          imagePullPolicy: IfNotPresent
          name: node-agent
          ports:
          - containerPort: 8085
            name: metrics
            protocol: TCP
          resources: {}
          securityContext:
            privileged: false
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /host_pods
            mountPropagation: HostToContainer
            name: host-pods
          - mountPath: /var/lib/kubelet/plugins
            mountPropagation: HostToContainer
            name: host-plugins
          - mountPath: /scratch
            name: scratch
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          runAsUser: 0
        serviceAccount: velero
        serviceAccountName: velero
        terminationGracePeriodSeconds: 30
        volumes:
        - hostPath:
            path: /var/lib/kubelet/pods
            type: ""
          name: host-pods
        - hostPath:
            path: /var/lib/kubelet/plugins
            type: ""
          name: host-plugins
        - emptyDir: {}
          name: scratch
    updateStrategy:
      rollingUpdate:
        maxSurge: 0
        maxUnavailable: 1
      type: RollingUpdate
  status:
    currentNumberScheduled: 1
    desiredNumberScheduled: 1
    numberAvailable: 1
    numberMisscheduled: 0
    numberReady: 1
    observedGeneration: 1
    updatedNumberScheduled: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "3"
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"apps/v1","kind":"Deployment","metadata":{"annotations":{"management.cattle.io/scale-available":"2"},"name":"cattle-cluster-agent","namespace":"cattle-system"},"spec":{"selector":{"matchLabels":{"app":"cattle-cluster-agent"}},"strategy":{"rollingUpdate":{"maxSurge":1,"maxUnavailable":0},"type":"RollingUpdate"},"template":{"metadata":{"labels":{"app":"cattle-cluster-agent"}},"spec":{"affinity":{"nodeAffinity":{"preferredDuringSchedulingIgnoredDuringExecution":[{"preference":{"matchExpressions":[{"key":"node-role.kubernetes.io/controlplane","operator":"In","values":["true"]}]},"weight":100},{"preference":{"matchExpressions":[{"key":"node-role.kubernetes.io/control-plane","operator":"In","values":["true"]}]},"weight":100},{"preference":{"matchExpressions":[{"key":"node-role.kubernetes.io/master","operator":"In","values":["true"]}]},"weight":100},{"preference":{"matchExpressions":[{"key":"cattle.io/cluster-agent","operator":"In","values":["true"]}]},"weight":1}],"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchExpressions":[{"key":"beta.kubernetes.io/os","operator":"NotIn","values":["windows"]}]}]}},"podAntiAffinity":{"preferredDuringSchedulingIgnoredDuringExecution":[{"podAffinityTerm":{"labelSelector":{"matchExpressions":[{"key":"app","operator":"In","values":["cattle-cluster-agent"]}]},"topologyKey":"kubernetes.io/hostname"},"weight":100}]}},"containers":[{"env":[{"name":"CATTLE_FEATURES","value":"embedded-cluster-api=false,fleet=false,managed-system-upgrade-controller=true,multi-cluster-management=false,multi-cluster-management-agent=true,provisioningprebootstrap=false,provisioningv2=false,rke2=false,ui-sql-cache=false"},{"name":"CATTLE_IS_RKE","value":"false"},{"name":"CATTLE_SERVER","value":"https://44.220.0.65.sslip.io"},{"name":"CATTLE_CA_CHECKSUM","value":""},{"name":"CATTLE_CLUSTER","value":"true"},{"name":"CATTLE_K8S_MANAGED","value":"true"},{"name":"CATTLE_CLUSTER_REGISTRY","value":""},{"name":"CATTLE_SERVER_VERSION","value":"v2.10.2"},{"name":"CATTLE_INSTALL_UUID","value":"dd7c2187-12b9-4774-8ac3-7ac6e676ed0b"},{"name":"CATTLE_INGRESS_IP_DOMAIN","value":"sslip.io"},{"name":"STRICT_VERIFY","value":"false"}],"image":"rancher/rancher-agent:v2.10.2","imagePullPolicy":"IfNotPresent","name":"cluster-register","volumeMounts":[{"mountPath":"/cattle-credentials","name":"cattle-credentials","readOnly":true}]}],"serviceAccountName":"cattle","tolerations":[{"effect":"NoSchedule","key":"node-role.kubernetes.io/controlplane","value":"true"},{"effect":"NoSchedule","key":"node-role.kubernetes.io/control-plane","operator":"Exists"},{"effect":"NoSchedule","key":"node-role.kubernetes.io/master","operator":"Exists"}],"volumes":[{"name":"cattle-credentials","secret":{"defaultMode":320,"secretName":"cattle-credentials-7494863"}}]}}}}
      management.cattle.io/scale-available: "2"
    creationTimestamp: "2025-02-27T05:26:29Z"
    generation: 3
    name: cattle-cluster-agent
    namespace: cattle-system
    resourceVersion: "23018"
    uid: 3dafc0ec-b4fe-4ad6-8e60-17cf6b1d78e2
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app: cattle-cluster-agent
    strategy:
      rollingUpdate:
        maxSurge: 1
        maxUnavailable: 0
      type: RollingUpdate
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: cattle-cluster-agent
      spec:
        affinity:
          nodeAffinity:
            preferredDuringSchedulingIgnoredDuringExecution:
            - preference:
                matchExpressions:
                - key: node-role.kubernetes.io/controlplane
                  operator: In
                  values:
                  - "true"
              weight: 100
            - preference:
                matchExpressions:
                - key: node-role.kubernetes.io/control-plane
                  operator: In
                  values:
                  - "true"
              weight: 100
            - preference:
                matchExpressions:
                - key: node-role.kubernetes.io/master
                  operator: In
                  values:
                  - "true"
              weight: 100
            - preference:
                matchExpressions:
                - key: cattle.io/cluster-agent
                  operator: In
                  values:
                  - "true"
              weight: 1
            requiredDuringSchedulingIgnoredDuringExecution:
              nodeSelectorTerms:
              - matchExpressions:
                - key: beta.kubernetes.io/os
                  operator: NotIn
                  values:
                  - windows
          podAntiAffinity:
            preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchExpressions:
                  - key: app
                    operator: In
                    values:
                    - cattle-cluster-agent
                topologyKey: kubernetes.io/hostname
              weight: 100
        containers:
        - env:
          - name: CATTLE_FEATURES
            value: embedded-cluster-api=false,fleet=false,managed-system-upgrade-controller=true,multi-cluster-management=false,multi-cluster-management-agent=true,provisioningprebootstrap=false,provisioningv2=false,rke2=false,ui-sql-cache=false
          - name: CATTLE_IS_RKE
            value: "false"
          - name: CATTLE_SERVER
            value: https://44.220.0.65.sslip.io
          - name: CATTLE_CA_CHECKSUM
          - name: CATTLE_CLUSTER
            value: "true"
          - name: CATTLE_K8S_MANAGED
            value: "true"
          - name: CATTLE_CLUSTER_REGISTRY
          - name: CATTLE_SERVER_VERSION
            value: v2.10.2
          - name: CATTLE_INSTALL_UUID
            value: dd7c2187-12b9-4774-8ac3-7ac6e676ed0b
          - name: CATTLE_INGRESS_IP_DOMAIN
            value: sslip.io
          - name: STRICT_VERIFY
            value: "false"
          image: rancher/rancher-agent:v2.10.2
          imagePullPolicy: IfNotPresent
          name: cluster-register
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /cattle-credentials
            name: cattle-credentials
            readOnly: true
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: cattle
        serviceAccountName: cattle
        terminationGracePeriodSeconds: 30
        tolerations:
        - effect: NoSchedule
          key: node-role.kubernetes.io/controlplane
          value: "true"
        - effect: NoSchedule
          key: node-role.kubernetes.io/control-plane
          operator: Exists
        - effect: NoSchedule
          key: node-role.kubernetes.io/master
          operator: Exists
        volumes:
        - name: cattle-credentials
          secret:
            defaultMode: 320
            secretName: cattle-credentials-7494863
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2025-02-27T05:42:26Z"
      lastUpdateTime: "2025-02-27T05:42:56Z"
      message: ReplicaSet "cattle-cluster-agent-74975b7654" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    - lastTransitionTime: "2025-03-03T02:56:44Z"
      lastUpdateTime: "2025-03-03T02:56:44Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    observedGeneration: 3
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "1"
      meta.helm.sh/release-name: rancher-webhook
      meta.helm.sh/release-namespace: cattle-system
    creationTimestamp: "2025-02-27T05:44:06Z"
    generation: 1
    labels:
      app.kubernetes.io/managed-by: Helm
    name: rancher-webhook
    namespace: cattle-system
    resourceVersion: "22945"
    uid: f2dd2b25-faa1-46ad-ba31-472df123155c
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app: rancher-webhook
    strategy:
      rollingUpdate:
        maxSurge: 25%
        maxUnavailable: 25%
      type: RollingUpdate
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: rancher-webhook
      spec:
        containers:
        - env:
          - name: STAMP
          - name: ENABLE_MCM
            value: "false"
          - name: CATTLE_PORT
            value: "9443"
          - name: NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          image: rancher/rancher-webhook:v0.6.3
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /healthz
              port: https
              scheme: HTTPS
            periodSeconds: 5
            successThreshold: 1
            timeoutSeconds: 1
          name: rancher-webhook
          ports:
          - containerPort: 9443
            name: https
            protocol: TCP
          resources: {}
          startupProbe:
            failureThreshold: 60
            httpGet:
              path: /healthz
              port: https
              scheme: HTTPS
            periodSeconds: 5
            successThreshold: 1
            timeoutSeconds: 1
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        nodeSelector:
          kubernetes.io/os: linux
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: rancher-webhook
        serviceAccountName: rancher-webhook
        terminationGracePeriodSeconds: 30
        tolerations:
        - effect: NoSchedule
          key: cattle.io/os
          operator: Equal
          value: linux
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2025-02-27T05:44:06Z"
      lastUpdateTime: "2025-02-27T05:44:27Z"
      message: ReplicaSet "rancher-webhook-586f888bb" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    - lastTransitionTime: "2025-03-03T02:55:19Z"
      lastUpdateTime: "2025-03-03T02:55:19Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "1"
      meta.helm.sh/release-name: system-upgrade-controller
      meta.helm.sh/release-namespace: cattle-system
    creationTimestamp: "2025-02-27T05:44:38Z"
    generation: 1
    labels:
      app.kubernetes.io/managed-by: Helm
    name: system-upgrade-controller
    namespace: cattle-system
    resourceVersion: "22888"
    uid: 4020206c-d22a-4310-86f9-91f617e3e2a9
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        upgrade.cattle.io/controller: system-upgrade-controller
    strategy:
      rollingUpdate:
        maxSurge: 25%
        maxUnavailable: 25%
      type: RollingUpdate
    template:
      metadata:
        creationTimestamp: null
        labels:
          upgrade.cattle.io/controller: system-upgrade-controller
      spec:
        affinity:
          nodeAffinity:
            preferredDuringSchedulingIgnoredDuringExecution:
            - preference:
                matchExpressions:
                - key: node-role.kubernetes.io/control-plane
                  operator: In
                  values:
                  - "true"
              weight: 100
            - preference:
                matchExpressions:
                - key: node-role.kubernetes.io/master
                  operator: In
                  values:
                  - "true"
              weight: 100
            requiredDuringSchedulingIgnoredDuringExecution:
              nodeSelectorTerms:
              - matchExpressions:
                - key: kubernetes.io/os
                  operator: NotIn
                  values:
                  - windows
        containers:
        - env:
          - name: SYSTEM_UPGRADE_CONTROLLER_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.labels['upgrade.cattle.io/controller']
          - name: SYSTEM_UPGRADE_CONTROLLER_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          envFrom:
          - configMapRef:
              name: system-upgrade-controller-config
          image: rancher/system-upgrade-controller:v0.14.2
          imagePullPolicy: IfNotPresent
          name: system-upgrade-controller
          resources: {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            runAsGroup: 65534
            runAsNonRoot: true
            runAsUser: 65534
            seccompProfile:
              type: RuntimeDefault
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/ssl
            name: etc-ssl
            readOnly: true
          - mountPath: /etc/pki
            name: etc-pki
            readOnly: true
          - mountPath: /etc/ca-certificates
            name: etc-ca-certificates
            readOnly: true
          - mountPath: /tmp
            name: tmp
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: system-upgrade-controller
        serviceAccountName: system-upgrade-controller
        terminationGracePeriodSeconds: 30
        tolerations:
        - operator: Exists
        volumes:
        - hostPath:
            path: /etc/ssl
            type: DirectoryOrCreate
          name: etc-ssl
        - hostPath:
            path: /etc/pki
            type: DirectoryOrCreate
          name: etc-pki
        - hostPath:
            path: /etc/ca-certificates
            type: DirectoryOrCreate
          name: etc-ca-certificates
        - emptyDir: {}
          name: tmp
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2025-02-27T05:44:38Z"
      lastUpdateTime: "2025-02-27T05:44:41Z"
      message: ReplicaSet "system-upgrade-controller-5fb67f585d" has successfully
        progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    - lastTransitionTime: "2025-03-03T02:55:09Z"
      lastUpdateTime: "2025-03-03T02:55:09Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "1"
      objectset.rio.cattle.io/applied: H4sIAAAAAAAA/6xVQW/bOBP9Kx/mLMVW0jaugO/QjbPboq3XqJNeiqCgqZHFNcXhkiMnRqD/vhhJduw2TdrFniyTb4ZvHucN70F58xlDNOQgB+V9HG0ySGBtXAE5TNFb2tboGBKokVWhWEF+D8o5YsWGXJS/tPwLNUfkk2DoRCtmiyeGRkaSQPLDfbp1GNLVZg05rM/iwc4mS/733rji/2+KgtyzKZyqEXLQFLBw8afg0SstMetmiWncRsYa2gSsWqLtilpPYqq830H6vPIZHDJGyTYce0EBp7PFE8dWKlaQw1Lj6eTs9PVkkmXnL87U+GzySi1fZuPytHx1juX5i9MXY/3yXIh8V9ITpKNHLZQDbozc5VsTmcL2g6kNQz5OIKJFzRQEVCvW1YenymwlJQfFuNp2acla41bXvlCMfYq7a6c2yli1tAh51ibAWy/MPh1hZR1rb3dxBy30pNDtQVGaHCvjMETIv9yDCiv5gFSTKyGBEbIeDSqN5CZKYxFuEjC1WgmjoJyuMIxqE4LA0gG8+82zk+z0ZAxDxLyxdk7W6C3k8K6cEc8Dxt4C1mzQYYzzQMuuoFIZ2wS8qgLGimwB+VkCFbP/A1n2vWK591GFynIFCXgKDPlkPJFL0RV2d/z26mouUhln2Cg7Rau2C9Tkigj5q3ECHoOhYr+USXCjNcZ4cHKWAJsaqeEH4GN9JBR6KffKzjtWL8/26AEZiEmThRyup8LwmZCUtT8Ou7p4NOx1dhBYIwej4yOBNwkEVIX5V5JL5PZB8WyS/azi3wt++gt6B4zUBI1da1txYOxbv6YgLZWdjz8a6IB/Nxj7Xe0b2RqP627QDtAeKVZA3QTD2wtyjHddmcpaup0HszEWV3gZtbLdPIa8VDZiAlp5tTTWsOmpqKIQ28wur77+9m42/bq4/PT53cWlOKUI5GVPWQs3bS/6n85uPxHx78biMGhyDg22CWzINjV+pMYNfVTL53zQ/cCOcNB9rjSrtI+EhxN2OX+cY6SbyFQfpOr+p89kvJHmKVzcO3mKpWqsmNhRgYuDeXg80ilCDta45k7uyAdDnfBWxTjrCfRqpNo2kTGkOhg2WlmQawobo/GN1lLM7FvjMVkMu0fzyz2sUYhdDPHdQxe7EhIgL0jhB5d3RppENMKyRM2Qw4wWusKisVJ5n0aqSgNZPDmuR5wXyKbeKof/aeZaSf2Pp7yRaj1ZWm0XXq7mgpy8KGbXMt30X/zyq1Sru8Uab3vzDQe871gec6soctcvCdxW6K5dVGxiafrnCqY0I94XKmz7PtqPxdKsPiovRAxjfXRduxcm2U2a/YoI2YNmVOBbEiX2qIclOe6bodz+wCjD6HxgcxyX7r1BXtpK2b1HnzJLe9O2bftPAAAA//8kyNqv/AkAAA
      objectset.rio.cattle.io/id: ""
      objectset.rio.cattle.io/owner-gvk: k3s.cattle.io/v1, Kind=Addon
      objectset.rio.cattle.io/owner-name: coredns
      objectset.rio.cattle.io/owner-namespace: kube-system
    creationTimestamp: "2025-02-27T05:22:10Z"
    generation: 1
    labels:
      k8s-app: kube-dns
      kubernetes.io/name: CoreDNS
      objectset.rio.cattle.io/hash: bce283298811743a0386ab510f2f67ef74240c57
    name: coredns
    namespace: kube-system
    resourceVersion: "3257"
    uid: 73c8bc90-9022-44ec-a186-75a16e105147
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 0
    selector:
      matchLabels:
        k8s-app: kube-dns
    strategy:
      rollingUpdate:
        maxSurge: 25%
        maxUnavailable: 1
      type: RollingUpdate
    template:
      metadata:
        creationTimestamp: null
        labels:
          k8s-app: kube-dns
      spec:
        containers:
        - args:
          - -conf
          - /etc/coredns/Corefile
          image: rancher/mirrored-coredns-coredns:1.12.0
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /health
              port: 8080
              scheme: HTTP
            initialDelaySeconds: 60
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          name: coredns
          ports:
          - containerPort: 53
            name: dns
            protocol: UDP
          - containerPort: 53
            name: dns-tcp
            protocol: TCP
          - containerPort: 9153
            name: metrics
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /ready
              port: 8181
              scheme: HTTP
            periodSeconds: 2
            successThreshold: 1
            timeoutSeconds: 1
          resources:
            limits:
              memory: 170Mi
            requests:
              cpu: 100m
              memory: 70Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              add:
              - NET_BIND_SERVICE
              drop:
              - all
            readOnlyRootFilesystem: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/coredns
            name: config-volume
            readOnly: true
          - mountPath: /etc/coredns/custom
            name: custom-config-volume
            readOnly: true
        dnsPolicy: Default
        nodeSelector:
          kubernetes.io/os: linux
        priorityClassName: system-cluster-critical
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: coredns
        serviceAccountName: coredns
        terminationGracePeriodSeconds: 30
        tolerations:
        - key: CriticalAddonsOnly
          operator: Exists
        - effect: NoSchedule
          key: node-role.kubernetes.io/control-plane
          operator: Exists
        - effect: NoSchedule
          key: node-role.kubernetes.io/master
          operator: Exists
        topologySpreadConstraints:
        - labelSelector:
            matchLabels:
              k8s-app: kube-dns
          maxSkew: 1
          topologyKey: kubernetes.io/hostname
          whenUnsatisfiable: DoNotSchedule
        volumes:
        - configMap:
            defaultMode: 420
            items:
            - key: Corefile
              path: Corefile
            - key: NodeHosts
              path: NodeHosts
            name: coredns
          name: config-volume
        - configMap:
            defaultMode: 420
            name: coredns-custom
            optional: true
          name: custom-config-volume
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2025-02-27T05:22:13Z"
      lastUpdateTime: "2025-02-27T05:22:13Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    - lastTransitionTime: "2025-02-27T05:22:13Z"
      lastUpdateTime: "2025-02-27T05:22:31Z"
      message: ReplicaSet "coredns-ccb96694c" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "1"
      objectset.rio.cattle.io/applied: H4sIAAAAAAAA/6xUXWvrRhD9K2WeJdmuE2MEfTBJSksTxySkL8GU8Wpkb7zaXXbHaoTRf7+MJeeDGyf3wn3SfsycPTPnjPaAXv9LIWpnIQf0Pg7qESSw1baAHC7JG9dUZBkSqIixQEbI94DWOkbWzkbZutUTKY7EWdAuU8hsKNNuoAUEkpP37n9LIV3XW8hhO45vbupR8ts/2hZ/zIrC2S8hLFYEORin0KSRXcA1/VBS9Kgkc7tbURqbyFRBm4DBFZlPS9tg3EAOo+m4HJ+ryXlZrtR4ODmbDMfl2bgcnU+HxVRNpvh7gaviTEDfkfTIm9QHV2tpPgXo7k/wiZ6UsAnUxf+lpcjmWleaIR8mEMmQYhckqEJWm+uXCtD706+2As4BmdbN4QFnjLbrB18gUwf2/GCxRm1wZQjyUZsAN1443r2LlXOqvDnmvXGL+QkufaHKWUZtKUTIH2VbVSiWfDzdvsgYxKdpqpwt9RoSGBCrQbfrP9lTdBaWCZCtD8i9KIvby//ms5ur+8Xs4goSqNHs6M/gKiFTajLFHZUv6wWyiH+sMXtVrm3bZQK6Ev/lENCqDYXBx5zzepgNs/EQ+oTFzpiFM1o1kMPf5dzxIlDshu8r79TO7Cq6cTvLXccqWfY837bhFas7SLtMaJdC3AftgubmwmCM8y6uc2FqXUGpCpq1QiPtplBrRTOl5KX5Z/zSPjbFLhgSYGcoHH8gj3vYkhR90cMfhj7eWtPIEHuJFGvD1bOOHKFN9kBlSYohh7m7VxsqdkYGvoM5UA3OUCZjFCwxRZlZMVVwJvUGLf1S5AojH3T4AHJ5VOdoZWn7DXpx0/ey9t5tT8vUtu23AAAA///s6eu+uAUAAA
      objectset.rio.cattle.io/id: ""
      objectset.rio.cattle.io/owner-gvk: k3s.cattle.io/v1, Kind=Addon
      objectset.rio.cattle.io/owner-name: local-storage
      objectset.rio.cattle.io/owner-namespace: kube-system
    creationTimestamp: "2025-02-27T05:22:10Z"
    generation: 1
    labels:
      objectset.rio.cattle.io/hash: 183f35c65ffbc3064603f43f1580d8c68a2dabd4
    name: local-path-provisioner
    namespace: kube-system
    resourceVersion: "10285"
    uid: 5d0948ef-f84b-4518-aecd-867213b46a02
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 0
    selector:
      matchLabels:
        app: local-path-provisioner
    strategy:
      rollingUpdate:
        maxSurge: 25%
        maxUnavailable: 1
      type: RollingUpdate
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: local-path-provisioner
      spec:
        containers:
        - command:
          - local-path-provisioner
          - start
          - --config
          - /etc/config/config.json
          env:
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          image: rancher/local-path-provisioner:v0.0.30
          imagePullPolicy: IfNotPresent
          name: local-path-provisioner
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/config/
            name: config-volume
        dnsPolicy: ClusterFirst
        priorityClassName: system-node-critical
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: local-path-provisioner-service-account
        serviceAccountName: local-path-provisioner-service-account
        terminationGracePeriodSeconds: 30
        tolerations:
        - key: CriticalAddonsOnly
          operator: Exists
        - effect: NoSchedule
          key: node-role.kubernetes.io/control-plane
          operator: Exists
        - effect: NoSchedule
          key: node-role.kubernetes.io/master
          operator: Exists
        volumes:
        - configMap:
            defaultMode: 420
            name: local-path-config
          name: config-volume
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2025-02-27T05:22:13Z"
      lastUpdateTime: "2025-02-27T05:22:13Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    - lastTransitionTime: "2025-02-27T05:22:13Z"
      lastUpdateTime: "2025-02-27T05:22:26Z"
      message: ReplicaSet "local-path-provisioner-5cf85fd84d" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "1"
      objectset.rio.cattle.io/applied: H4sIAAAAAAAA/6xV4W8auRP9V36az7vJLoHQrsQHRPiV6tIUFXqnUxUh4x3Ah9f2eWZpuIj//TS7SUovpelV9wUt9vPzmzfjmXtQwfyKkYx3UIAKgc53OSSwNa6EAq4wWL+v0DEkUCGrUrGC4h6Uc54VG+9I/vrlH6iZkM+i8WdaMVs8M/7cCAkkJ/f9Z4cxXe+2UMD2go52dnnyv1+MKwfDsvTuRQqnKoRCJEajKSWMO4xpeSz/ZQIKSgvLtl5iSntirOCQgFVLtE2Y21eUqhCeXfQd9o2iDRSAeYadbg/zC+xeqn6np1+XZVl2V69x2bl4vcq7/X6/uyrlvm/GAu36CYkUUIvAiDsjuZwYYh/316YyDEWWAKFFzT4KqFKsN9cvB3UQYo6Kcb1vyL21xq0/hlIxtkR3H53aKWPV0iIU+SEB3gfR9+ErrKxjFezjuaNC+gFzT1pyFLj2jpVxGAmKT/eg4lo+IE01Rk5LEwfnXAVIIE0JdR0xDT7yIM86vaxZFUMtchoirjBGLFNVlhGJUomIBm8dY3TKvp0m47unz4knbrQdU9SEqfMlpsSKa2puagCt/DQieVvL2xnkPWp22FKqTdhgTKk2jDSYX88W49HVZCy/s+Hit7fzyWI4ni06vcvFm9G7xWwyvHjVTb7gPvwQ6h9seefVI67TuzzFdhJ1xDaaDEeTYSdbTN9f/55fZL1vkT0DwW0CplJryW5UTm8wnlcmRi8Z+DrdxS476591IAFrduiQaBr9simolTK2jjjfRKSNtyUUFwlsmMMbZNkPiuURnsvBvyCBJiNFgxD/SW+wqa/JfD6dSVkZZ9goe4VW7WeovSsJisssgYDR+PJpKZenVWuNREeX5wmwqdDX/AX4nXctatqyfariaSOwqc6nc49qQ/TstbdQwHw0hcNtAhFVaX7KETm5/3lLnjvS+ReGyEOoo0ZqW9efNRI33zrUUECeZVUzdiof91BAP3tn2qYkL9jwfuQd410Tj7LWf55GszMW1zgmrWwznaBYKUvYWvTe2f0H7/n/xuJD7yw41rJbuyHdeCe7X619JIySiCw7JLDztq7wna/dQ74q+Zw+WNn2l4dkcRWk68DhVvITovGNYKuIblpEK6BtFDoaNlpZMR7jzmgcai3cNydKhr3F+Dh+P93DFsWg0QNNMzJJopXBFAQpnR/Gd0YMPiT3gKsVakn4jZ/pDZa1lR7W0jSSord4Jh0tOmQkGWVSndHbNFjl8D9lrhRxO0WfU94++t5GilXg/ZWRQXb4ltuHw+HvAAAA//9PFN5y1QgAAA
      objectset.rio.cattle.io/id: ""
      objectset.rio.cattle.io/owner-gvk: k3s.cattle.io/v1, Kind=Addon
      objectset.rio.cattle.io/owner-name: metrics-server-deployment
      objectset.rio.cattle.io/owner-namespace: kube-system
    creationTimestamp: "2025-02-27T05:22:10Z"
    generation: 1
    labels:
      k8s-app: metrics-server
      objectset.rio.cattle.io/hash: e10e245e13e46a725c9dddd4f9eb239f147774fd
    name: metrics-server
    namespace: kube-system
    resourceVersion: "22953"
    uid: 7fc4861b-e81a-4f94-bd7d-4162c5e94576
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 0
    selector:
      matchLabels:
        k8s-app: metrics-server
    strategy:
      rollingUpdate:
        maxSurge: 25%
        maxUnavailable: 1
      type: RollingUpdate
    template:
      metadata:
        creationTimestamp: null
        labels:
          k8s-app: metrics-server
        name: metrics-server
      spec:
        containers:
        - args:
          - --cert-dir=/tmp
          - --secure-port=10250
          - --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname
          - --kubelet-use-node-status-port
          - --metric-resolution=15s
          - --tls-cipher-suites=TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305
          image: rancher/mirrored-metrics-server:v0.7.2
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /livez
              port: https
              scheme: HTTPS
            initialDelaySeconds: 60
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          name: metrics-server
          ports:
          - containerPort: 10250
            name: https
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /readyz
              port: https
              scheme: HTTPS
            periodSeconds: 2
            successThreshold: 1
            timeoutSeconds: 1
          resources:
            requests:
              cpu: 100m
              memory: 70Mi
          securityContext:
            allowPrivilegeEscalation: false
            readOnlyRootFilesystem: true
            runAsNonRoot: true
            runAsUser: 1000
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /tmp
            name: tmp-dir
        dnsPolicy: ClusterFirst
        priorityClassName: system-node-critical
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: metrics-server
        serviceAccountName: metrics-server
        terminationGracePeriodSeconds: 30
        tolerations:
        - key: CriticalAddonsOnly
          operator: Exists
        - effect: NoSchedule
          key: node-role.kubernetes.io/control-plane
          operator: Exists
        - effect: NoSchedule
          key: node-role.kubernetes.io/master
          operator: Exists
        volumes:
        - emptyDir: {}
          name: tmp-dir
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2025-02-27T05:22:13Z"
      lastUpdateTime: "2025-02-27T05:22:13Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    - lastTransitionTime: "2025-02-27T05:22:13Z"
      lastUpdateTime: "2025-02-27T05:22:46Z"
      message: ReplicaSet "metrics-server-5985cbc9d7" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "1"
      field.cattle.io/publicEndpoints: '[{"addresses":["172.31.91.35"],"port":80,"protocol":"TCP","serviceName":"kube-system:traefik","allNodes":false},{"addresses":["172.31.91.35"],"port":443,"protocol":"TCP","serviceName":"kube-system:traefik","allNodes":false}]'
      meta.helm.sh/release-name: traefik
      meta.helm.sh/release-namespace: kube-system
    creationTimestamp: "2025-02-27T05:22:56Z"
    generation: 2
    labels:
      app.kubernetes.io/instance: traefik-kube-system
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: traefik
      helm.sh/chart: traefik-27.0.201_up27.0.2
    name: traefik
    namespace: kube-system
    resourceVersion: "22894"
    uid: 0c52fb42-fbde-4e68-b715-1ebf78c01cc5
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app.kubernetes.io/instance: traefik-kube-system
        app.kubernetes.io/name: traefik
    strategy:
      rollingUpdate:
        maxSurge: 1
        maxUnavailable: 0
      type: RollingUpdate
    template:
      metadata:
        annotations:
          prometheus.io/path: /metrics
          prometheus.io/port: "9100"
          prometheus.io/scrape: "true"
        creationTimestamp: null
        labels:
          app.kubernetes.io/instance: traefik-kube-system
          app.kubernetes.io/managed-by: Helm
          app.kubernetes.io/name: traefik
          helm.sh/chart: traefik-27.0.201_up27.0.2
      spec:
        containers:
        - args:
          - --global.checknewversion
          - --global.sendanonymoususage
          - --entrypoints.metrics.address=:9100/tcp
          - --entrypoints.traefik.address=:9000/tcp
          - --entrypoints.web.address=:8000/tcp
          - --entrypoints.websecure.address=:8443/tcp
          - --api.dashboard=true
          - --ping=true
          - --metrics.prometheus=true
          - --metrics.prometheus.entrypoint=metrics
          - --providers.kubernetescrd
          - --providers.kubernetesingress
          - --providers.kubernetesingress.ingressendpoint.publishedservice=kube-system/traefik
          - --entrypoints.websecure.http.tls=true
          env:
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          image: rancher/mirrored-library-traefik:2.11.18
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /ping
              port: 9000
              scheme: HTTP
            initialDelaySeconds: 2
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 2
          name: traefik
          ports:
          - containerPort: 9100
            name: metrics
            protocol: TCP
          - containerPort: 9000
            name: traefik
            protocol: TCP
          - containerPort: 8000
            name: web
            protocol: TCP
          - containerPort: 8443
            name: websecure
            protocol: TCP
          readinessProbe:
            failureThreshold: 1
            httpGet:
              path: /ping
              port: 9000
              scheme: HTTP
            initialDelaySeconds: 2
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 2
          resources: {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /data
            name: data
          - mountPath: /tmp
            name: tmp
        dnsPolicy: ClusterFirst
        priorityClassName: system-cluster-critical
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroupChangePolicy: OnRootMismatch
          runAsGroup: 65532
          runAsNonRoot: true
          runAsUser: 65532
        serviceAccount: traefik
        serviceAccountName: traefik
        terminationGracePeriodSeconds: 60
        tolerations:
        - key: CriticalAddonsOnly
          operator: Exists
        - effect: NoSchedule
          key: node-role.kubernetes.io/control-plane
          operator: Exists
        - effect: NoSchedule
          key: node-role.kubernetes.io/master
          operator: Exists
        volumes:
        - emptyDir: {}
          name: data
        - emptyDir: {}
          name: tmp
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2025-02-27T05:22:56Z"
      lastUpdateTime: "2025-02-27T05:23:05Z"
      message: ReplicaSet "traefik-5d45fc8cc9" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    - lastTransitionTime: "2025-03-03T02:55:10Z"
      lastUpdateTime: "2025-03-03T02:55:10Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    observedGeneration: 2
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "1"
      meta.helm.sh/release-name: prometheus
      meta.helm.sh/release-namespace: monitoring
    creationTimestamp: "2025-02-27T05:45:42Z"
    generation: 1
    labels:
      app.kubernetes.io/component: metrics
      app.kubernetes.io/instance: prometheus
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: kube-state-metrics
      app.kubernetes.io/part-of: kube-state-metrics
      app.kubernetes.io/version: 2.15.0
      helm.sh/chart: kube-state-metrics-5.30.0
    name: prometheus-kube-state-metrics
    namespace: monitoring
    resourceVersion: "22874"
    uid: aaac3025-9269-4835-a4de-d47d8dac5307
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app.kubernetes.io/instance: prometheus
        app.kubernetes.io/name: kube-state-metrics
    strategy:
      rollingUpdate:
        maxSurge: 25%
        maxUnavailable: 25%
      type: RollingUpdate
    template:
      metadata:
        creationTimestamp: null
        labels:
          app.kubernetes.io/component: metrics
          app.kubernetes.io/instance: prometheus
          app.kubernetes.io/managed-by: Helm
          app.kubernetes.io/name: kube-state-metrics
          app.kubernetes.io/part-of: kube-state-metrics
          app.kubernetes.io/version: 2.15.0
          helm.sh/chart: kube-state-metrics-5.30.0
      spec:
        automountServiceAccountToken: true
        containers:
        - args:
          - --port=8080
          - --resources=certificatesigningrequests,configmaps,cronjobs,daemonsets,deployments,endpoints,horizontalpodautoscalers,ingresses,jobs,leases,limitranges,mutatingwebhookconfigurations,namespaces,networkpolicies,nodes,persistentvolumeclaims,persistentvolumes,poddisruptionbudgets,pods,replicasets,replicationcontrollers,resourcequotas,secrets,services,statefulsets,storageclasses,validatingwebhookconfigurations,volumeattachments
          image: registry.k8s.io/kube-state-metrics/kube-state-metrics:v2.15.0
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /livez
              port: 8080
              scheme: HTTP
            initialDelaySeconds: 5
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          name: kube-state-metrics
          ports:
          - containerPort: 8080
            name: http
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /readyz
              port: 8081
              scheme: HTTP
            initialDelaySeconds: 5
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          resources: {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 65534
          runAsGroup: 65534
          runAsNonRoot: true
          runAsUser: 65534
          seccompProfile:
            type: RuntimeDefault
        serviceAccount: prometheus-kube-state-metrics
        serviceAccountName: prometheus-kube-state-metrics
        terminationGracePeriodSeconds: 30
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2025-02-27T05:45:43Z"
      lastUpdateTime: "2025-02-27T05:45:53Z"
      message: ReplicaSet "prometheus-kube-state-metrics-65846b5c64" has successfully
        progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    - lastTransitionTime: "2025-03-03T02:55:09Z"
      lastUpdateTime: "2025-03-03T02:55:09Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "1"
      meta.helm.sh/release-name: prometheus
      meta.helm.sh/release-namespace: monitoring
    creationTimestamp: "2025-02-27T05:45:42Z"
    generation: 1
    labels:
      app.kubernetes.io/instance: prometheus
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: prometheus-pushgateway
      app.kubernetes.io/version: v1.11.0
      helm.sh/chart: prometheus-pushgateway-3.0.0
    name: prometheus-prometheus-pushgateway
    namespace: monitoring
    resourceVersion: "22938"
    uid: 79064954-6afb-4220-8c7e-a8e106fb810f
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app.kubernetes.io/instance: prometheus
        app.kubernetes.io/name: prometheus-pushgateway
    strategy:
      type: Recreate
    template:
      metadata:
        creationTimestamp: null
        labels:
          app.kubernetes.io/instance: prometheus
          app.kubernetes.io/managed-by: Helm
          app.kubernetes.io/name: prometheus-pushgateway
          app.kubernetes.io/version: v1.11.0
          helm.sh/chart: prometheus-pushgateway-3.0.0
      spec:
        automountServiceAccountToken: true
        containers:
        - image: quay.io/prometheus/pushgateway:v1.11.0
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /-/healthy
              port: 9091
              scheme: HTTP
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 10
          name: pushgateway
          ports:
          - containerPort: 9091
            name: metrics
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /-/ready
              port: 9091
              scheme: HTTP
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 10
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /data
            name: storage-volume
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 65534
          runAsNonRoot: true
          runAsUser: 65534
        serviceAccount: prometheus-prometheus-pushgateway
        serviceAccountName: prometheus-prometheus-pushgateway
        terminationGracePeriodSeconds: 30
        volumes:
        - emptyDir: {}
          name: storage-volume
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2025-02-27T05:45:43Z"
      lastUpdateTime: "2025-02-27T05:46:03Z"
      message: ReplicaSet "prometheus-prometheus-pushgateway-64bc8bff6f" has successfully
        progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    - lastTransitionTime: "2025-03-03T02:55:17Z"
      lastUpdateTime: "2025-03-03T02:55:17Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "1"
      field.cattle.io/publicEndpoints: '[{"port":30882,"protocol":"TCP","serviceName":"monitoring:prometheus-server","allNodes":true}]'
      meta.helm.sh/release-name: prometheus
      meta.helm.sh/release-namespace: monitoring
    creationTimestamp: "2025-02-27T05:45:42Z"
    generation: 2
    labels:
      app.kubernetes.io/component: server
      app.kubernetes.io/instance: prometheus
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: prometheus
      app.kubernetes.io/part-of: prometheus
      app.kubernetes.io/version: v3.2.0
      helm.sh/chart: prometheus-27.5.0
    name: prometheus-server
    namespace: monitoring
    resourceVersion: "22977"
    uid: d3b047f6-8056-44f5-b59d-2b0c4b439944
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app.kubernetes.io/component: server
        app.kubernetes.io/instance: prometheus
        app.kubernetes.io/name: prometheus
    strategy:
      type: Recreate
    template:
      metadata:
        creationTimestamp: null
        labels:
          app.kubernetes.io/component: server
          app.kubernetes.io/instance: prometheus
          app.kubernetes.io/name: prometheus
          app.kubernetes.io/part-of: prometheus
          app.kubernetes.io/version: v3.2.0
          helm.sh/chart: prometheus-27.5.0
      spec:
        containers:
        - args:
          - --watched-dir=/etc/config
          - --listen-address=0.0.0.0:8080
          - --reload-url=http://127.0.0.1:9090/-/reload
          image: quay.io/prometheus-operator/prometheus-config-reloader:v0.80.1
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /healthz
              port: metrics
              scheme: HTTP
            initialDelaySeconds: 2
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          name: prometheus-server-configmap-reload
          ports:
          - containerPort: 8080
            name: metrics
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /healthz
              port: metrics
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/config
            name: config-volume
            readOnly: true
        - args:
          - --storage.tsdb.retention.time=15d
          - --config.file=/etc/config/prometheus.yml
          - --storage.tsdb.path=/data
          - --web.console.libraries=/etc/prometheus/console_libraries
          - --web.console.templates=/etc/prometheus/consoles
          - --web.enable-lifecycle
          image: quay.io/prometheus/prometheus:v3.2.0
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /-/healthy
              port: 9090
              scheme: HTTP
            initialDelaySeconds: 30
            periodSeconds: 15
            successThreshold: 1
            timeoutSeconds: 10
          name: prometheus-server
          ports:
          - containerPort: 9090
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /-/ready
              port: 9090
              scheme: HTTP
            initialDelaySeconds: 30
            periodSeconds: 5
            successThreshold: 1
            timeoutSeconds: 4
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/config
            name: config-volume
          - mountPath: /data
            name: storage-volume
        dnsPolicy: ClusterFirst
        enableServiceLinks: true
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 65534
          runAsGroup: 65534
          runAsNonRoot: true
          runAsUser: 65534
        serviceAccount: prometheus-server
        serviceAccountName: prometheus-server
        terminationGracePeriodSeconds: 300
        volumes:
        - configMap:
            defaultMode: 420
            name: prometheus-server
          name: config-volume
        - name: storage-volume
          persistentVolumeClaim:
            claimName: prometheus-server
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2025-02-27T05:45:43Z"
      lastUpdateTime: "2025-02-27T05:46:28Z"
      message: ReplicaSet "prometheus-server-847fc44949" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    - lastTransitionTime: "2025-03-03T02:55:36Z"
      lastUpdateTime: "2025-03-03T02:55:36Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    observedGeneration: 2
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "1"
      field.cattle.io/publicEndpoints: '[{"port":30080,"protocol":"TCP","serviceName":"monitoring:webhook-service","allNodes":true}]'
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"apps/v1","kind":"Deployment","metadata":{"annotations":{},"name":"webhook-deployment","namespace":"monitoring"},"spec":{"replicas":1,"selector":{"matchLabels":{"app":"webhook"}},"template":{"metadata":{"labels":{"app":"webhook"}},"spec":{"containers":[{"image":"gnh374/webhook-express:latest","name":"webhook","ports":[{"containerPort":3000}]}]}}}}
    creationTimestamp: "2025-03-02T08:10:23Z"
    generation: 2
    name: webhook-deployment
    namespace: monitoring
    resourceVersion: "12662"
    uid: e5feae53-2671-4943-8714-7be74e79cec3
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app: webhook
    strategy:
      rollingUpdate:
        maxSurge: 25%
        maxUnavailable: 25%
      type: RollingUpdate
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: webhook
      spec:
        containers:
        - image: gnh374/webhook-express:latest
          imagePullPolicy: Always
          name: webhook
          ports:
          - containerPort: 3000
            protocol: TCP
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2025-03-02T08:10:45Z"
      lastUpdateTime: "2025-03-02T08:10:45Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    - lastTransitionTime: "2025-03-02T08:10:23Z"
      lastUpdateTime: "2025-03-02T08:10:45Z"
      message: ReplicaSet "webhook-deployment-64b5cc8c76" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    observedGeneration: 2
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "1"
    creationTimestamp: "2025-03-02T07:49:24Z"
    generation: 1
    labels:
      component: velero
    name: velero
    namespace: velero
    resourceVersion: "22878"
    uid: 9f7c236e-4de8-4339-a334-07ea5af96bdd
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        deploy: velero
    strategy:
      rollingUpdate:
        maxSurge: 25%
        maxUnavailable: 25%
      type: RollingUpdate
    template:
      metadata:
        annotations:
          prometheus.io/path: /metrics
          prometheus.io/port: "8085"
          prometheus.io/scrape: "true"
        creationTimestamp: null
        labels:
          component: velero
          deploy: velero
      spec:
        containers:
        - args:
          - server
          - --features=
          - --uploader-type=kopia
          command:
          - /velero
          env:
          - name: VELERO_SCRATCH_DIR
            value: /scratch
          - name: VELERO_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          - name: LD_LIBRARY_PATH
            value: /plugins
          image: velero/velero:v1.15.2
          imagePullPolicy: IfNotPresent
          name: velero
          ports:
          - containerPort: 8085
            name: metrics
            protocol: TCP
          resources:
            limits:
              cpu: "1"
              memory: 512Mi
            requests:
              cpu: 500m
              memory: 128Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /plugins
            name: plugins
          - mountPath: /scratch
            name: scratch
        dnsPolicy: ClusterFirst
        initContainers:
        - image: velero/velero-plugin-for-aws:v1.10.0
          imagePullPolicy: IfNotPresent
          name: velero-velero-plugin-for-aws
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /target
            name: plugins
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: velero
        serviceAccountName: velero
        terminationGracePeriodSeconds: 30
        volumes:
        - emptyDir: {}
          name: plugins
        - emptyDir: {}
          name: scratch
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2025-03-02T07:49:24Z"
      lastUpdateTime: "2025-03-02T07:49:30Z"
      message: ReplicaSet "velero-57fdbd48db" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    - lastTransitionTime: "2025-03-03T02:55:09Z"
      lastUpdateTime: "2025-03-03T02:55:09Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "1"
      management.cattle.io/scale-available: "2"
    creationTimestamp: "2025-02-27T05:26:29Z"
    generation: 2
    labels:
      app: cattle-cluster-agent
      pod-template-hash: 65b7448684
    name: cattle-cluster-agent-65b7448684
    namespace: cattle-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: cattle-cluster-agent
      uid: 3dafc0ec-b4fe-4ad6-8e60-17cf6b1d78e2
    resourceVersion: "1222"
    uid: f3b2658f-06e0-4967-b2f7-8c9fa46a66de
  spec:
    replicas: 0
    selector:
      matchLabels:
        app: cattle-cluster-agent
        pod-template-hash: 65b7448684
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: cattle-cluster-agent
          pod-template-hash: 65b7448684
      spec:
        affinity:
          nodeAffinity:
            preferredDuringSchedulingIgnoredDuringExecution:
            - preference:
                matchExpressions:
                - key: node-role.kubernetes.io/controlplane
                  operator: In
                  values:
                  - "true"
              weight: 100
            - preference:
                matchExpressions:
                - key: node-role.kubernetes.io/control-plane
                  operator: In
                  values:
                  - "true"
              weight: 100
            - preference:
                matchExpressions:
                - key: node-role.kubernetes.io/master
                  operator: In
                  values:
                  - "true"
              weight: 100
            - preference:
                matchExpressions:
                - key: cattle.io/cluster-agent
                  operator: In
                  values:
                  - "true"
              weight: 1
            requiredDuringSchedulingIgnoredDuringExecution:
              nodeSelectorTerms:
              - matchExpressions:
                - key: beta.kubernetes.io/os
                  operator: NotIn
                  values:
                  - windows
          podAntiAffinity:
            preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchExpressions:
                  - key: app
                    operator: In
                    values:
                    - cattle-cluster-agent
                topologyKey: kubernetes.io/hostname
              weight: 100
        containers:
        - env:
          - name: CATTLE_IS_RKE
            value: "false"
          - name: CATTLE_SERVER
            value: https://44.220.0.65.sslip.io
          - name: CATTLE_CA_CHECKSUM
          - name: CATTLE_CLUSTER
            value: "true"
          - name: CATTLE_K8S_MANAGED
            value: "true"
          - name: CATTLE_CLUSTER_REGISTRY
          - name: CATTLE_SERVER_VERSION
            value: v2.10.2
          - name: CATTLE_INSTALL_UUID
            value: dd7c2187-12b9-4774-8ac3-7ac6e676ed0b
          - name: CATTLE_INGRESS_IP_DOMAIN
            value: sslip.io
          - name: STRICT_VERIFY
            value: "false"
          image: rancher/rancher-agent:v2.10.2
          imagePullPolicy: IfNotPresent
          name: cluster-register
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /cattle-credentials
            name: cattle-credentials
            readOnly: true
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: cattle
        serviceAccountName: cattle
        terminationGracePeriodSeconds: 30
        tolerations:
        - effect: NoSchedule
          key: node-role.kubernetes.io/controlplane
          value: "true"
        - effect: NoSchedule
          key: node-role.kubernetes.io/control-plane
          operator: Exists
        - effect: NoSchedule
          key: node-role.kubernetes.io/master
          operator: Exists
        volumes:
        - name: cattle-credentials
          secret:
            defaultMode: 320
            secretName: cattle-credentials-cf9abff
  status:
    observedGeneration: 2
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "2"
      management.cattle.io/scale-available: "2"
    creationTimestamp: "2025-02-27T05:28:12Z"
    generation: 2
    labels:
      app: cattle-cluster-agent
      pod-template-hash: 66c9954997
    name: cattle-cluster-agent-66c9954997
    namespace: cattle-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: cattle-cluster-agent
      uid: 3dafc0ec-b4fe-4ad6-8e60-17cf6b1d78e2
    resourceVersion: "1614"
    uid: fe409bb8-8a0e-44f6-8c35-8b63b463dfec
  spec:
    replicas: 0
    selector:
      matchLabels:
        app: cattle-cluster-agent
        pod-template-hash: 66c9954997
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: cattle-cluster-agent
          pod-template-hash: 66c9954997
      spec:
        affinity:
          nodeAffinity:
            preferredDuringSchedulingIgnoredDuringExecution:
            - preference:
                matchExpressions:
                - key: node-role.kubernetes.io/controlplane
                  operator: In
                  values:
                  - "true"
              weight: 100
            - preference:
                matchExpressions:
                - key: node-role.kubernetes.io/control-plane
                  operator: In
                  values:
                  - "true"
              weight: 100
            - preference:
                matchExpressions:
                - key: node-role.kubernetes.io/master
                  operator: In
                  values:
                  - "true"
              weight: 100
            - preference:
                matchExpressions:
                - key: cattle.io/cluster-agent
                  operator: In
                  values:
                  - "true"
              weight: 1
            requiredDuringSchedulingIgnoredDuringExecution:
              nodeSelectorTerms:
              - matchExpressions:
                - key: beta.kubernetes.io/os
                  operator: NotIn
                  values:
                  - windows
          podAntiAffinity:
            preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchExpressions:
                  - key: app
                    operator: In
                    values:
                    - cattle-cluster-agent
                topologyKey: kubernetes.io/hostname
              weight: 100
        containers:
        - env:
          - name: CATTLE_FEATURES
            value: embedded-cluster-api=false,fleet=false,managed-system-upgrade-controller=false,multi-cluster-management=false,multi-cluster-management-agent=true,provisioningprebootstrap=false,provisioningv2=false,rke2=false,ui-sql-cache=false
          - name: CATTLE_IS_RKE
            value: "false"
          - name: CATTLE_SERVER
            value: https://44.220.0.65.sslip.io
          - name: CATTLE_CA_CHECKSUM
          - name: CATTLE_CLUSTER
            value: "true"
          - name: CATTLE_K8S_MANAGED
            value: "true"
          - name: CATTLE_CLUSTER_REGISTRY
          - name: CATTLE_SERVER_VERSION
            value: v2.10.2
          - name: CATTLE_INSTALL_UUID
            value: dd7c2187-12b9-4774-8ac3-7ac6e676ed0b
          - name: CATTLE_INGRESS_IP_DOMAIN
            value: sslip.io
          - name: STRICT_VERIFY
            value: "false"
          image: rancher/rancher-agent:v2.10.2
          imagePullPolicy: IfNotPresent
          name: cluster-register
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /cattle-credentials
            name: cattle-credentials
            readOnly: true
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: cattle
        serviceAccountName: cattle
        terminationGracePeriodSeconds: 30
        tolerations:
        - effect: NoSchedule
          key: node-role.kubernetes.io/controlplane
          value: "true"
        - effect: NoSchedule
          key: node-role.kubernetes.io/control-plane
          operator: Exists
        - effect: NoSchedule
          key: node-role.kubernetes.io/master
          operator: Exists
        volumes:
        - name: cattle-credentials
          secret:
            defaultMode: 320
            secretName: cattle-credentials-7494863
  status:
    observedGeneration: 2
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "3"
      management.cattle.io/scale-available: "2"
    creationTimestamp: "2025-02-27T05:42:55Z"
    generation: 1
    labels:
      app: cattle-cluster-agent
      pod-template-hash: 74975b7654
    name: cattle-cluster-agent-74975b7654
    namespace: cattle-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: cattle-cluster-agent
      uid: 3dafc0ec-b4fe-4ad6-8e60-17cf6b1d78e2
    resourceVersion: "23015"
    uid: 4638bd7a-1754-431b-ade5-1e7ad697e3b6
  spec:
    replicas: 1
    selector:
      matchLabels:
        app: cattle-cluster-agent
        pod-template-hash: 74975b7654
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: cattle-cluster-agent
          pod-template-hash: 74975b7654
      spec:
        affinity:
          nodeAffinity:
            preferredDuringSchedulingIgnoredDuringExecution:
            - preference:
                matchExpressions:
                - key: node-role.kubernetes.io/controlplane
                  operator: In
                  values:
                  - "true"
              weight: 100
            - preference:
                matchExpressions:
                - key: node-role.kubernetes.io/control-plane
                  operator: In
                  values:
                  - "true"
              weight: 100
            - preference:
                matchExpressions:
                - key: node-role.kubernetes.io/master
                  operator: In
                  values:
                  - "true"
              weight: 100
            - preference:
                matchExpressions:
                - key: cattle.io/cluster-agent
                  operator: In
                  values:
                  - "true"
              weight: 1
            requiredDuringSchedulingIgnoredDuringExecution:
              nodeSelectorTerms:
              - matchExpressions:
                - key: beta.kubernetes.io/os
                  operator: NotIn
                  values:
                  - windows
          podAntiAffinity:
            preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchExpressions:
                  - key: app
                    operator: In
                    values:
                    - cattle-cluster-agent
                topologyKey: kubernetes.io/hostname
              weight: 100
        containers:
        - env:
          - name: CATTLE_FEATURES
            value: embedded-cluster-api=false,fleet=false,managed-system-upgrade-controller=true,multi-cluster-management=false,multi-cluster-management-agent=true,provisioningprebootstrap=false,provisioningv2=false,rke2=false,ui-sql-cache=false
          - name: CATTLE_IS_RKE
            value: "false"
          - name: CATTLE_SERVER
            value: https://44.220.0.65.sslip.io
          - name: CATTLE_CA_CHECKSUM
          - name: CATTLE_CLUSTER
            value: "true"
          - name: CATTLE_K8S_MANAGED
            value: "true"
          - name: CATTLE_CLUSTER_REGISTRY
          - name: CATTLE_SERVER_VERSION
            value: v2.10.2
          - name: CATTLE_INSTALL_UUID
            value: dd7c2187-12b9-4774-8ac3-7ac6e676ed0b
          - name: CATTLE_INGRESS_IP_DOMAIN
            value: sslip.io
          - name: STRICT_VERIFY
            value: "false"
          image: rancher/rancher-agent:v2.10.2
          imagePullPolicy: IfNotPresent
          name: cluster-register
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /cattle-credentials
            name: cattle-credentials
            readOnly: true
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: cattle
        serviceAccountName: cattle
        terminationGracePeriodSeconds: 30
        tolerations:
        - effect: NoSchedule
          key: node-role.kubernetes.io/controlplane
          value: "true"
        - effect: NoSchedule
          key: node-role.kubernetes.io/control-plane
          operator: Exists
        - effect: NoSchedule
          key: node-role.kubernetes.io/master
          operator: Exists
        volumes:
        - name: cattle-credentials
          secret:
            defaultMode: 320
            secretName: cattle-credentials-7494863
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "1"
      meta.helm.sh/release-name: rancher-webhook
      meta.helm.sh/release-namespace: cattle-system
    creationTimestamp: "2025-02-27T05:44:06Z"
    generation: 1
    labels:
      app: rancher-webhook
      pod-template-hash: 586f888bb
    name: rancher-webhook-586f888bb
    namespace: cattle-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: rancher-webhook
      uid: f2dd2b25-faa1-46ad-ba31-472df123155c
    resourceVersion: "22943"
    uid: f92df8b9-d137-4275-bed8-b3b0e52d9e4a
  spec:
    replicas: 1
    selector:
      matchLabels:
        app: rancher-webhook
        pod-template-hash: 586f888bb
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: rancher-webhook
          pod-template-hash: 586f888bb
      spec:
        containers:
        - env:
          - name: STAMP
          - name: ENABLE_MCM
            value: "false"
          - name: CATTLE_PORT
            value: "9443"
          - name: NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          image: rancher/rancher-webhook:v0.6.3
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /healthz
              port: https
              scheme: HTTPS
            periodSeconds: 5
            successThreshold: 1
            timeoutSeconds: 1
          name: rancher-webhook
          ports:
          - containerPort: 9443
            name: https
            protocol: TCP
          resources: {}
          startupProbe:
            failureThreshold: 60
            httpGet:
              path: /healthz
              port: https
              scheme: HTTPS
            periodSeconds: 5
            successThreshold: 1
            timeoutSeconds: 1
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        nodeSelector:
          kubernetes.io/os: linux
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: rancher-webhook
        serviceAccountName: rancher-webhook
        terminationGracePeriodSeconds: 30
        tolerations:
        - effect: NoSchedule
          key: cattle.io/os
          operator: Equal
          value: linux
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "1"
      meta.helm.sh/release-name: system-upgrade-controller
      meta.helm.sh/release-namespace: cattle-system
    creationTimestamp: "2025-02-27T05:44:38Z"
    generation: 1
    labels:
      pod-template-hash: 5fb67f585d
      upgrade.cattle.io/controller: system-upgrade-controller
    name: system-upgrade-controller-5fb67f585d
    namespace: cattle-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: system-upgrade-controller
      uid: 4020206c-d22a-4310-86f9-91f617e3e2a9
    resourceVersion: "22885"
    uid: d2e481c1-836a-4254-b48d-44782e294944
  spec:
    replicas: 1
    selector:
      matchLabels:
        pod-template-hash: 5fb67f585d
        upgrade.cattle.io/controller: system-upgrade-controller
    template:
      metadata:
        creationTimestamp: null
        labels:
          pod-template-hash: 5fb67f585d
          upgrade.cattle.io/controller: system-upgrade-controller
      spec:
        affinity:
          nodeAffinity:
            preferredDuringSchedulingIgnoredDuringExecution:
            - preference:
                matchExpressions:
                - key: node-role.kubernetes.io/control-plane
                  operator: In
                  values:
                  - "true"
              weight: 100
            - preference:
                matchExpressions:
                - key: node-role.kubernetes.io/master
                  operator: In
                  values:
                  - "true"
              weight: 100
            requiredDuringSchedulingIgnoredDuringExecution:
              nodeSelectorTerms:
              - matchExpressions:
                - key: kubernetes.io/os
                  operator: NotIn
                  values:
                  - windows
        containers:
        - env:
          - name: SYSTEM_UPGRADE_CONTROLLER_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.labels['upgrade.cattle.io/controller']
          - name: SYSTEM_UPGRADE_CONTROLLER_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          envFrom:
          - configMapRef:
              name: system-upgrade-controller-config
          image: rancher/system-upgrade-controller:v0.14.2
          imagePullPolicy: IfNotPresent
          name: system-upgrade-controller
          resources: {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            runAsGroup: 65534
            runAsNonRoot: true
            runAsUser: 65534
            seccompProfile:
              type: RuntimeDefault
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/ssl
            name: etc-ssl
            readOnly: true
          - mountPath: /etc/pki
            name: etc-pki
            readOnly: true
          - mountPath: /etc/ca-certificates
            name: etc-ca-certificates
            readOnly: true
          - mountPath: /tmp
            name: tmp
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: system-upgrade-controller
        serviceAccountName: system-upgrade-controller
        terminationGracePeriodSeconds: 30
        tolerations:
        - operator: Exists
        volumes:
        - hostPath:
            path: /etc/ssl
            type: DirectoryOrCreate
          name: etc-ssl
        - hostPath:
            path: /etc/pki
            type: DirectoryOrCreate
          name: etc-pki
        - hostPath:
            path: /etc/ca-certificates
            type: DirectoryOrCreate
          name: etc-ca-certificates
        - emptyDir: {}
          name: tmp
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "1"
      objectset.rio.cattle.io/applied: H4sIAAAAAAAA/6xVQW/bOBP9Kx/mLMVW0jaugO/QjbPboq3XqJNeiqCgqZHFNcXhkiMnRqD/vhhJduw2TdrFniyTb4ZvHucN70F58xlDNOQgB+V9HG0ySGBtXAE5TNFb2tboGBKokVWhWEF+D8o5YsWGXJS/tPwLNUfkk2DoRCtmiyeGRkaSQPLDfbp1GNLVZg05rM/iwc4mS/733rji/2+KgtyzKZyqEXLQFLBw8afg0SstMetmiWncRsYa2gSsWqLtilpPYqq830H6vPIZHDJGyTYce0EBp7PFE8dWKlaQw1Lj6eTs9PVkkmXnL87U+GzySi1fZuPytHx1juX5i9MXY/3yXIh8V9ITpKNHLZQDbozc5VsTmcL2g6kNQz5OIKJFzRQEVCvW1YenymwlJQfFuNp2acla41bXvlCMfYq7a6c2yli1tAh51ibAWy/MPh1hZR1rb3dxBy30pNDtQVGaHCvjMETIv9yDCiv5gFSTKyGBEbIeDSqN5CZKYxFuEjC1WgmjoJyuMIxqE4LA0gG8+82zk+z0ZAxDxLyxdk7W6C3k8K6cEc8Dxt4C1mzQYYzzQMuuoFIZ2wS8qgLGimwB+VkCFbP/A1n2vWK591GFynIFCXgKDPlkPJFL0RV2d/z26mouUhln2Cg7Rau2C9Tkigj5q3ECHoOhYr+USXCjNcZ4cHKWAJsaqeEH4GN9JBR6KffKzjtWL8/26AEZiEmThRyup8LwmZCUtT8Ou7p4NOx1dhBYIwej4yOBNwkEVIX5V5JL5PZB8WyS/azi3wt++gt6B4zUBI1da1txYOxbv6YgLZWdjz8a6IB/Nxj7Xe0b2RqP627QDtAeKVZA3QTD2wtyjHddmcpaup0HszEWV3gZtbLdPIa8VDZiAlp5tTTWsOmpqKIQ28wur77+9m42/bq4/PT53cWlOKUI5GVPWQs3bS/6n85uPxHx78biMGhyDg22CWzINjV+pMYNfVTL53zQ/cCOcNB9rjSrtI+EhxN2OX+cY6SbyFQfpOr+p89kvJHmKVzcO3mKpWqsmNhRgYuDeXg80ilCDta45k7uyAdDnfBWxTjrCfRqpNo2kTGkOhg2WlmQawobo/GN1lLM7FvjMVkMu0fzyz2sUYhdDPHdQxe7EhIgL0jhB5d3RppENMKyRM2Qw4wWusKisVJ5n0aqSgNZPDmuR5wXyKbeKof/aeZaSf2Pp7yRaj1ZWm0XXq7mgpy8KGbXMt30X/zyq1Sru8Uab3vzDQe871gec6soctcvCdxW6K5dVGxiafrnCqY0I94XKmz7PtqPxdKsPiovRAxjfXRduxcm2U2a/YoI2YNmVOBbEiX2qIclOe6bodz+wCjD6HxgcxyX7r1BXtpK2b1HnzJLe9O2bftPAAAA//8kyNqv/AkAAA
      objectset.rio.cattle.io/id: ""
      objectset.rio.cattle.io/owner-gvk: k3s.cattle.io/v1, Kind=Addon
      objectset.rio.cattle.io/owner-name: coredns
      objectset.rio.cattle.io/owner-namespace: kube-system
    creationTimestamp: "2025-02-27T05:22:13Z"
    generation: 1
    labels:
      k8s-app: kube-dns
      pod-template-hash: ccb96694c
    name: coredns-ccb96694c
    namespace: kube-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: coredns
      uid: 73c8bc90-9022-44ec-a186-75a16e105147
    resourceVersion: "3256"
    uid: 02df4866-c869-4ddb-b6e8-91c068b96fc9
  spec:
    replicas: 1
    selector:
      matchLabels:
        k8s-app: kube-dns
        pod-template-hash: ccb96694c
    template:
      metadata:
        creationTimestamp: null
        labels:
          k8s-app: kube-dns
          pod-template-hash: ccb96694c
      spec:
        containers:
        - args:
          - -conf
          - /etc/coredns/Corefile
          image: rancher/mirrored-coredns-coredns:1.12.0
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /health
              port: 8080
              scheme: HTTP
            initialDelaySeconds: 60
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          name: coredns
          ports:
          - containerPort: 53
            name: dns
            protocol: UDP
          - containerPort: 53
            name: dns-tcp
            protocol: TCP
          - containerPort: 9153
            name: metrics
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /ready
              port: 8181
              scheme: HTTP
            periodSeconds: 2
            successThreshold: 1
            timeoutSeconds: 1
          resources:
            limits:
              memory: 170Mi
            requests:
              cpu: 100m
              memory: 70Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              add:
              - NET_BIND_SERVICE
              drop:
              - all
            readOnlyRootFilesystem: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/coredns
            name: config-volume
            readOnly: true
          - mountPath: /etc/coredns/custom
            name: custom-config-volume
            readOnly: true
        dnsPolicy: Default
        nodeSelector:
          kubernetes.io/os: linux
        priorityClassName: system-cluster-critical
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: coredns
        serviceAccountName: coredns
        terminationGracePeriodSeconds: 30
        tolerations:
        - key: CriticalAddonsOnly
          operator: Exists
        - effect: NoSchedule
          key: node-role.kubernetes.io/control-plane
          operator: Exists
        - effect: NoSchedule
          key: node-role.kubernetes.io/master
          operator: Exists
        topologySpreadConstraints:
        - labelSelector:
            matchLabels:
              k8s-app: kube-dns
          maxSkew: 1
          topologyKey: kubernetes.io/hostname
          whenUnsatisfiable: DoNotSchedule
        volumes:
        - configMap:
            defaultMode: 420
            items:
            - key: Corefile
              path: Corefile
            - key: NodeHosts
              path: NodeHosts
            name: coredns
          name: config-volume
        - configMap:
            defaultMode: 420
            name: coredns-custom
            optional: true
          name: custom-config-volume
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "1"
      objectset.rio.cattle.io/applied: H4sIAAAAAAAA/6xUXWvrRhD9K2WeJdmuE2MEfTBJSksTxySkL8GU8Wpkb7zaXXbHaoTRf7+MJeeDGyf3wn3SfsycPTPnjPaAXv9LIWpnIQf0Pg7qESSw1baAHC7JG9dUZBkSqIixQEbI94DWOkbWzkbZutUTKY7EWdAuU8hsKNNuoAUEkpP37n9LIV3XW8hhO45vbupR8ts/2hZ/zIrC2S8hLFYEORin0KSRXcA1/VBS9Kgkc7tbURqbyFRBm4DBFZlPS9tg3EAOo+m4HJ+ryXlZrtR4ODmbDMfl2bgcnU+HxVRNpvh7gaviTEDfkfTIm9QHV2tpPgXo7k/wiZ6UsAnUxf+lpcjmWleaIR8mEMmQYhckqEJWm+uXCtD706+2As4BmdbN4QFnjLbrB18gUwf2/GCxRm1wZQjyUZsAN1443r2LlXOqvDnmvXGL+QkufaHKWUZtKUTIH2VbVSiWfDzdvsgYxKdpqpwt9RoSGBCrQbfrP9lTdBaWCZCtD8i9KIvby//ms5ur+8Xs4goSqNHs6M/gKiFTajLFHZUv6wWyiH+sMXtVrm3bZQK6Ev/lENCqDYXBx5zzepgNs/EQ+oTFzpiFM1o1kMPf5dzxIlDshu8r79TO7Cq6cTvLXccqWfY837bhFas7SLtMaJdC3AftgubmwmCM8y6uc2FqXUGpCpq1QiPtplBrRTOl5KX5Z/zSPjbFLhgSYGcoHH8gj3vYkhR90cMfhj7eWtPIEHuJFGvD1bOOHKFN9kBlSYohh7m7VxsqdkYGvoM5UA3OUCZjFCwxRZlZMVVwJvUGLf1S5AojH3T4AHJ5VOdoZWn7DXpx0/ey9t5tT8vUtu23AAAA///s6eu+uAUAAA
      objectset.rio.cattle.io/id: ""
      objectset.rio.cattle.io/owner-gvk: k3s.cattle.io/v1, Kind=Addon
      objectset.rio.cattle.io/owner-name: local-storage
      objectset.rio.cattle.io/owner-namespace: kube-system
    creationTimestamp: "2025-02-27T05:22:13Z"
    generation: 1
    labels:
      app: local-path-provisioner
      pod-template-hash: 5cf85fd84d
    name: local-path-provisioner-5cf85fd84d
    namespace: kube-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: local-path-provisioner
      uid: 5d0948ef-f84b-4518-aecd-867213b46a02
    resourceVersion: "10284"
    uid: 5cead904-ec0a-4418-8fab-e1e8874a8145
  spec:
    replicas: 1
    selector:
      matchLabels:
        app: local-path-provisioner
        pod-template-hash: 5cf85fd84d
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: local-path-provisioner
          pod-template-hash: 5cf85fd84d
      spec:
        containers:
        - command:
          - local-path-provisioner
          - start
          - --config
          - /etc/config/config.json
          env:
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          image: rancher/local-path-provisioner:v0.0.30
          imagePullPolicy: IfNotPresent
          name: local-path-provisioner
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/config/
            name: config-volume
        dnsPolicy: ClusterFirst
        priorityClassName: system-node-critical
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: local-path-provisioner-service-account
        serviceAccountName: local-path-provisioner-service-account
        terminationGracePeriodSeconds: 30
        tolerations:
        - key: CriticalAddonsOnly
          operator: Exists
        - effect: NoSchedule
          key: node-role.kubernetes.io/control-plane
          operator: Exists
        - effect: NoSchedule
          key: node-role.kubernetes.io/master
          operator: Exists
        volumes:
        - configMap:
            defaultMode: 420
            name: local-path-config
          name: config-volume
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "1"
      objectset.rio.cattle.io/applied: H4sIAAAAAAAA/6xV4W8auRP9V36az7vJLoHQrsQHRPiV6tIUFXqnUxUh4x3Ah9f2eWZpuIj//TS7SUovpelV9wUt9vPzmzfjmXtQwfyKkYx3UIAKgc53OSSwNa6EAq4wWL+v0DEkUCGrUrGC4h6Uc54VG+9I/vrlH6iZkM+i8WdaMVs8M/7cCAkkJ/f9Z4cxXe+2UMD2go52dnnyv1+MKwfDsvTuRQqnKoRCJEajKSWMO4xpeSz/ZQIKSgvLtl5iSntirOCQgFVLtE2Y21eUqhCeXfQd9o2iDRSAeYadbg/zC+xeqn6np1+XZVl2V69x2bl4vcq7/X6/uyrlvm/GAu36CYkUUIvAiDsjuZwYYh/316YyDEWWAKFFzT4KqFKsN9cvB3UQYo6Kcb1vyL21xq0/hlIxtkR3H53aKWPV0iIU+SEB3gfR9+ErrKxjFezjuaNC+gFzT1pyFLj2jpVxGAmKT/eg4lo+IE01Rk5LEwfnXAVIIE0JdR0xDT7yIM86vaxZFUMtchoirjBGLFNVlhGJUomIBm8dY3TKvp0m47unz4knbrQdU9SEqfMlpsSKa2puagCt/DQieVvL2xnkPWp22FKqTdhgTKk2jDSYX88W49HVZCy/s+Hit7fzyWI4ni06vcvFm9G7xWwyvHjVTb7gPvwQ6h9seefVI67TuzzFdhJ1xDaaDEeTYSdbTN9f/55fZL1vkT0DwW0CplJryW5UTm8wnlcmRi8Z+DrdxS476591IAFrduiQaBr9simolTK2jjjfRKSNtyUUFwlsmMMbZNkPiuURnsvBvyCBJiNFgxD/SW+wqa/JfD6dSVkZZ9goe4VW7WeovSsJisssgYDR+PJpKZenVWuNREeX5wmwqdDX/AX4nXctatqyfariaSOwqc6nc49qQ/TstbdQwHw0hcNtAhFVaX7KETm5/3lLnjvS+ReGyEOoo0ZqW9efNRI33zrUUECeZVUzdiof91BAP3tn2qYkL9jwfuQd410Tj7LWf55GszMW1zgmrWwznaBYKUvYWvTe2f0H7/n/xuJD7yw41rJbuyHdeCe7X619JIySiCw7JLDztq7wna/dQ74q+Zw+WNn2l4dkcRWk68DhVvITovGNYKuIblpEK6BtFDoaNlpZMR7jzmgcai3cNydKhr3F+Dh+P93DFsWg0QNNMzJJopXBFAQpnR/Gd0YMPiT3gKsVakn4jZ/pDZa1lR7W0jSSord4Jh0tOmQkGWVSndHbNFjl8D9lrhRxO0WfU94++t5GilXg/ZWRQXb4ltuHw+HvAAAA//9PFN5y1QgAAA
      objectset.rio.cattle.io/id: ""
      objectset.rio.cattle.io/owner-gvk: k3s.cattle.io/v1, Kind=Addon
      objectset.rio.cattle.io/owner-name: metrics-server-deployment
      objectset.rio.cattle.io/owner-namespace: kube-system
    creationTimestamp: "2025-02-27T05:22:13Z"
    generation: 1
    labels:
      k8s-app: metrics-server
      pod-template-hash: 5985cbc9d7
    name: metrics-server-5985cbc9d7
    namespace: kube-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: metrics-server
      uid: 7fc4861b-e81a-4f94-bd7d-4162c5e94576
    resourceVersion: "22951"
    uid: c648ac59-2e3f-4b68-9907-c71953d6db72
  spec:
    replicas: 1
    selector:
      matchLabels:
        k8s-app: metrics-server
        pod-template-hash: 5985cbc9d7
    template:
      metadata:
        creationTimestamp: null
        labels:
          k8s-app: metrics-server
          pod-template-hash: 5985cbc9d7
        name: metrics-server
      spec:
        containers:
        - args:
          - --cert-dir=/tmp
          - --secure-port=10250
          - --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname
          - --kubelet-use-node-status-port
          - --metric-resolution=15s
          - --tls-cipher-suites=TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305
          image: rancher/mirrored-metrics-server:v0.7.2
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /livez
              port: https
              scheme: HTTPS
            initialDelaySeconds: 60
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          name: metrics-server
          ports:
          - containerPort: 10250
            name: https
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /readyz
              port: https
              scheme: HTTPS
            periodSeconds: 2
            successThreshold: 1
            timeoutSeconds: 1
          resources:
            requests:
              cpu: 100m
              memory: 70Mi
          securityContext:
            allowPrivilegeEscalation: false
            readOnlyRootFilesystem: true
            runAsNonRoot: true
            runAsUser: 1000
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /tmp
            name: tmp-dir
        dnsPolicy: ClusterFirst
        priorityClassName: system-node-critical
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: metrics-server
        serviceAccountName: metrics-server
        terminationGracePeriodSeconds: 30
        tolerations:
        - key: CriticalAddonsOnly
          operator: Exists
        - effect: NoSchedule
          key: node-role.kubernetes.io/control-plane
          operator: Exists
        - effect: NoSchedule
          key: node-role.kubernetes.io/master
          operator: Exists
        volumes:
        - emptyDir: {}
          name: tmp-dir
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "1"
      field.cattle.io/publicEndpoints: '[{"addresses":["172.31.91.35"],"port":80,"protocol":"TCP","serviceName":"kube-system:traefik","allNodes":false},{"addresses":["172.31.91.35"],"port":443,"protocol":"TCP","serviceName":"kube-system:traefik","allNodes":false}]'
      meta.helm.sh/release-name: traefik
      meta.helm.sh/release-namespace: kube-system
    creationTimestamp: "2025-02-27T05:22:56Z"
    generation: 1
    labels:
      app.kubernetes.io/instance: traefik-kube-system
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: traefik
      helm.sh/chart: traefik-27.0.201_up27.0.2
      pod-template-hash: 5d45fc8cc9
    name: traefik-5d45fc8cc9
    namespace: kube-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: traefik
      uid: 0c52fb42-fbde-4e68-b715-1ebf78c01cc5
    resourceVersion: "22890"
    uid: bc91c3f6-9c21-4970-b44f-59dbc13d7b1b
  spec:
    replicas: 1
    selector:
      matchLabels:
        app.kubernetes.io/instance: traefik-kube-system
        app.kubernetes.io/name: traefik
        pod-template-hash: 5d45fc8cc9
    template:
      metadata:
        annotations:
          prometheus.io/path: /metrics
          prometheus.io/port: "9100"
          prometheus.io/scrape: "true"
        creationTimestamp: null
        labels:
          app.kubernetes.io/instance: traefik-kube-system
          app.kubernetes.io/managed-by: Helm
          app.kubernetes.io/name: traefik
          helm.sh/chart: traefik-27.0.201_up27.0.2
          pod-template-hash: 5d45fc8cc9
      spec:
        containers:
        - args:
          - --global.checknewversion
          - --global.sendanonymoususage
          - --entrypoints.metrics.address=:9100/tcp
          - --entrypoints.traefik.address=:9000/tcp
          - --entrypoints.web.address=:8000/tcp
          - --entrypoints.websecure.address=:8443/tcp
          - --api.dashboard=true
          - --ping=true
          - --metrics.prometheus=true
          - --metrics.prometheus.entrypoint=metrics
          - --providers.kubernetescrd
          - --providers.kubernetesingress
          - --providers.kubernetesingress.ingressendpoint.publishedservice=kube-system/traefik
          - --entrypoints.websecure.http.tls=true
          env:
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          image: rancher/mirrored-library-traefik:2.11.18
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /ping
              port: 9000
              scheme: HTTP
            initialDelaySeconds: 2
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 2
          name: traefik
          ports:
          - containerPort: 9100
            name: metrics
            protocol: TCP
          - containerPort: 9000
            name: traefik
            protocol: TCP
          - containerPort: 8000
            name: web
            protocol: TCP
          - containerPort: 8443
            name: websecure
            protocol: TCP
          readinessProbe:
            failureThreshold: 1
            httpGet:
              path: /ping
              port: 9000
              scheme: HTTP
            initialDelaySeconds: 2
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 2
          resources: {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /data
            name: data
          - mountPath: /tmp
            name: tmp
        dnsPolicy: ClusterFirst
        priorityClassName: system-cluster-critical
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroupChangePolicy: OnRootMismatch
          runAsGroup: 65532
          runAsNonRoot: true
          runAsUser: 65532
        serviceAccount: traefik
        serviceAccountName: traefik
        terminationGracePeriodSeconds: 60
        tolerations:
        - key: CriticalAddonsOnly
          operator: Exists
        - effect: NoSchedule
          key: node-role.kubernetes.io/control-plane
          operator: Exists
        - effect: NoSchedule
          key: node-role.kubernetes.io/master
          operator: Exists
        volumes:
        - emptyDir: {}
          name: data
        - emptyDir: {}
          name: tmp
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "1"
      meta.helm.sh/release-name: prometheus
      meta.helm.sh/release-namespace: monitoring
    creationTimestamp: "2025-02-27T05:45:43Z"
    generation: 1
    labels:
      app.kubernetes.io/component: metrics
      app.kubernetes.io/instance: prometheus
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: kube-state-metrics
      app.kubernetes.io/part-of: kube-state-metrics
      app.kubernetes.io/version: 2.15.0
      helm.sh/chart: kube-state-metrics-5.30.0
      pod-template-hash: 65846b5c64
    name: prometheus-kube-state-metrics-65846b5c64
    namespace: monitoring
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: prometheus-kube-state-metrics
      uid: aaac3025-9269-4835-a4de-d47d8dac5307
    resourceVersion: "22872"
    uid: 8df3a5da-62fd-45a8-8c54-7e95b0438fda
  spec:
    replicas: 1
    selector:
      matchLabels:
        app.kubernetes.io/instance: prometheus
        app.kubernetes.io/name: kube-state-metrics
        pod-template-hash: 65846b5c64
    template:
      metadata:
        creationTimestamp: null
        labels:
          app.kubernetes.io/component: metrics
          app.kubernetes.io/instance: prometheus
          app.kubernetes.io/managed-by: Helm
          app.kubernetes.io/name: kube-state-metrics
          app.kubernetes.io/part-of: kube-state-metrics
          app.kubernetes.io/version: 2.15.0
          helm.sh/chart: kube-state-metrics-5.30.0
          pod-template-hash: 65846b5c64
      spec:
        automountServiceAccountToken: true
        containers:
        - args:
          - --port=8080
          - --resources=certificatesigningrequests,configmaps,cronjobs,daemonsets,deployments,endpoints,horizontalpodautoscalers,ingresses,jobs,leases,limitranges,mutatingwebhookconfigurations,namespaces,networkpolicies,nodes,persistentvolumeclaims,persistentvolumes,poddisruptionbudgets,pods,replicasets,replicationcontrollers,resourcequotas,secrets,services,statefulsets,storageclasses,validatingwebhookconfigurations,volumeattachments
          image: registry.k8s.io/kube-state-metrics/kube-state-metrics:v2.15.0
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /livez
              port: 8080
              scheme: HTTP
            initialDelaySeconds: 5
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          name: kube-state-metrics
          ports:
          - containerPort: 8080
            name: http
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /readyz
              port: 8081
              scheme: HTTP
            initialDelaySeconds: 5
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          resources: {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 65534
          runAsGroup: 65534
          runAsNonRoot: true
          runAsUser: 65534
          seccompProfile:
            type: RuntimeDefault
        serviceAccount: prometheus-kube-state-metrics
        serviceAccountName: prometheus-kube-state-metrics
        terminationGracePeriodSeconds: 30
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "1"
      deployment.kubernetes.io/revision: "1"
      meta.helm.sh/release-name: prometheus
      meta.helm.sh/release-namespace: monitoring
    creationTimestamp: "2025-02-27T05:45:43Z"
    generation: 1
    labels:
      app.kubernetes.io/instance: prometheus
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: prometheus-pushgateway
      app.kubernetes.io/version: v1.11.0
      helm.sh/chart: prometheus-pushgateway-3.0.0
      pod-template-hash: 64bc8bff6f
    name: prometheus-prometheus-pushgateway-64bc8bff6f
    namespace: monitoring
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: prometheus-prometheus-pushgateway
      uid: 79064954-6afb-4220-8c7e-a8e106fb810f
    resourceVersion: "22936"
    uid: 23984fcf-969a-4b71-a1be-107f84e38ca2
  spec:
    replicas: 1
    selector:
      matchLabels:
        app.kubernetes.io/instance: prometheus
        app.kubernetes.io/name: prometheus-pushgateway
        pod-template-hash: 64bc8bff6f
    template:
      metadata:
        creationTimestamp: null
        labels:
          app.kubernetes.io/instance: prometheus
          app.kubernetes.io/managed-by: Helm
          app.kubernetes.io/name: prometheus-pushgateway
          app.kubernetes.io/version: v1.11.0
          helm.sh/chart: prometheus-pushgateway-3.0.0
          pod-template-hash: 64bc8bff6f
      spec:
        automountServiceAccountToken: true
        containers:
        - image: quay.io/prometheus/pushgateway:v1.11.0
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /-/healthy
              port: 9091
              scheme: HTTP
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 10
          name: pushgateway
          ports:
          - containerPort: 9091
            name: metrics
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /-/ready
              port: 9091
              scheme: HTTP
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 10
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /data
            name: storage-volume
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 65534
          runAsNonRoot: true
          runAsUser: 65534
        serviceAccount: prometheus-prometheus-pushgateway
        serviceAccountName: prometheus-prometheus-pushgateway
        terminationGracePeriodSeconds: 30
        volumes:
        - emptyDir: {}
          name: storage-volume
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "1"
      deployment.kubernetes.io/revision: "1"
      field.cattle.io/publicEndpoints: '[{"port":30882,"protocol":"TCP","serviceName":"monitoring:prometheus-server","allNodes":true}]'
      meta.helm.sh/release-name: prometheus
      meta.helm.sh/release-namespace: monitoring
    creationTimestamp: "2025-02-27T05:45:43Z"
    generation: 1
    labels:
      app.kubernetes.io/component: server
      app.kubernetes.io/instance: prometheus
      app.kubernetes.io/name: prometheus
      app.kubernetes.io/part-of: prometheus
      app.kubernetes.io/version: v3.2.0
      helm.sh/chart: prometheus-27.5.0
      pod-template-hash: 847fc44949
    name: prometheus-server-847fc44949
    namespace: monitoring
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: prometheus-server
      uid: d3b047f6-8056-44f5-b59d-2b0c4b439944
    resourceVersion: "22976"
    uid: 44195ab6-3515-455a-937f-4506cec56146
  spec:
    replicas: 1
    selector:
      matchLabels:
        app.kubernetes.io/component: server
        app.kubernetes.io/instance: prometheus
        app.kubernetes.io/name: prometheus
        pod-template-hash: 847fc44949
    template:
      metadata:
        creationTimestamp: null
        labels:
          app.kubernetes.io/component: server
          app.kubernetes.io/instance: prometheus
          app.kubernetes.io/name: prometheus
          app.kubernetes.io/part-of: prometheus
          app.kubernetes.io/version: v3.2.0
          helm.sh/chart: prometheus-27.5.0
          pod-template-hash: 847fc44949
      spec:
        containers:
        - args:
          - --watched-dir=/etc/config
          - --listen-address=0.0.0.0:8080
          - --reload-url=http://127.0.0.1:9090/-/reload
          image: quay.io/prometheus-operator/prometheus-config-reloader:v0.80.1
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /healthz
              port: metrics
              scheme: HTTP
            initialDelaySeconds: 2
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          name: prometheus-server-configmap-reload
          ports:
          - containerPort: 8080
            name: metrics
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /healthz
              port: metrics
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/config
            name: config-volume
            readOnly: true
        - args:
          - --storage.tsdb.retention.time=15d
          - --config.file=/etc/config/prometheus.yml
          - --storage.tsdb.path=/data
          - --web.console.libraries=/etc/prometheus/console_libraries
          - --web.console.templates=/etc/prometheus/consoles
          - --web.enable-lifecycle
          image: quay.io/prometheus/prometheus:v3.2.0
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /-/healthy
              port: 9090
              scheme: HTTP
            initialDelaySeconds: 30
            periodSeconds: 15
            successThreshold: 1
            timeoutSeconds: 10
          name: prometheus-server
          ports:
          - containerPort: 9090
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /-/ready
              port: 9090
              scheme: HTTP
            initialDelaySeconds: 30
            periodSeconds: 5
            successThreshold: 1
            timeoutSeconds: 4
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/config
            name: config-volume
          - mountPath: /data
            name: storage-volume
        dnsPolicy: ClusterFirst
        enableServiceLinks: true
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 65534
          runAsGroup: 65534
          runAsNonRoot: true
          runAsUser: 65534
        serviceAccount: prometheus-server
        serviceAccountName: prometheus-server
        terminationGracePeriodSeconds: 300
        volumes:
        - configMap:
            defaultMode: 420
            name: prometheus-server
          name: config-volume
        - name: storage-volume
          persistentVolumeClaim:
            claimName: prometheus-server
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "1"
      field.cattle.io/publicEndpoints: '[{"port":30080,"protocol":"TCP","serviceName":"monitoring:webhook-service","allNodes":true}]'
    creationTimestamp: "2025-03-02T08:10:23Z"
    generation: 1
    labels:
      app: webhook
      pod-template-hash: 64b5cc8c76
    name: webhook-deployment-64b5cc8c76
    namespace: monitoring
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: webhook-deployment
      uid: e5feae53-2671-4943-8714-7be74e79cec3
    resourceVersion: "22847"
    uid: 7fbc2e2c-3474-418d-8ac9-f77231f9197c
  spec:
    replicas: 1
    selector:
      matchLabels:
        app: webhook
        pod-template-hash: 64b5cc8c76
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: webhook
          pod-template-hash: 64b5cc8c76
      spec:
        containers:
        - image: gnh374/webhook-express:latest
          imagePullPolicy: Always
          name: webhook
          ports:
          - containerPort: 3000
            protocol: TCP
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "1"
    creationTimestamp: "2025-03-02T07:49:24Z"
    generation: 1
    labels:
      component: velero
      deploy: velero
      pod-template-hash: 57fdbd48db
    name: velero-57fdbd48db
    namespace: velero
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: velero
      uid: 9f7c236e-4de8-4339-a334-07ea5af96bdd
    resourceVersion: "22875"
    uid: 3edacc2d-151c-44fe-bc8f-8ea46daa0056
  spec:
    replicas: 1
    selector:
      matchLabels:
        deploy: velero
        pod-template-hash: 57fdbd48db
    template:
      metadata:
        annotations:
          prometheus.io/path: /metrics
          prometheus.io/port: "8085"
          prometheus.io/scrape: "true"
        creationTimestamp: null
        labels:
          component: velero
          deploy: velero
          pod-template-hash: 57fdbd48db
      spec:
        containers:
        - args:
          - server
          - --features=
          - --uploader-type=kopia
          command:
          - /velero
          env:
          - name: VELERO_SCRATCH_DIR
            value: /scratch
          - name: VELERO_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          - name: LD_LIBRARY_PATH
            value: /plugins
          image: velero/velero:v1.15.2
          imagePullPolicy: IfNotPresent
          name: velero
          ports:
          - containerPort: 8085
            name: metrics
            protocol: TCP
          resources:
            limits:
              cpu: "1"
              memory: 512Mi
            requests:
              cpu: 500m
              memory: 128Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /plugins
            name: plugins
          - mountPath: /scratch
            name: scratch
        dnsPolicy: ClusterFirst
        initContainers:
        - image: velero/velero-plugin-for-aws:v1.10.0
          imagePullPolicy: IfNotPresent
          name: velero-velero-plugin-for-aws
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /target
            name: plugins
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: velero
        serviceAccountName: velero
        terminationGracePeriodSeconds: 30
        volumes:
        - emptyDir: {}
          name: plugins
        - emptyDir: {}
          name: scratch
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
- apiVersion: apps/v1
  kind: StatefulSet
  metadata:
    annotations:
      meta.helm.sh/release-name: fleet-agent-cluster-3
      meta.helm.sh/release-namespace: cattle-fleet-system
      objectset.rio.cattle.io/applied: H4sIAAAAAAAA/8xVbW/bNhD+L/ysOHa8tIm+ebbSGXVsw043DIURnKiTzZUiNfLoxgv034eT5Jd0adABDZBvInXvz3N8HkWBBBkQiPhRgDGWgJQ1no82/QsleaSOU7YjgUhjR9lzlYlY5BqRzmCNhs5Sa8mTg1JUkZAO6xB3qkBPUJQiNkHrSGhIUb8YeAN+w6H719dZN+33unmapxdXeP3u3ft+7yLP5Hvo968vf8GL9EpecTYDBT6tRjSXvgTJf5roZ42B33nCgv18iZJL8ahRknX8XQDJzeRQJZTlN5ErdkS3VRKnz+UlLEoNhHWwk7n+wEi+k6ytEvJcGUU7/jY2w8HJuXSYo3OYjYJTZr2UG8yCVmY9Xht7uE4eUAauQcSf9z5oJB76Th5Kh9432H9+FF9wty/oBKF9p7ZEB/XYxNiISGxBB2RHQS6gWFWrKhJfUa03JOJeteJmpDUEyqBrEkhbFGAy9qmzNKFXkUCzrQ1aaH/9NB1NklEyn8z+vE2md/eLZDibDseTZHH/x2zxMVks9wWIWFx2RRUdfEeL8c3/cpgObpPlfDBM9hY3zhY8o1yhzhaYH77nQMzVPcydI+eq6iTg4AOXvBzO5omoVpFQBaz53oGRG3TnJ4jH226n1+v0RWs1D1rPrVaSgRjnU0tzh/7I8P/Qz6G3wUnG4bFmqgxO0W5oDeED1TzS2n6dO7VVGteYeAkaGlLkoD1GQkIJqdKKVB1FZM6WjNBgMhEMabn3zQ4uDiGbGb1bWEs3SmO7YzEToYrE1upQ4K0NhhrcC/5sp3fe+RJSPPZTn5g736NHJKQOntB5Agr+Gbr8dASHvyXDj+Pp/Xh6lyx+H0xOuNO7LLr+dWE9e9rvmwS5HoBRNPyRBedYa8UtvR54rwrIofw3iwUrxPJE13irnEFCz0+49SIWWpnwIJ6v2QUz8B+cDaWIe91uN2puptZwxiZPe/fJo2uMjso4kJI3/HmBtJp14yAymOcoiYG3rW7xY9BID3fRkdqGrHR2qzJ0naeNBMOkU6DVP5g91aTk7wD6ZFFrUeKdfinhUeWsfzlcO73V/nVrmylK2o2Ua8jwzYtWRSKUGRAuyQHhmqW7Vvhmr5kqW1AaUo0LLLWS4EXMoz8equrfAAAA//8mGISYrgkAAA
      objectset.rio.cattle.io/id: fleet-agent-bootstrap
    creationTimestamp: "2025-02-27T05:43:01Z"
    generation: 1
    labels:
      app.kubernetes.io/managed-by: Helm
      objectset.rio.cattle.io/hash: f399d0b310fbfb28e9667312fdc7a33954e2b8c8
    name: fleet-agent
    namespace: cattle-fleet-system
    resourceVersion: "23059"
    uid: 4fb5e7d2-ecb6-4fac-b4b7-f6f571257863
  spec:
    persistentVolumeClaimRetentionPolicy:
      whenDeleted: Retain
      whenScaled: Retain
    podManagementPolicy: OrderedReady
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app: fleet-agent
    serviceName: fleet-agent
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: fleet-agent
      spec:
        affinity:
          nodeAffinity:
            preferredDuringSchedulingIgnoredDuringExecution:
            - preference:
                matchExpressions:
                - key: fleet.cattle.io/agent
                  operator: In
                  values:
                  - "true"
              weight: 1
        containers:
        - command:
          - fleetagent
          env:
          - name: BUNDLEDEPLOYMENT_RECONCILER_WORKERS
            value: "50"
          - name: DRIFT_RECONCILER_WORKERS
            value: "50"
          - name: NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          - name: AGENT_SCOPE
          image: rancher/fleet-agent:v0.11.3
          imagePullPolicy: IfNotPresent
          name: fleet-agent
          resources: {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            privileged: false
            readOnlyRootFilesystem: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /.kube
            name: kube
        - command:
          - fleetagent
          - clusterstatus
          env:
          - name: NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          - name: CHECKIN_INTERVAL
            value: 15m0s
          image: rancher/fleet-agent:v0.11.3
          imagePullPolicy: IfNotPresent
          name: fleet-agent-clusterstatus
          resources: {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            privileged: false
            readOnlyRootFilesystem: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        initContainers:
        - command:
          - fleetagent
          - register
          env:
          - name: NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          image: rancher/fleet-agent:v0.11.3
          imagePullPolicy: IfNotPresent
          name: fleet-agent-register
          resources: {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            privileged: false
            readOnlyRootFilesystem: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        nodeSelector:
          kubernetes.io/os: linux
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          runAsGroup: 1000
          runAsNonRoot: true
          runAsUser: 1000
        serviceAccount: fleet-agent
        serviceAccountName: fleet-agent
        terminationGracePeriodSeconds: 30
        tolerations:
        - effect: NoSchedule
          key: node.cloudprovider.kubernetes.io/uninitialized
          operator: Equal
          value: "true"
        - effect: NoSchedule
          key: cattle.io/os
          operator: Equal
          value: linux
        volumes:
        - emptyDir: {}
          name: kube
    updateStrategy:
      rollingUpdate:
        partition: 0
      type: RollingUpdate
  status:
    availableReplicas: 1
    collisionCount: 0
    currentReplicas: 1
    currentRevision: fleet-agent-6f4bc75f47
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
    updateRevision: fleet-agent-6f4bc75f47
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: StatefulSet
  metadata:
    annotations:
      meta.helm.sh/release-name: prometheus
      meta.helm.sh/release-namespace: monitoring
    creationTimestamp: "2025-02-27T05:45:43Z"
    generation: 1
    labels:
      app.kubernetes.io/instance: prometheus
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: alertmanager
      app.kubernetes.io/version: v0.28.0
      helm.sh/chart: alertmanager-1.15.0
    name: prometheus-alertmanager
    namespace: monitoring
    resourceVersion: "4939"
    uid: 89704cf2-5cbd-4469-aec7-84eef9e23347
  spec:
    persistentVolumeClaimRetentionPolicy:
      whenDeleted: Retain
      whenScaled: Retain
    podManagementPolicy: OrderedReady
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app.kubernetes.io/instance: prometheus
        app.kubernetes.io/name: alertmanager
    serviceName: prometheus-alertmanager-headless
    template:
      metadata:
        annotations:
          checksum/config: c6322136dbc9671149df2fefe7d70ceb2a5f680d5aa29ba005d080f37bee0106
        creationTimestamp: null
        labels:
          app.kubernetes.io/instance: prometheus
          app.kubernetes.io/name: alertmanager
      spec:
        automountServiceAccountToken: true
        containers:
        - args:
          - --storage.path=/alertmanager
          - --config.file=/etc/alertmanager/alertmanager.yml
          env:
          - name: POD_IP
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.podIP
          image: quay.io/prometheus/alertmanager:v0.28.0
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /
              port: http
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          name: alertmanager
          ports:
          - containerPort: 9093
            name: http
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /
              port: http
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          resources: {}
          securityContext:
            runAsGroup: 65534
            runAsNonRoot: true
            runAsUser: 65534
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/alertmanager
            name: config
          - mountPath: /alertmanager
            name: storage
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 65534
          runAsGroup: 65534
          runAsNonRoot: true
          runAsUser: 65534
        serviceAccount: prometheus-alertmanager
        serviceAccountName: prometheus-alertmanager
        terminationGracePeriodSeconds: 30
        volumes:
        - configMap:
            defaultMode: 420
            name: prometheus-alertmanager
          name: config
    updateStrategy:
      rollingUpdate:
        partition: 0
      type: RollingUpdate
    volumeClaimTemplates:
    - apiVersion: v1
      kind: PersistentVolumeClaim
      metadata:
        creationTimestamp: null
        name: storage
      spec:
        accessModes:
        - ReadWriteOnce
        resources:
          requests:
            storage: 2Gi
        volumeMode: Filesystem
      status:
        phase: Pending
  status:
    availableReplicas: 1
    collisionCount: 0
    currentReplicas: 1
    currentRevision: prometheus-alertmanager-769db6f968
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
    updateRevision: prometheus-alertmanager-769db6f968
    updatedReplicas: 1
- apiVersion: batch/v1
  kind: Job
  metadata:
    annotations:
      objectset.rio.cattle.io/applied: H4sIAAAAAAAA/6xWUY/aOBD+KydLfbokJLC7kEh9yEL24MpCFGh1p9MKGWcCPhw7sh1atOK/n+yk3WzZbrfSiZfYmfkyM983MzwiXNFPIBUVHEVoizXZ944BctCB8hxF6E+xRQ4qQeMca4yiR4Q5FxprKrgyR7H9F4hWoD1JhUew1gw8KnrUeO+BlS4RXEvBGEiX7LHUroQdVVpaDOT8EEF85iDd3fHQAnVeHQPntw+U5++nwMqxAf0pDscloAhpiaGghzeZqwoT43Oot+Cqk9JQorODiAQb+pqWoDQuKxTxmjEHMbwFZotiwrW5Ku+7yO3tm+LYY7VHESJ+EQ4BD3H/GgY5kGEQXg/JEA/6VwEejIoBXBcYrgsTWZujrTrlSmPG3KcP/SglB9mUMyhAAiegUPTPd7q4KD9y0JYJclgazwkwsFxGBWYKHPTE+LerVk5dvi4Yqa1msB8Mt2Ef3Bu/yN2rHBM3zDG4YbENb4j5wRCdH84OUhUQU+0tJgdRFHNaUo2iwPd9B2koK4Y1mPevqPcVogQv6G7akLCaxv3rm/fJ3c31KBiF41FwMwquAn8c3A6T/mh8G975/sAPhn44GN2G4SQYT4bJOEyu/NtRvz/832Vz7qRvqo0pB9kSJ3fmAbUCQA5yXQXaVVpSvkMO2jGxxcxr2J9AgWums6YlT+/Rg4OAHy1SS9Aivk+Qg46Y1V2+zs43i09JtpotF92rLEmX3fM0md9vJtnsU5J1sBQQCbprN57G2XpjPrlK43H3u8+78LlDx2yvdaWiXu/d44ePt0m2SNbJahOns/O7njLEk6aWqtfm4faHnu/1/eD3ujKPF0G/kNw6zv5IfinK+ON6uknj1WozzpJJsljP4vmq42a7pOswW6yS8ccs2aw+zNLNer4ycczu/n7NJ53Hs8Vmul6nr1ktlps0W/7VRfLUkTgeYbXSID0mCGZO4HtXfc/3/F5wYw+D9tDFuotncxNkupzPxl1ECV/ld35wEC3xzt5iTvYgewdGqwqka0QeHX0v9AbutqYs7/v9q8D3R6j1SWvGUsEoOZmSFAuhUwkKeGd8GAzkIAlK1NIOr0fTG0BqSfVpLLiGL9p2PmPicyrpkTLYQaIIZvj51MIV3lJGNbUoKJeiMo0Uz+fIjBsJOF9ydsqE0HeUQctypGUNZwcdBatLuBc1100jluYxxdoMkN5elPAs757XRt7m0X1nS/wzd4LJHi79m+s3AdgJ9wJCc38BocuqM7XL6tLie0SrBvWC3fMNYAaY4fT8YMTCRQ4rYEC0kIYG01GSgwZlt7NCEWKU11+QpURpLPU3iSz5HaaslqYuL0hA1jxWC8ENgw1v1oyIskqlKCiz60KfKjvBaq5pCe2AbCYuyCMlEBNisll0tu3TEmtk0CgAykqfJlQ2SyindYkidA+lkKfOur5g/tfcngj/Rb8nmt/m+JXy5+Q6X4d49Ng+tXVp/us1Vm5nb9mFVdDdPa6MD+9at0p4Mn9JI5YJjXVte/38XwAAAP//Wh46x8MKAAA
      objectset.rio.cattle.io/id: helm-controller-chart-registration
      objectset.rio.cattle.io/owner-gvk: helm.cattle.io/v1, Kind=HelmChart
      objectset.rio.cattle.io/owner-name: traefik
      objectset.rio.cattle.io/owner-namespace: kube-system
    creationTimestamp: "2025-02-27T05:22:14Z"
    generation: 1
    labels:
      helmcharts.helm.cattle.io/chart: traefik
      objectset.rio.cattle.io/hash: c0f97ea7a25e3dec71957c7a3241a38f3e5fae5f
    name: helm-install-traefik
    namespace: kube-system
    ownerReferences:
    - apiVersion: helm.cattle.io/v1
      blockOwnerDeletion: false
      controller: false
      kind: HelmChart
      name: traefik
      uid: a017b92e-60fd-4dac-9dae-9fb96c6c6ce7
    resourceVersion: "676"
    uid: c1641b53-c755-4d8e-a64e-b7393e622d86
  spec:
    backoffLimit: 1000
    completionMode: NonIndexed
    completions: 1
    manualSelector: false
    parallelism: 1
    podReplacementPolicy: TerminatingOrFailed
    selector:
      matchLabels:
        batch.kubernetes.io/controller-uid: c1641b53-c755-4d8e-a64e-b7393e622d86
    suspend: false
    template:
      metadata:
        annotations:
          helmcharts.helm.cattle.io/configHash: SHA256=EF658189C81681410C1B7E28CB9F0030170938B99D1CD7EC9E40B82278BB67A8
        creationTimestamp: null
        labels:
          batch.kubernetes.io/controller-uid: c1641b53-c755-4d8e-a64e-b7393e622d86
          batch.kubernetes.io/job-name: helm-install-traefik
          controller-uid: c1641b53-c755-4d8e-a64e-b7393e622d86
          helmcharts.helm.cattle.io/chart: traefik
          job-name: helm-install-traefik
      spec:
        containers:
        - args:
          - install
          - --set-string
          - global.systemDefaultRegistry=
          env:
          - name: NAME
            value: traefik
          - name: VERSION
          - name: REPO
          - name: HELM_DRIVER
            value: secret
          - name: CHART_NAMESPACE
            value: kube-system
          - name: CHART
            value: https://%{KUBERNETES_API}%/static/charts/traefik-27.0.201+up27.0.2.tgz
          - name: HELM_VERSION
          - name: TARGET_NAMESPACE
            value: kube-system
          - name: AUTH_PASS_CREDENTIALS
            value: "false"
          - name: INSECURE_SKIP_TLS_VERIFY
            value: "false"
          - name: PLAIN_HTTP
            value: "false"
          - name: NO_PROXY
            value: .svc,.cluster.local,10.42.0.0/16,10.43.0.0/16
          - name: FAILURE_POLICY
            value: reinstall
          image: rancher/klipper-helm:v0.9.3-build20241008
          imagePullPolicy: IfNotPresent
          name: helm
          resources: {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /home/klipper-helm/.helm
            name: klipper-helm
          - mountPath: /home/klipper-helm/.cache
            name: klipper-cache
          - mountPath: /home/klipper-helm/.config
            name: klipper-config
          - mountPath: /tmp
            name: tmp
          - mountPath: /config
            name: values
          - mountPath: /chart
            name: content
        dnsPolicy: ClusterFirst
        nodeSelector:
          kubernetes.io/os: linux
        restartPolicy: OnFailure
        schedulerName: default-scheduler
        securityContext:
          runAsNonRoot: true
          seccompProfile:
            type: RuntimeDefault
        serviceAccount: helm-traefik
        serviceAccountName: helm-traefik
        terminationGracePeriodSeconds: 30
        volumes:
        - emptyDir:
            medium: Memory
          name: klipper-helm
        - emptyDir:
            medium: Memory
          name: klipper-cache
        - emptyDir:
            medium: Memory
          name: klipper-config
        - emptyDir:
            medium: Memory
          name: tmp
        - name: values
          secret:
            defaultMode: 420
            secretName: chart-values-traefik
        - configMap:
            defaultMode: 420
            name: chart-content-traefik
          name: content
  status:
    completionTime: "2025-02-27T05:22:59Z"
    conditions:
    - lastProbeTime: "2025-02-27T05:22:59Z"
      lastTransitionTime: "2025-02-27T05:22:59Z"
      message: Reached expected number of succeeded pods
      reason: CompletionsReached
      status: "True"
      type: SuccessCriteriaMet
    - lastProbeTime: "2025-02-27T05:22:59Z"
      lastTransitionTime: "2025-02-27T05:22:59Z"
      message: Reached expected number of succeeded pods
      reason: CompletionsReached
      status: "True"
      type: Complete
    ready: 0
    startTime: "2025-02-27T05:22:14Z"
    succeeded: 1
    terminating: 0
    uncountedTerminatedPods: {}
- apiVersion: batch/v1
  kind: Job
  metadata:
    annotations:
      objectset.rio.cattle.io/applied: H4sIAAAAAAAA/7RWUY/aOBD+KydLfbokJCEsm0h9yLLhlisLEdDqTqcVMs4EfDh2ZDu0aMV/P9mkbdjdbrcP90ac+b7MzPd5hkeEa/oJpKKCowRtsCa73iFADtpTXqAE/Sk2yEEVaFxgjVHyiDDnQmNNBVfmUWz+BaIVaE9S4RGsNQOPih416B2wyiWCaykYA+mSHZbalbClSkvLgZwfMojPHKS7Pexbos6rQ+D89oHy4v0dsGpkSH/Kw3EFKEFaYijp3iWyeBNE1ZgY3L7ZgKuOSkOFTg4iEmz6K1qB0riqUcIbxhzE8AaYbYxJ2darvCfZ29M357LDaocSFF2XZb8YkH4QDDf9YViSguCBHw/jQRmX4dAPcekP4tBk19Zqu0+50pgx9/JjPyrNQbb0BZQggRNQKPnniUeeSYEctGGC7OcGeQsMrK5JiZkCB31X/9tRa62udi+q01gPFQM8KEm/70Z+hN3I3/huXGywWw59fziM8DD2I3R6ODlI1UBM5zeY7EVZTmlFNUoC3/cdpKGqGdZg3r/i5ldEE7yk27uzGMu7NBxcvc/6N/4oisL4ejwKRkEUp+ObcTS6juOr8U0cRuEwzaIgi66i+CbuR6M0igdxHNwM/xcLnTotMF3HlINsBZRb8wO1ZkAPDgJ+sK/azs/S+ww56IBZ81SIk/Mt6lO2WE7ms+7RIsvn3ee7bHq/vl1MPmWLDp8CIkF340Z36WK1Np9d5umo++3Lq3YJ6ITttK5V0uu9e/zw8SZbzLJVtlyn+eT0rqeMouTcJNXr1OKGQ8/3Qj/4vamfJf1Ccat08Uf2S1mmH1d36zxdLtejRXabzVaTdLrswOwV6AIms2U2+rjI1ssPk3y9mi5NHpPx369h8mk6ma3vVqv8tajZfJ0v5n91mTx1II5HWKM0SI8JgpkT+F4Uer7n94Ir+9BvH7pc43QyNUnm8+lk1GWU8NVTpwcH0Qpv7SnmZAeyt2e0rkG6xr3Jwfdir+9uGsqK0A+jwPevUYvJG8ZywSg5mpaUM6FzCQp4ZzYYDuQgCUo00k6mR2N4II2k+jgSXMMXba80Y+JzLumBMthCpghm+HIk4RpvKKOaWhZUSFGb25FOp8jMEQm4mHN2XAihx5RBq3KiZQMnBx0Eayq4Fw3X59tVmZ851mYy9Haigou6e16beVtH951t8c/gBJMdPMefj99EYEfXCwzn82cUuqo7I7mqn0c8ZbRuUC/EXY53M5WMpqcHYxYuClgCA6KFNDKYGyU5aFB2BSuUIEZ58wVZSZTGUn+zyJyPMWWNNH15wQKy4amaCW4UPOtmw4io6lyKkjK7B/SxthOs4ZpWcAslbpg+j1GQB0ogJcRUM+us08sNdbbC2QVQ1fp4S+V5wxS0qVCC7qES8tjZyc/U/zXYd9F/Efdd6rcBv8p+KbDzdZAnj+2vtjfnP3bnKPfJUrLbqKTbe1wbHO8iWkdcQl7yi1VFY93Ye3/6LwAA//+SPvRkuAoAAA
      objectset.rio.cattle.io/id: helm-controller-chart-registration
      objectset.rio.cattle.io/owner-gvk: helm.cattle.io/v1, Kind=HelmChart
      objectset.rio.cattle.io/owner-name: traefik-crd
      objectset.rio.cattle.io/owner-namespace: kube-system
    creationTimestamp: "2025-02-27T05:22:14Z"
    generation: 1
    labels:
      helmcharts.helm.cattle.io/chart: traefik-crd
      objectset.rio.cattle.io/hash: 48ff3d5c3117b372fcdca509795f9f2702af0592
    name: helm-install-traefik-crd
    namespace: kube-system
    ownerReferences:
    - apiVersion: helm.cattle.io/v1
      blockOwnerDeletion: false
      controller: false
      kind: HelmChart
      name: traefik-crd
      uid: d5a5fc33-404a-40b0-9dba-f700774a7904
    resourceVersion: "608"
    uid: 05b08b97-af47-4ad4-b681-eb5d61c522a9
  spec:
    backoffLimit: 1000
    completionMode: NonIndexed
    completions: 1
    manualSelector: false
    parallelism: 1
    podReplacementPolicy: TerminatingOrFailed
    selector:
      matchLabels:
        batch.kubernetes.io/controller-uid: 05b08b97-af47-4ad4-b681-eb5d61c522a9
    suspend: false
    template:
      metadata:
        annotations:
          helmcharts.helm.cattle.io/configHash: SHA256=E3B0C44298FC1C149AFBF4C8996FB92427AE41E4649B934CA495991B7852B855
        creationTimestamp: null
        labels:
          batch.kubernetes.io/controller-uid: 05b08b97-af47-4ad4-b681-eb5d61c522a9
          batch.kubernetes.io/job-name: helm-install-traefik-crd
          controller-uid: 05b08b97-af47-4ad4-b681-eb5d61c522a9
          helmcharts.helm.cattle.io/chart: traefik-crd
          job-name: helm-install-traefik-crd
      spec:
        containers:
        - args:
          - install
          env:
          - name: NAME
            value: traefik-crd
          - name: VERSION
          - name: REPO
          - name: HELM_DRIVER
            value: secret
          - name: CHART_NAMESPACE
            value: kube-system
          - name: CHART
            value: https://%{KUBERNETES_API}%/static/charts/traefik-crd-27.0.201+up27.0.2.tgz
          - name: HELM_VERSION
          - name: TARGET_NAMESPACE
            value: kube-system
          - name: AUTH_PASS_CREDENTIALS
            value: "false"
          - name: INSECURE_SKIP_TLS_VERIFY
            value: "false"
          - name: PLAIN_HTTP
            value: "false"
          - name: NO_PROXY
            value: .svc,.cluster.local,10.42.0.0/16,10.43.0.0/16
          - name: FAILURE_POLICY
            value: reinstall
          image: rancher/klipper-helm:v0.9.3-build20241008
          imagePullPolicy: IfNotPresent
          name: helm
          resources: {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /home/klipper-helm/.helm
            name: klipper-helm
          - mountPath: /home/klipper-helm/.cache
            name: klipper-cache
          - mountPath: /home/klipper-helm/.config
            name: klipper-config
          - mountPath: /tmp
            name: tmp
          - mountPath: /config
            name: values
          - mountPath: /chart
            name: content
        dnsPolicy: ClusterFirst
        nodeSelector:
          kubernetes.io/os: linux
        restartPolicy: OnFailure
        schedulerName: default-scheduler
        securityContext:
          runAsNonRoot: true
          seccompProfile:
            type: RuntimeDefault
        serviceAccount: helm-traefik-crd
        serviceAccountName: helm-traefik-crd
        terminationGracePeriodSeconds: 30
        volumes:
        - emptyDir:
            medium: Memory
          name: klipper-helm
        - emptyDir:
            medium: Memory
          name: klipper-cache
        - emptyDir:
            medium: Memory
          name: klipper-config
        - emptyDir:
            medium: Memory
          name: tmp
        - name: values
          secret:
            defaultMode: 420
            secretName: chart-values-traefik-crd
        - configMap:
            defaultMode: 420
            name: chart-content-traefik-crd
          name: content
  status:
    completionTime: "2025-02-27T05:22:42Z"
    conditions:
    - lastProbeTime: "2025-02-27T05:22:42Z"
      lastTransitionTime: "2025-02-27T05:22:42Z"
      message: Reached expected number of succeeded pods
      reason: CompletionsReached
      status: "True"
      type: SuccessCriteriaMet
    - lastProbeTime: "2025-02-27T05:22:42Z"
      lastTransitionTime: "2025-02-27T05:22:42Z"
      message: Reached expected number of succeeded pods
      reason: CompletionsReached
      status: "True"
      type: Complete
    ready: 0
    startTime: "2025-02-27T05:22:14Z"
    succeeded: 1
    terminating: 0
    uncountedTerminatedPods: {}
kind: List
metadata:
  resourceVersion: ""
