apiVersion: v1
items:
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2025-03-03T02:56:47Z"
    generateName: fleet-agent-
    labels:
      app: fleet-agent
      apps.kubernetes.io/pod-index: "0"
      controller-revision-hash: fleet-agent-57565ff5cd
      statefulset.kubernetes.io/pod-name: fleet-agent-0
    name: fleet-agent-0
    namespace: cattle-fleet-local-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: StatefulSet
      name: fleet-agent
      uid: 369f846f-3ada-4c9c-971f-f53c7a4dd7b8
    resourceVersion: "196798"
    uid: 85b4149c-5f52-4efd-abcb-e5223855a840
  spec:
    affinity:
      nodeAffinity:
        preferredDuringSchedulingIgnoredDuringExecution:
        - preference:
            matchExpressions:
            - key: fleet.cattle.io/agent
              operator: In
              values:
              - "true"
          weight: 1
    containers:
    - command:
      - fleetagent
      env:
      - name: BUNDLEDEPLOYMENT_RECONCILER_WORKERS
        value: "50"
      - name: DRIFT_RECONCILER_WORKERS
        value: "50"
      - name: NAMESPACE
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.namespace
      - name: AGENT_SCOPE
        value: cattle-fleet-local-system
      image: rancher/fleet-agent:v0.11.3
      imagePullPolicy: IfNotPresent
      name: fleet-agent
      resources: {}
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
        privileged: false
        readOnlyRootFilesystem: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /.kube
        name: kube
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-8qmqc
        readOnly: true
    - command:
      - fleetagent
      - clusterstatus
      env:
      - name: NAMESPACE
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.namespace
      - name: CHECKIN_INTERVAL
        value: 15m0s
      image: rancher/fleet-agent:v0.11.3
      imagePullPolicy: IfNotPresent
      name: fleet-agent-clusterstatus
      resources: {}
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
        privileged: false
        readOnlyRootFilesystem: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-8qmqc
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostname: fleet-agent-0
    initContainers:
    - command:
      - fleetagent
      - register
      env:
      - name: NAMESPACE
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.namespace
      image: rancher/fleet-agent:v0.11.3
      imagePullPolicy: IfNotPresent
      name: fleet-agent-register
      resources: {}
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
        privileged: false
        readOnlyRootFilesystem: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-8qmqc
        readOnly: true
    nodeName: ip-172-31-17-126
    nodeSelector:
      kubernetes.io/os: linux
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      runAsGroup: 1000
      runAsNonRoot: true
      runAsUser: 1000
    serviceAccount: fleet-agent
    serviceAccountName: fleet-agent
    subdomain: fleet-agent
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoSchedule
      key: node.cloudprovider.kubernetes.io/uninitialized
      operator: Equal
      value: "true"
    - effect: NoSchedule
      key: cattle.io/os
      operator: Equal
      value: linux
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - emptyDir: {}
      name: kube
    - name: kube-api-access-8qmqc
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-03-03T02:56:49Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-03-03T02:56:59Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2025-03-03T02:57:00Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2025-03-03T02:57:00Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-03-03T02:56:47Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://acbb6f2a9dc0882a74b9cdc953e1b9eacf8416b7de9322e25b4255c6b74a5c52
      image: docker.io/rancher/fleet-agent:v0.11.3
      imageID: docker.io/rancher/fleet-agent@sha256:2a229b1be5f095e5c56e06c51318cf6d0aeed19204fb3c97e15052bc92591d23
      lastState: {}
      name: fleet-agent
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2025-03-03T02:56:59Z"
      volumeMounts:
      - mountPath: /.kube
        name: kube
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-8qmqc
        readOnly: true
        recursiveReadOnly: Disabled
    - containerID: containerd://ffd799fc22054cdb49d62772a2882fa46768abcf4c2badb5e7db2c8c22c0c208
      image: docker.io/rancher/fleet-agent:v0.11.3
      imageID: docker.io/rancher/fleet-agent@sha256:2a229b1be5f095e5c56e06c51318cf6d0aeed19204fb3c97e15052bc92591d23
      lastState: {}
      name: fleet-agent-clusterstatus
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2025-03-03T02:57:00Z"
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-8qmqc
        readOnly: true
        recursiveReadOnly: Disabled
    hostIP: 172.31.17.126
    hostIPs:
    - ip: 172.31.17.126
    initContainerStatuses:
    - containerID: containerd://b98fc58c13619bf6855fdd106e1e84c166d18a98ecde4c2c23256846b0818ad1
      image: docker.io/rancher/fleet-agent:v0.11.3
      imageID: docker.io/rancher/fleet-agent@sha256:2a229b1be5f095e5c56e06c51318cf6d0aeed19204fb3c97e15052bc92591d23
      lastState: {}
      name: fleet-agent-register
      ready: true
      restartCount: 0
      started: false
      state:
        terminated:
          containerID: containerd://b98fc58c13619bf6855fdd106e1e84c166d18a98ecde4c2c23256846b0818ad1
          exitCode: 0
          finishedAt: "2025-03-03T02:56:57Z"
          reason: Completed
          startedAt: "2025-03-03T02:56:49Z"
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-8qmqc
        readOnly: true
        recursiveReadOnly: Disabled
    phase: Running
    podIP: 10.42.0.18
    podIPs:
    - ip: 10.42.0.18
    qosClass: BestEffort
    startTime: "2025-03-03T02:56:47Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2025-02-05T05:33:55Z"
    generateName: fleet-controller-86bfd4bff-
    labels:
      app: fleet-controller
      fleet.cattle.io/shard-default: "true"
      fleet.cattle.io/shard-id: ""
      pod-template-hash: 86bfd4bff
    name: fleet-controller-86bfd4bff-f85xm
    namespace: cattle-fleet-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: fleet-controller-86bfd4bff
      uid: b54bd895-59b7-4c0e-805e-fe80cc675e33
    resourceVersion: "196038"
    uid: 396e0066-425b-4f05-9395-328769e6bc70
  spec:
    containers:
    - command:
      - fleetcontroller
      env:
      - name: NAMESPACE
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.namespace
      - name: CATTLE_ELECTION_LEASE_DURATION
        value: 30s
      - name: CATTLE_ELECTION_RETRY_PERIOD
        value: 10s
      - name: CATTLE_ELECTION_RENEW_DEADLINE
        value: 25s
      - name: BUNDLE_RECONCILER_WORKERS
        value: "50"
      - name: BUNDLEDEPLOYMENT_RECONCILER_WORKERS
        value: "50"
      - name: CLUSTER_RECONCILER_WORKERS
        value: "50"
      - name: CLUSTERGROUP_RECONCILER_WORKERS
        value: "50"
      - name: IMAGESCAN_RECONCILER_WORKERS
        value: "50"
      image: rancher/fleet:v0.11.3
      imagePullPolicy: IfNotPresent
      name: fleet-controller
      ports:
      - containerPort: 8080
        name: metrics
        protocol: TCP
      resources: {}
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
        privileged: false
        readOnlyRootFilesystem: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /tmp
        name: tmp
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-lxknq
        readOnly: true
    - command:
      - fleetcontroller
      - cleanup
      env:
      - name: NAMESPACE
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.namespace
      - name: CATTLE_ELECTION_LEASE_DURATION
        value: 30s
      - name: CATTLE_ELECTION_RETRY_PERIOD
        value: 10s
      - name: CATTLE_ELECTION_RENEW_DEADLINE
        value: 25s
      image: rancher/fleet:v0.11.3
      imagePullPolicy: IfNotPresent
      name: fleet-cleanup
      resources: {}
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
        privileged: false
        readOnlyRootFilesystem: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-lxknq
        readOnly: true
    - command:
      - fleetcontroller
      - agentmanagement
      - --disable-bootstrap
      env:
      - name: NAMESPACE
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.namespace
      - name: FLEET_PROPAGATE_DEBUG_SETTINGS_TO_AGENTS
        value: "true"
      - name: FLEET_DEBUG_DISABLE_SECURITY_CONTEXT
        value: "false"
      - name: CATTLE_ELECTION_LEASE_DURATION
        value: 30s
      - name: CATTLE_ELECTION_RETRY_PERIOD
        value: 10s
      - name: CATTLE_ELECTION_RENEW_DEADLINE
        value: 25s
      image: rancher/fleet:v0.11.3
      imagePullPolicy: IfNotPresent
      name: fleet-agentmanagement
      resources: {}
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
        privileged: false
        readOnlyRootFilesystem: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-lxknq
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: ip-172-31-17-126
    nodeSelector:
      kubernetes.io/os: linux
    preemptionPolicy: PreemptLowerPriority
    priority: 1000000000
    priorityClassName: rancher-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      runAsGroup: 1000
      runAsNonRoot: true
      runAsUser: 1000
    serviceAccount: fleet-controller
    serviceAccountName: fleet-controller
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoSchedule
      key: cattle.io/os
      operator: Equal
      value: linux
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - emptyDir: {}
      name: tmp
    - name: kube-api-access-lxknq
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-03-03T02:55:16Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-02-05T05:33:55Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2025-03-03T02:55:16Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2025-03-03T02:55:16Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-02-05T05:33:55Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://ba0be2d8d48a519243f64e1cc2b48036073cb25bb85fd23fcafaf51f0a66fe81
      image: docker.io/rancher/fleet:v0.11.3
      imageID: docker.io/rancher/fleet@sha256:2c99a3eb64e3df50a69389c78c127198e8d4c02107fbdf898e597d3af96b2b85
      lastState:
        terminated:
          containerID: containerd://a7abb297d950e4159695a8a1b93265afdbe06bd5e6d3dca4ab577d5b49114b1a
          exitCode: 255
          finishedAt: "2025-03-03T02:54:53Z"
          reason: Unknown
          startedAt: "2025-03-02T07:30:34Z"
      name: fleet-agentmanagement
      ready: true
      restartCount: 33
      started: true
      state:
        running:
          startedAt: "2025-03-03T02:55:06Z"
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-lxknq
        readOnly: true
        recursiveReadOnly: Disabled
    - containerID: containerd://27494d07fd80e094c44397e8f597d6a1f5e688db586e187df42bc62ee4efe474
      image: docker.io/rancher/fleet:v0.11.3
      imageID: docker.io/rancher/fleet@sha256:2c99a3eb64e3df50a69389c78c127198e8d4c02107fbdf898e597d3af96b2b85
      lastState:
        terminated:
          containerID: containerd://1afc30740e69b1f483598135cc969fb8a9f21fb6882a9e283f6d3be50a08dd1c
          exitCode: 255
          finishedAt: "2025-03-03T02:54:53Z"
          reason: Unknown
          startedAt: "2025-03-02T07:30:33Z"
      name: fleet-cleanup
      ready: true
      restartCount: 33
      started: true
      state:
        running:
          startedAt: "2025-03-03T02:55:06Z"
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-lxknq
        readOnly: true
        recursiveReadOnly: Disabled
    - containerID: containerd://1f783fafd657a9b51f702155def9956dc364a25463eedca69770f400f565169d
      image: docker.io/rancher/fleet:v0.11.3
      imageID: docker.io/rancher/fleet@sha256:2c99a3eb64e3df50a69389c78c127198e8d4c02107fbdf898e597d3af96b2b85
      lastState:
        terminated:
          containerID: containerd://5f72f1b667c8e75d2c070282b29d9b5f9fca43735b49af835fead15738e9b4e1
          exitCode: 255
          finishedAt: "2025-03-03T02:54:53Z"
          reason: Unknown
          startedAt: "2025-03-02T07:30:32Z"
      name: fleet-controller
      ready: true
      restartCount: 33
      started: true
      state:
        running:
          startedAt: "2025-03-03T02:55:05Z"
      volumeMounts:
      - mountPath: /tmp
        name: tmp
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-lxknq
        readOnly: true
        recursiveReadOnly: Disabled
    hostIP: 172.31.17.126
    hostIPs:
    - ip: 172.31.17.126
    phase: Running
    podIP: 10.42.0.251
    podIPs:
    - ip: 10.42.0.251
    qosClass: BestEffort
    startTime: "2025-02-05T05:33:55Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2025-02-05T05:33:55Z"
    generateName: gitjob-547d87f97f-
    labels:
      app: gitjob
      fleet.cattle.io/shard-default: "true"
      fleet.cattle.io/shard-id: ""
      pod-template-hash: 547d87f97f
    name: gitjob-547d87f97f-74jhl
    namespace: cattle-fleet-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: gitjob-547d87f97f
      uid: 62877784-4e30-4fbc-91cd-0b615a6c601e
    resourceVersion: "196267"
    uid: 8fa3f077-c12b-443b-a7ae-55bb010cf3cd
  spec:
    containers:
    - args:
      - fleetcontroller
      - gitjob
      - --gitjob-image
      - rancher/fleet:v0.11.3
      env:
      - name: NAMESPACE
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.namespace
      - name: CATTLE_ELECTION_LEASE_DURATION
        value: 30s
      - name: CATTLE_ELECTION_RETRY_PERIOD
        value: 10s
      - name: CATTLE_ELECTION_RENEW_DEADLINE
        value: 25s
      - name: GITREPO_SYNC_PERIOD
        value: 2h
      - name: GITREPO_RECONCILER_WORKERS
        value: "50"
      image: rancher/fleet:v0.11.3
      imagePullPolicy: IfNotPresent
      name: gitjob
      ports:
      - containerPort: 8081
        name: metrics
        protocol: TCP
      resources: {}
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
        privileged: false
        readOnlyRootFilesystem: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /tmp
        name: tmp
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-jfxbc
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: ip-172-31-17-126
    nodeSelector:
      kubernetes.io/os: linux
    preemptionPolicy: PreemptLowerPriority
    priority: 1000000000
    priorityClassName: rancher-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      runAsGroup: 1000
      runAsNonRoot: true
      runAsUser: 1000
    serviceAccount: gitjob
    serviceAccountName: gitjob
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoSchedule
      key: cattle.io/os
      operator: Equal
      value: linux
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - emptyDir: {}
      name: tmp
    - name: kube-api-access-jfxbc
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-03-03T02:55:22Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-02-05T05:33:55Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2025-03-03T02:55:22Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2025-03-03T02:55:22Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-02-05T05:33:55Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://d615d656fe7698a47cde1ac46daa20d7c4e74d5bfecf7aacf234335c74c248ab
      image: docker.io/rancher/fleet:v0.11.3
      imageID: docker.io/rancher/fleet@sha256:2c99a3eb64e3df50a69389c78c127198e8d4c02107fbdf898e597d3af96b2b85
      lastState:
        terminated:
          containerID: containerd://51f891909b7235e73a70dc5da1d39fbd966afb3975323e80c3d3f676ad0891ac
          exitCode: 255
          finishedAt: "2025-03-03T02:54:53Z"
          reason: Unknown
          startedAt: "2025-03-02T07:30:29Z"
      name: gitjob
      ready: true
      restartCount: 33
      started: true
      state:
        running:
          startedAt: "2025-03-03T02:55:03Z"
      volumeMounts:
      - mountPath: /tmp
        name: tmp
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-jfxbc
        readOnly: true
        recursiveReadOnly: Disabled
    hostIP: 172.31.17.126
    hostIPs:
    - ip: 172.31.17.126
    phase: Running
    podIP: 10.42.0.247
    podIPs:
    - ip: 10.42.0.247
    qosClass: BestEffort
    startTime: "2025-02-05T05:33:55Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2025-02-05T05:35:00Z"
    generateName: capi-controller-manager-cc78bcdb8-
    labels:
      cluster.x-k8s.io/provider: cluster-api
      control-plane: controller-manager
      pod-template-hash: cc78bcdb8
    name: capi-controller-manager-cc78bcdb8-lkg4d
    namespace: cattle-provisioning-capi-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: capi-controller-manager-cc78bcdb8
      uid: 96880f01-e8bc-4f80-adb8-ca5139f82d39
    resourceVersion: "196035"
    uid: 2de8a5d3-5be6-49e4-af98-468e3a57cde0
  spec:
    containers:
    - args:
      - --leader-elect
      - --metrics-bind-addr=localhost:8080
      - --feature-gates=MachinePool=false,ClusterResourceSet=false,ClusterTopology=false,RuntimeSDK=false
      command:
      - /manager
      env:
      - name: POD_NAMESPACE
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.namespace
      - name: POD_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.name
      - name: POD_UID
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.uid
      image: rancher/mirrored-cluster-api-controller:v1.8.3
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 3
        httpGet:
          path: /healthz
          port: healthz
          scheme: HTTP
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      name: manager
      ports:
      - containerPort: 9443
        name: webhook-server
        protocol: TCP
      - containerPort: 9440
        name: healthz
        protocol: TCP
      - containerPort: 8443
        name: metrics
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /readyz
          port: healthz
          scheme: HTTP
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      resources: {}
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
        privileged: false
        runAsGroup: 65532
        runAsUser: 65532
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: FallbackToLogsOnError
      volumeMounts:
      - mountPath: /tmp/k8s-webhook-server/serving-certs
        name: cert
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-lnqx4
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: ip-172-31-17-126
    nodeSelector:
      kubernetes.io/os: linux
    preemptionPolicy: PreemptLowerPriority
    priority: 1000000000
    priorityClassName: rancher-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      runAsNonRoot: true
      seccompProfile:
        type: RuntimeDefault
    serviceAccount: capi-manager
    serviceAccountName: capi-manager
    terminationGracePeriodSeconds: 10
    tolerations:
    - effect: NoSchedule
      key: cattle.io/os
      operator: Equal
      value: linux
    - effect: NoSchedule
      key: node-role.kubernetes.io/controlplane
      value: "true"
    - effect: NoSchedule
      key: node-role.kubernetes.io/control-plane
      operator: Exists
    - effect: NoSchedule
      key: node-role.kubernetes.io/master
      operator: Exists
    - effect: NoExecute
      key: node-role.kubernetes.io/etcd
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: cert
      secret:
        defaultMode: 420
        secretName: capi-webhook-service-cert
    - name: kube-api-access-lnqx4
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-03-03T02:55:18Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-02-05T05:35:00Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2025-03-03T02:55:19Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2025-03-03T02:55:19Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-02-05T05:35:00Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://cab62827d1b9ee259264d73ae40daacfa6b82d144dc52bddca2be917db83739f
      image: docker.io/rancher/mirrored-cluster-api-controller:v1.8.3
      imageID: docker.io/rancher/mirrored-cluster-api-controller@sha256:885831c581023cc87caef6f781c5967330a34fcba543985d71545d1730512e9a
      lastState:
        terminated:
          containerID: containerd://d057df10cbb4f2ff7cfc60aa0dfc78810a303d387cfbfa57d5037ef7c1f55a7f
          exitCode: 255
          finishedAt: "2025-03-03T02:54:53Z"
          message: |
            hine"
            I0302 07:30:55.050119       1 controller.go:173] "Starting EventSource" controller="machinehealthcheck" controllerGroup="cluster.x-k8s.io" controllerKind="MachineHealthCheck" source="kind source: *v1beta1.Cluster"
            I0302 07:30:55.050136       1 controller.go:181] "Starting Controller" controller="machinehealthcheck" controllerGroup="cluster.x-k8s.io" controllerKind="MachineHealthCheck"
            I0302 07:30:55.174536       1 reflector.go:359] Caches populated for *v1beta1.MachineDeployment from k8s.io/client-go@v0.30.3/tools/cache/reflector.go:232
            I0302 07:31:01.447770       1 reflector.go:359] Caches populated for *v1beta1.Cluster from k8s.io/client-go@v0.30.3/tools/cache/reflector.go:232
            I0302 07:31:01.453237       1 reflector.go:359] Caches populated for *v1beta1.MachineSet from k8s.io/client-go@v0.30.3/tools/cache/reflector.go:232
            I0302 07:31:01.453962       1 reflector.go:359] Caches populated for *v1beta1.MachineHealthCheck from k8s.io/client-go@v0.30.3/tools/cache/reflector.go:232
            I0302 07:31:01.554549       1 controller.go:215] "Starting workers" controller="remote/clustercache" controllerGroup="cluster.x-k8s.io" controllerKind="Cluster" worker count=10
            I0302 07:31:01.555757       1 controller.go:215] "Starting workers" controller="cluster" controllerGroup="cluster.x-k8s.io" controllerKind="Cluster" worker count=10
            I0302 07:31:01.555937       1 controller.go:215] "Starting workers" controller="machine" controllerGroup="cluster.x-k8s.io" controllerKind="Machine" worker count=10
            I0302 07:31:01.555986       1 controller.go:215] "Starting workers" controller="machinedeployment" controllerGroup="cluster.x-k8s.io" controllerKind="MachineDeployment" worker count=10
            I0302 07:31:01.556026       1 controller.go:215] "Starting workers" controller="machineset" controllerGroup="cluster.x-k8s.io" controllerKind="MachineSet" worker count=10
            I0302 07:31:01.558163       1 controller.go:215] "Starting workers" controller="machinehealthcheck" controllerGroup="cluster.x-k8s.io" controllerKind="MachineHealthCheck" worker count=10
          reason: Unknown
          startedAt: "2025-03-02T07:30:32Z"
      name: manager
      ready: true
      restartCount: 33
      started: true
      state:
        running:
          startedAt: "2025-03-03T02:55:03Z"
      volumeMounts:
      - mountPath: /tmp/k8s-webhook-server/serving-certs
        name: cert
        readOnly: true
        recursiveReadOnly: Disabled
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-lnqx4
        readOnly: true
        recursiveReadOnly: Disabled
    hostIP: 172.31.17.126
    hostIPs:
    - ip: 172.31.17.126
    phase: Running
    podIP: 10.42.0.244
    podIPs:
    - ip: 10.42.0.244
    qosClass: BestEffort
    startTime: "2025-02-05T05:35:00Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      kubectl.kubernetes.io/restartedAt: "2025-02-05T06:43:35Z"
    creationTimestamp: "2025-02-05T06:43:35Z"
    generateName: rancher-c6b4fd756-
    labels:
      app: rancher
      pod-template-hash: c6b4fd756
      release: rancher
    name: rancher-c6b4fd756-4fxvc
    namespace: cattle-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: rancher-c6b4fd756
      uid: 898e6c1f-00b4-4dcc-aad6-e05639c4eb7c
    resourceVersion: "196563"
    uid: 04857b2b-168d-437c-9a3d-3263aa322d5b
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchExpressions:
            - key: kubernetes.io/os
              operator: NotIn
              values:
              - windows
      podAntiAffinity:
        preferredDuringSchedulingIgnoredDuringExecution:
        - podAffinityTerm:
            labelSelector:
              matchExpressions:
              - key: app
                operator: In
                values:
                - rancher
            topologyKey: kubernetes.io/hostname
          weight: 100
    containers:
    - args:
      - --no-cacerts
      - --http-listen-port=80
      - --https-listen-port=443
      - --add-local=true
      env:
      - name: CATTLE_NAMESPACE
        value: cattle-system
      - name: CATTLE_PEER_SERVICE
        value: rancher
      image: rancher/rancher:v2.10.2
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 5
        httpGet:
          path: /healthz
          port: 80
          scheme: HTTP
        periodSeconds: 30
        successThreshold: 1
        timeoutSeconds: 5
      name: rancher
      ports:
      - containerPort: 80
        protocol: TCP
      readinessProbe:
        failureThreshold: 5
        httpGet:
          path: /healthz
          port: 80
          scheme: HTTP
        periodSeconds: 30
        successThreshold: 1
        timeoutSeconds: 5
      resources: {}
      startupProbe:
        failureThreshold: 12
        httpGet:
          path: /healthz
          port: 80
          scheme: HTTP
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 5
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-zn2cb
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: ip-172-31-17-126
    preemptionPolicy: PreemptLowerPriority
    priority: 1000000000
    priorityClassName: rancher-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: rancher
    serviceAccountName: rancher
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoSchedule
      key: cattle.io/os
      operator: Equal
      value: linux
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: kube-api-access-zn2cb
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-03-03T02:55:23Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-02-05T06:43:35Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2025-03-03T02:56:16Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2025-03-03T02:56:16Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-02-05T06:43:35Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://bc13e5e778cf73043f7151c323c4cc9b6622892720ee095b6e809e729d0b6290
      image: docker.io/rancher/rancher:v2.10.2
      imageID: docker.io/rancher/rancher@sha256:c2d87320b6936cf3f8f8aa57e0a4bad3338dadc9b572f3810afecf09e6ca278d
      lastState:
        terminated:
          containerID: containerd://44fc60376c29260e716697a6ea0ecdfad37f8dd1fbf4be31826f56fd06b2da06
          exitCode: 255
          finishedAt: "2025-03-03T02:54:53Z"
          reason: Unknown
          startedAt: "2025-03-02T07:30:28Z"
      name: rancher
      ready: true
      restartCount: 33
      started: true
      state:
        running:
          startedAt: "2025-03-03T02:55:11Z"
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-zn2cb
        readOnly: true
        recursiveReadOnly: Disabled
    hostIP: 172.31.17.126
    hostIPs:
    - ip: 172.31.17.126
    phase: Running
    podIP: 10.42.0.2
    podIPs:
    - ip: 10.42.0.2
    qosClass: BestEffort
    startTime: "2025-02-05T06:43:35Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      kubectl.kubernetes.io/restartedAt: "2025-02-05T06:43:35Z"
    creationTimestamp: "2025-02-05T06:44:07Z"
    generateName: rancher-c6b4fd756-
    labels:
      app: rancher
      pod-template-hash: c6b4fd756
      release: rancher
    name: rancher-c6b4fd756-7jk65
    namespace: cattle-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: rancher-c6b4fd756
      uid: 898e6c1f-00b4-4dcc-aad6-e05639c4eb7c
    resourceVersion: "196573"
    uid: a0fd7d6d-1f8f-41f0-a3a4-a7c9ce427de8
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchExpressions:
            - key: kubernetes.io/os
              operator: NotIn
              values:
              - windows
      podAntiAffinity:
        preferredDuringSchedulingIgnoredDuringExecution:
        - podAffinityTerm:
            labelSelector:
              matchExpressions:
              - key: app
                operator: In
                values:
                - rancher
            topologyKey: kubernetes.io/hostname
          weight: 100
    containers:
    - args:
      - --no-cacerts
      - --http-listen-port=80
      - --https-listen-port=443
      - --add-local=true
      env:
      - name: CATTLE_NAMESPACE
        value: cattle-system
      - name: CATTLE_PEER_SERVICE
        value: rancher
      image: rancher/rancher:v2.10.2
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 5
        httpGet:
          path: /healthz
          port: 80
          scheme: HTTP
        periodSeconds: 30
        successThreshold: 1
        timeoutSeconds: 5
      name: rancher
      ports:
      - containerPort: 80
        protocol: TCP
      readinessProbe:
        failureThreshold: 5
        httpGet:
          path: /healthz
          port: 80
          scheme: HTTP
        periodSeconds: 30
        successThreshold: 1
        timeoutSeconds: 5
      resources: {}
      startupProbe:
        failureThreshold: 12
        httpGet:
          path: /healthz
          port: 80
          scheme: HTTP
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 5
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-7lhm6
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: ip-172-31-17-126
    preemptionPolicy: PreemptLowerPriority
    priority: 1000000000
    priorityClassName: rancher-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: rancher
    serviceAccountName: rancher
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoSchedule
      key: cattle.io/os
      operator: Equal
      value: linux
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: kube-api-access-7lhm6
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-03-03T02:55:16Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-02-05T06:44:07Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2025-03-03T02:56:17Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2025-03-03T02:56:17Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-02-05T06:44:07Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://e3d0307b94fe4e5283d5cd148115626e36ecea62cee00c10ef5ba236303a00ec
      image: docker.io/rancher/rancher:v2.10.2
      imageID: docker.io/rancher/rancher@sha256:c2d87320b6936cf3f8f8aa57e0a4bad3338dadc9b572f3810afecf09e6ca278d
      lastState:
        terminated:
          containerID: containerd://965940e447facc1d127fd628d819032f9c08c5258eeb1afa5a69ba067e8348d5
          exitCode: 255
          finishedAt: "2025-03-03T02:54:53Z"
          reason: Unknown
          startedAt: "2025-03-02T07:30:34Z"
      name: rancher
      ready: true
      restartCount: 33
      started: true
      state:
        running:
          startedAt: "2025-03-03T02:55:04Z"
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-7lhm6
        readOnly: true
        recursiveReadOnly: Disabled
    hostIP: 172.31.17.126
    hostIPs:
    - ip: 172.31.17.126
    phase: Running
    podIP: 10.42.0.248
    podIPs:
    - ip: 10.42.0.248
    qosClass: BestEffort
    startTime: "2025-02-05T06:44:07Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      kubectl.kubernetes.io/restartedAt: "2025-02-05T06:43:35Z"
    creationTimestamp: "2025-02-05T06:43:35Z"
    generateName: rancher-c6b4fd756-
    labels:
      app: rancher
      pod-template-hash: c6b4fd756
      release: rancher
    name: rancher-c6b4fd756-nnfwq
    namespace: cattle-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: rancher-c6b4fd756
      uid: 898e6c1f-00b4-4dcc-aad6-e05639c4eb7c
    resourceVersion: "196587"
    uid: 0b96752d-5933-4d74-b65d-c5fa0a3e0157
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchExpressions:
            - key: kubernetes.io/os
              operator: NotIn
              values:
              - windows
      podAntiAffinity:
        preferredDuringSchedulingIgnoredDuringExecution:
        - podAffinityTerm:
            labelSelector:
              matchExpressions:
              - key: app
                operator: In
                values:
                - rancher
            topologyKey: kubernetes.io/hostname
          weight: 100
    containers:
    - args:
      - --no-cacerts
      - --http-listen-port=80
      - --https-listen-port=443
      - --add-local=true
      env:
      - name: CATTLE_NAMESPACE
        value: cattle-system
      - name: CATTLE_PEER_SERVICE
        value: rancher
      image: rancher/rancher:v2.10.2
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 5
        httpGet:
          path: /healthz
          port: 80
          scheme: HTTP
        periodSeconds: 30
        successThreshold: 1
        timeoutSeconds: 5
      name: rancher
      ports:
      - containerPort: 80
        protocol: TCP
      readinessProbe:
        failureThreshold: 5
        httpGet:
          path: /healthz
          port: 80
          scheme: HTTP
        periodSeconds: 30
        successThreshold: 1
        timeoutSeconds: 5
      resources: {}
      startupProbe:
        failureThreshold: 12
        httpGet:
          path: /healthz
          port: 80
          scheme: HTTP
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 5
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-rp6ng
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: ip-172-31-17-126
    preemptionPolicy: PreemptLowerPriority
    priority: 1000000000
    priorityClassName: rancher-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: rancher
    serviceAccountName: rancher
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoSchedule
      key: cattle.io/os
      operator: Equal
      value: linux
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: kube-api-access-rp6ng
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-03-03T02:55:22Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-02-05T06:43:35Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2025-03-03T02:56:19Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2025-03-03T02:56:19Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-02-05T06:43:35Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://64f9d2889e22641ce80dc48d5b8042e3e0c5afb945efa58cb41f589096cdda92
      image: docker.io/rancher/rancher:v2.10.2
      imageID: docker.io/rancher/rancher@sha256:c2d87320b6936cf3f8f8aa57e0a4bad3338dadc9b572f3810afecf09e6ca278d
      lastState:
        terminated:
          containerID: containerd://d1ccb5fa7361d577130bc9038ec404960db15d58ada8e6483e5691bacf10685a
          exitCode: 255
          finishedAt: "2025-03-03T02:54:53Z"
          reason: Unknown
          startedAt: "2025-03-02T07:30:30Z"
      name: rancher
      ready: true
      restartCount: 33
      started: true
      state:
        running:
          startedAt: "2025-03-03T02:55:04Z"
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-rp6ng
        readOnly: true
        recursiveReadOnly: Disabled
    hostIP: 172.31.17.126
    hostIPs:
    - ip: 172.31.17.126
    phase: Running
    podIP: 10.42.0.243
    podIPs:
    - ip: 10.42.0.243
    qosClass: BestEffort
    startTime: "2025-02-05T06:43:35Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2025-02-27T05:06:32Z"
    generateName: rancher-webhook-554ffd94d8-
    labels:
      app: rancher-webhook
      pod-template-hash: 554ffd94d8
    name: rancher-webhook-554ffd94d8-9brqs
    namespace: cattle-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: rancher-webhook-554ffd94d8
      uid: 9689e049-3c6a-4881-828f-6405469c70ba
    resourceVersion: "196306"
    uid: 4d238557-01c6-4bfe-84b4-f17f10c4c187
  spec:
    containers:
    - env:
      - name: STAMP
      - name: ENABLE_MCM
        value: "true"
      - name: CATTLE_PORT
        value: "9443"
      - name: NAMESPACE
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.namespace
      image: rancher/rancher-webhook:v0.6.3
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 3
        httpGet:
          path: /healthz
          port: https
          scheme: HTTPS
        periodSeconds: 5
        successThreshold: 1
        timeoutSeconds: 1
      name: rancher-webhook
      ports:
      - containerPort: 9443
        name: https
        protocol: TCP
      resources: {}
      startupProbe:
        failureThreshold: 60
        httpGet:
          path: /healthz
          port: https
          scheme: HTTPS
        periodSeconds: 5
        successThreshold: 1
        timeoutSeconds: 1
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-57dbl
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: ip-172-31-17-126
    nodeSelector:
      kubernetes.io/os: linux
    preemptionPolicy: PreemptLowerPriority
    priority: 1000000000
    priorityClassName: rancher-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: rancher-webhook
    serviceAccountName: rancher-webhook
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoSchedule
      key: cattle.io/os
      operator: Equal
      value: linux
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: kube-api-access-57dbl
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-03-03T02:55:24Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-02-27T05:06:33Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2025-03-03T02:55:34Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2025-03-03T02:55:34Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-02-27T05:06:33Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://4c75dc4b983cfbc6983a89789695c633b6c7e6ac8d6c8cdb757def623b9fd731
      image: docker.io/rancher/rancher-webhook:v0.6.3
      imageID: docker.io/rancher/rancher-webhook@sha256:79a21351e4536a3e609b41bf8beade3833d100a509a74e868cef4c6a826b50b3
      lastState:
        terminated:
          containerID: containerd://9583deef33e0b8f5532f73fa2f8bffd0663572e73b6cf8c49939c0d43af2ca03
          exitCode: 255
          finishedAt: "2025-03-03T02:54:53Z"
          reason: Unknown
          startedAt: "2025-03-02T07:30:32Z"
      name: rancher-webhook
      ready: true
      restartCount: 6
      started: true
      state:
        running:
          startedAt: "2025-03-03T02:55:11Z"
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-57dbl
        readOnly: true
        recursiveReadOnly: Disabled
    hostIP: 172.31.17.126
    hostIPs:
    - ip: 172.31.17.126
    phase: Running
    podIP: 10.42.0.3
    podIPs:
    - ip: 10.42.0.3
    qosClass: BestEffort
    startTime: "2025-02-27T05:06:33Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2025-02-05T05:35:18Z"
    generateName: system-upgrade-controller-5fb67f585d-
    labels:
      pod-template-hash: 5fb67f585d
      upgrade.cattle.io/controller: system-upgrade-controller
    name: system-upgrade-controller-5fb67f585d-b54z4
    namespace: cattle-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: system-upgrade-controller-5fb67f585d
      uid: eb788a2c-eb8b-42a4-9ea4-2f8711f37bf7
    resourceVersion: "196238"
    uid: 70ab450e-3845-4418-9776-a621f08fe17b
  spec:
    affinity:
      nodeAffinity:
        preferredDuringSchedulingIgnoredDuringExecution:
        - preference:
            matchExpressions:
            - key: node-role.kubernetes.io/control-plane
              operator: In
              values:
              - "true"
          weight: 100
        - preference:
            matchExpressions:
            - key: node-role.kubernetes.io/master
              operator: In
              values:
              - "true"
          weight: 100
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchExpressions:
            - key: kubernetes.io/os
              operator: NotIn
              values:
              - windows
    containers:
    - env:
      - name: SYSTEM_UPGRADE_CONTROLLER_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.labels['upgrade.cattle.io/controller']
      - name: SYSTEM_UPGRADE_CONTROLLER_NAMESPACE
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.namespace
      envFrom:
      - configMapRef:
          name: system-upgrade-controller-config
      image: rancher/system-upgrade-controller:v0.14.2
      imagePullPolicy: IfNotPresent
      name: system-upgrade-controller
      resources: {}
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
        runAsGroup: 65534
        runAsNonRoot: true
        runAsUser: 65534
        seccompProfile:
          type: RuntimeDefault
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/ssl
        name: etc-ssl
        readOnly: true
      - mountPath: /etc/pki
        name: etc-pki
        readOnly: true
      - mountPath: /etc/ca-certificates
        name: etc-ca-certificates
        readOnly: true
      - mountPath: /tmp
        name: tmp
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-4gd4d
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: ip-172-31-17-126
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: system-upgrade-controller
    serviceAccountName: system-upgrade-controller
    terminationGracePeriodSeconds: 30
    tolerations:
    - operator: Exists
    volumes:
    - hostPath:
        path: /etc/ssl
        type: DirectoryOrCreate
      name: etc-ssl
    - hostPath:
        path: /etc/pki
        type: DirectoryOrCreate
      name: etc-pki
    - hostPath:
        path: /etc/ca-certificates
        type: DirectoryOrCreate
      name: etc-ca-certificates
    - emptyDir: {}
      name: tmp
    - name: kube-api-access-4gd4d
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-03-03T02:55:23Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-02-05T05:35:18Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2025-03-03T02:55:23Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2025-03-03T02:55:23Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-02-05T05:35:18Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://34f8f2039ffe850f5a0ee5395582ffbf1eceb54e81ef3a39eed1382c69c6fc9f
      image: docker.io/rancher/system-upgrade-controller:v0.14.2
      imageID: docker.io/rancher/system-upgrade-controller@sha256:3cdbfdd90f814702cefb832fc4bdb09ea93865a4d06c6bafd019d1dc6a9f34c9
      lastState:
        terminated:
          containerID: containerd://b18f891702ab43dbea7e57dc9658fba70611d5480ef32cd4b33fdd491e5f3cbe
          exitCode: 255
          finishedAt: "2025-03-03T02:54:53Z"
          reason: Unknown
          startedAt: "2025-03-02T07:30:26Z"
      name: system-upgrade-controller
      ready: true
      restartCount: 33
      started: true
      state:
        running:
          startedAt: "2025-03-03T02:55:13Z"
      volumeMounts:
      - mountPath: /etc/ssl
        name: etc-ssl
        readOnly: true
        recursiveReadOnly: Disabled
      - mountPath: /etc/pki
        name: etc-pki
        readOnly: true
        recursiveReadOnly: Disabled
      - mountPath: /etc/ca-certificates
        name: etc-ca-certificates
        readOnly: true
        recursiveReadOnly: Disabled
      - mountPath: /tmp
        name: tmp
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-4gd4d
        readOnly: true
        recursiveReadOnly: Disabled
    hostIP: 172.31.17.126
    hostIPs:
    - ip: 172.31.17.126
    phase: Running
    podIP: 10.42.0.9
    podIPs:
    - ip: 10.42.0.9
    qosClass: BestEffort
    startTime: "2025-02-05T05:35:18Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      prometheus.io/path: /metrics
      prometheus.io/port: "9402"
      prometheus.io/scrape: "true"
    creationTimestamp: "2025-02-05T04:55:46Z"
    generateName: cert-manager-6846f9d58c-
    labels:
      app: cert-manager
      app.kubernetes.io/component: controller
      app.kubernetes.io/instance: cert-manager
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: cert-manager
      app.kubernetes.io/version: v1.17.0
      helm.sh/chart: cert-manager-v1.17.0
      pod-template-hash: 6846f9d58c
    name: cert-manager-6846f9d58c-tcs9z
    namespace: cert-manager
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: cert-manager-6846f9d58c
      uid: d9803be7-33b4-489c-ab8f-8f2d794f2adb
    resourceVersion: "196223"
    uid: fdfc918a-3a5c-45fe-8022-4be176632fff
  spec:
    containers:
    - args:
      - --v=2
      - --cluster-resource-namespace=$(POD_NAMESPACE)
      - --leader-election-namespace=kube-system
      - --acme-http01-solver-image=quay.io/jetstack/cert-manager-acmesolver:v1.17.0
      - --max-concurrent-challenges=60
      env:
      - name: POD_NAMESPACE
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.namespace
      image: quay.io/jetstack/cert-manager-controller:v1.17.0
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 8
        httpGet:
          path: /livez
          port: http-healthz
          scheme: HTTP
        initialDelaySeconds: 10
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 15
      name: cert-manager-controller
      ports:
      - containerPort: 9402
        name: http-metrics
        protocol: TCP
      - containerPort: 9403
        name: http-healthz
        protocol: TCP
      resources: {}
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
        readOnlyRootFilesystem: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-jjkm4
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: false
    nodeName: ip-172-31-17-126
    nodeSelector:
      kubernetes.io/os: linux
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      runAsNonRoot: true
      seccompProfile:
        type: RuntimeDefault
    serviceAccount: cert-manager
    serviceAccountName: cert-manager
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: kube-api-access-jjkm4
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-03-03T02:55:22Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-02-05T04:55:46Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2025-03-03T02:55:22Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2025-03-03T02:55:22Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-02-05T04:55:46Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://7ea465ec2fabdbaf796f3f30df35ef2cc097f0a1b517c02fa7ae6903e659ee4b
      image: quay.io/jetstack/cert-manager-controller:v1.17.0
      imageID: quay.io/jetstack/cert-manager-controller@sha256:424263d7a11b54b56b26953d22bf28bd87338056b518dd007e391ad0b702c5f0
      lastState:
        terminated:
          containerID: containerd://248ed68dbc4704b6119c0f3c78b3f2223969d379979d0172c8da29db535464f1
          exitCode: 255
          finishedAt: "2025-03-03T02:54:53Z"
          reason: Unknown
          startedAt: "2025-03-02T07:30:34Z"
      name: cert-manager-controller
      ready: true
      restartCount: 34
      started: true
      state:
        running:
          startedAt: "2025-03-03T02:55:03Z"
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-jjkm4
        readOnly: true
        recursiveReadOnly: Disabled
    hostIP: 172.31.17.126
    hostIPs:
    - ip: 172.31.17.126
    phase: Running
    podIP: 10.42.0.246
    podIPs:
    - ip: 10.42.0.246
    qosClass: BestEffort
    startTime: "2025-02-05T04:55:46Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      prometheus.io/path: /metrics
      prometheus.io/port: "9402"
      prometheus.io/scrape: "true"
    creationTimestamp: "2025-02-05T04:55:46Z"
    generateName: cert-manager-cainjector-6fc95694fb-
    labels:
      app: cainjector
      app.kubernetes.io/component: cainjector
      app.kubernetes.io/instance: cert-manager
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: cainjector
      app.kubernetes.io/version: v1.17.0
      helm.sh/chart: cert-manager-v1.17.0
      pod-template-hash: 6fc95694fb
    name: cert-manager-cainjector-6fc95694fb-4z99j
    namespace: cert-manager
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: cert-manager-cainjector-6fc95694fb
      uid: 6d3904c6-d088-4b28-8e8f-725bacc5b49d
    resourceVersion: "196275"
    uid: 6bdd12af-3d7a-4ec2-86f6-f5a245407a39
  spec:
    containers:
    - args:
      - --v=2
      - --leader-election-namespace=kube-system
      env:
      - name: POD_NAMESPACE
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.namespace
      image: quay.io/jetstack/cert-manager-cainjector:v1.17.0
      imagePullPolicy: IfNotPresent
      name: cert-manager-cainjector
      ports:
      - containerPort: 9402
        name: http-metrics
        protocol: TCP
      resources: {}
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
        readOnlyRootFilesystem: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-mn4lf
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: false
    nodeName: ip-172-31-17-126
    nodeSelector:
      kubernetes.io/os: linux
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      runAsNonRoot: true
      seccompProfile:
        type: RuntimeDefault
    serviceAccount: cert-manager-cainjector
    serviceAccountName: cert-manager-cainjector
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: kube-api-access-mn4lf
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-03-03T02:55:23Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-02-05T04:55:46Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2025-03-03T02:55:23Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2025-03-03T02:55:23Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-02-05T04:55:46Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://d233d4445c100d90c0e6a1ff7e7a7311cce44bcf341c96923eca5a814bec1609
      image: quay.io/jetstack/cert-manager-cainjector:v1.17.0
      imageID: quay.io/jetstack/cert-manager-cainjector@sha256:63ea34d0e82ebfd91242498c54ad104d6bce7a56c7a2e24de5461aabd744a9b2
      lastState:
        terminated:
          containerID: containerd://05232692bfe6c15863f84c51526aa8092ea39c3e5c613475eba6b9246b6db2c1
          exitCode: 255
          finishedAt: "2025-03-03T02:54:53Z"
          reason: Unknown
          startedAt: "2025-03-02T07:30:25Z"
      name: cert-manager-cainjector
      ready: true
      restartCount: 34
      started: true
      state:
        running:
          startedAt: "2025-03-03T02:55:16Z"
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-mn4lf
        readOnly: true
        recursiveReadOnly: Disabled
    hostIP: 172.31.17.126
    hostIPs:
    - ip: 172.31.17.126
    phase: Running
    podIP: 10.42.0.12
    podIPs:
    - ip: 10.42.0.12
    qosClass: BestEffort
    startTime: "2025-02-05T04:55:46Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      prometheus.io/path: /metrics
      prometheus.io/port: "9402"
      prometheus.io/scrape: "true"
    creationTimestamp: "2025-02-05T04:55:46Z"
    generateName: cert-manager-webhook-66f876d88c-
    labels:
      app: webhook
      app.kubernetes.io/component: webhook
      app.kubernetes.io/instance: cert-manager
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: webhook
      app.kubernetes.io/version: v1.17.0
      helm.sh/chart: cert-manager-v1.17.0
      pod-template-hash: 66f876d88c
    name: cert-manager-webhook-66f876d88c-zqfln
    namespace: cert-manager
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: cert-manager-webhook-66f876d88c
      uid: 4700a818-3bd4-4c8f-b843-f99c1d1125a7
    resourceVersion: "196218"
    uid: e5df2a13-35df-4d45-879c-2f6ee02f52f3
  spec:
    containers:
    - args:
      - --v=2
      - --secure-port=10250
      - --dynamic-serving-ca-secret-namespace=$(POD_NAMESPACE)
      - --dynamic-serving-ca-secret-name=cert-manager-webhook-ca
      - --dynamic-serving-dns-names=cert-manager-webhook
      - --dynamic-serving-dns-names=cert-manager-webhook.$(POD_NAMESPACE)
      - --dynamic-serving-dns-names=cert-manager-webhook.$(POD_NAMESPACE).svc
      env:
      - name: POD_NAMESPACE
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.namespace
      image: quay.io/jetstack/cert-manager-webhook:v1.17.0
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 3
        httpGet:
          path: /livez
          port: 6080
          scheme: HTTP
        initialDelaySeconds: 60
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      name: cert-manager-webhook
      ports:
      - containerPort: 10250
        name: https
        protocol: TCP
      - containerPort: 6080
        name: healthcheck
        protocol: TCP
      - containerPort: 9402
        name: http-metrics
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /healthz
          port: 6080
          scheme: HTTP
        initialDelaySeconds: 5
        periodSeconds: 5
        successThreshold: 1
        timeoutSeconds: 1
      resources: {}
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
        readOnlyRootFilesystem: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-pbqc2
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: false
    nodeName: ip-172-31-17-126
    nodeSelector:
      kubernetes.io/os: linux
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      runAsNonRoot: true
      seccompProfile:
        type: RuntimeDefault
    serviceAccount: cert-manager-webhook
    serviceAccountName: cert-manager-webhook
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: kube-api-access-pbqc2
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-03-03T02:55:26Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-02-05T04:55:46Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2025-03-03T02:55:27Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2025-03-03T02:55:27Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-02-05T04:55:46Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://e8e654cba571687d07d5f7a7eb114c020dd386a876333925d7b8e05f102a832b
      image: quay.io/jetstack/cert-manager-webhook:v1.17.0
      imageID: quay.io/jetstack/cert-manager-webhook@sha256:6e8f1c96b7acd9526b7dec7ce8af45e4a774877e24be183e3ce36a25c9d5ed3b
      lastState:
        terminated:
          containerID: containerd://3ef213a4a5cf921f5560093cb712d4eff11492ce5237a4afeb7883dbf4f2f23c
          exitCode: 255
          finishedAt: "2025-03-03T02:54:53Z"
          reason: Unknown
          startedAt: "2025-03-02T07:30:25Z"
      name: cert-manager-webhook
      ready: true
      restartCount: 34
      started: true
      state:
        running:
          startedAt: "2025-03-03T02:55:03Z"
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-pbqc2
        readOnly: true
        recursiveReadOnly: Disabled
    hostIP: 172.31.17.126
    hostIPs:
    - ip: 172.31.17.126
    phase: Running
    podIP: 10.42.0.245
    podIPs:
    - ip: 10.42.0.245
    qosClass: BestEffort
    startTime: "2025-02-05T04:55:46Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2025-02-27T12:30:54Z"
    generateName: flask-lstm-6f8bc77f4f-
    labels:
      app: flask-lstm
      pod-template-hash: 6f8bc77f4f
    name: flask-lstm-6f8bc77f4f-t4mrz
    namespace: default
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: flask-lstm-6f8bc77f4f
      uid: 73423626-c514-413c-968b-36a624ec0414
    resourceVersion: "196071"
    uid: 01499fe1-3bcc-4d74-8a67-cecfd55219f6
  spec:
    containers:
    - image: gnh374/lstm-model
      imagePullPolicy: Always
      name: flask-lstm
      ports:
      - containerPort: 5000
        protocol: TCP
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-ll7qg
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: ip-172-31-17-126
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: default
    serviceAccountName: default
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: kube-api-access-ll7qg
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-03-03T02:55:21Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-02-27T12:30:54Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2025-03-03T02:55:21Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2025-03-03T02:55:21Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-02-27T12:30:54Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://5a0ce47f904619ed1247f6793faf3490266c51a14a77aee8a6a9da90ed18141e
      image: docker.io/gnh374/lstm-model:latest
      imageID: docker.io/gnh374/lstm-model@sha256:0567574c0330394c0d7e4fac6a9f4532ca3218e6c7bf3d5a2631b82cf24f97c1
      lastState:
        terminated:
          containerID: containerd://8124b99d20cf60379b970aa9ef090cc3968ad68f30ce0ee1d2e452182a0eb739
          exitCode: 255
          finishedAt: "2025-03-03T02:54:53Z"
          reason: Unknown
          startedAt: "2025-03-02T07:32:13Z"
      name: flask-lstm
      ready: true
      restartCount: 1
      started: true
      state:
        running:
          startedAt: "2025-03-03T02:55:03Z"
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-ll7qg
        readOnly: true
        recursiveReadOnly: Disabled
    hostIP: 172.31.17.126
    hostIPs:
    - ip: 172.31.17.126
    phase: Running
    podIP: 10.42.0.242
    podIPs:
    - ip: 10.42.0.242
    qosClass: BestEffort
    startTime: "2025-02-27T12:30:54Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2025-02-27T04:33:07Z"
    generateName: rke2-machineconfig-cleanup-cronjob-29010245-
    labels:
      batch.kubernetes.io/controller-uid: 0d1a231a-721c-4022-a184-4f4563e74802
      batch.kubernetes.io/job-name: rke2-machineconfig-cleanup-cronjob-29010245
      controller-uid: 0d1a231a-721c-4022-a184-4f4563e74802
      job-name: rke2-machineconfig-cleanup-cronjob-29010245
    name: rke2-machineconfig-cleanup-cronjob-29010245-45q55
    namespace: fleet-default
    ownerReferences:
    - apiVersion: batch/v1
      blockOwnerDeletion: true
      controller: true
      kind: Job
      name: rke2-machineconfig-cleanup-cronjob-29010245
      uid: 0d1a231a-721c-4022-a184-4f4563e74802
    resourceVersion: "146667"
    uid: f7cb01bb-479c-49eb-8b1b-0b759ca547a8
  spec:
    containers:
    - args:
      - /helper/cleanup.sh
      command:
      - /bin/sh
      env:
      - name: CATTLE_SERVER
        value: https://44.220.0.65.sslip.io
      - name: CATTLE_CA_CHECKSUM
      - name: CATTLE_TOKEN
        value: z8l87xnn8j6xwq4gkk6kb529xv5r9whb48vvqk7kvbpnm5znd2ktbc
      image: rancher/rancher-agent:v2.10.2
      imagePullPolicy: IfNotPresent
      name: rke2-machineconfig-cleanup-pod
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /helper
        name: config-volume
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-vz7jg
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: ip-172-31-17-126
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Never
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: rke2-machineconfig-cleanup-sa
    serviceAccountName: rke2-machineconfig-cleanup-sa
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - configMap:
        defaultMode: 420
        name: rke2-machineconfig-cleanup-script
      name: config-volume
    - name: kube-api-access-vz7jg
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-02-27T04:33:18Z"
      status: "False"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-02-27T04:33:07Z"
      reason: PodCompleted
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2025-02-27T04:33:17Z"
      reason: PodCompleted
      status: "False"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2025-02-27T04:33:17Z"
      reason: PodCompleted
      status: "False"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-02-27T04:33:07Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://b74b12e7686988189662808388ad049af4ba6c5e55673140707fb708f2b3923a
      image: docker.io/rancher/rancher-agent:v2.10.2
      imageID: docker.io/rancher/rancher-agent@sha256:0678aacdd192768b9112272f00332fa7303ef497b058b46a0f4e1cbbdee47350
      lastState: {}
      name: rke2-machineconfig-cleanup-pod
      ready: false
      restartCount: 0
      started: false
      state:
        terminated:
          containerID: containerd://b74b12e7686988189662808388ad049af4ba6c5e55673140707fb708f2b3923a
          exitCode: 0
          finishedAt: "2025-02-27T04:33:16Z"
          reason: Completed
          startedAt: "2025-02-27T04:33:10Z"
      volumeMounts:
      - mountPath: /helper
        name: config-volume
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-vz7jg
        readOnly: true
        recursiveReadOnly: Disabled
    hostIP: 172.31.17.126
    hostIPs:
    - ip: 172.31.17.126
    phase: Succeeded
    qosClass: BestEffort
    startTime: "2025-02-27T04:33:07Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2025-03-02T07:30:56Z"
    generateName: rke2-machineconfig-cleanup-cronjob-29014565-
    labels:
      batch.kubernetes.io/controller-uid: efe74705-8c46-4e66-a2a7-23a4fb54df89
      batch.kubernetes.io/job-name: rke2-machineconfig-cleanup-cronjob-29014565
      controller-uid: efe74705-8c46-4e66-a2a7-23a4fb54df89
      job-name: rke2-machineconfig-cleanup-cronjob-29014565
    name: rke2-machineconfig-cleanup-cronjob-29014565-l4gs2
    namespace: fleet-default
    ownerReferences:
    - apiVersion: batch/v1
      blockOwnerDeletion: true
      controller: true
      kind: Job
      name: rke2-machineconfig-cleanup-cronjob-29014565
      uid: efe74705-8c46-4e66-a2a7-23a4fb54df89
    resourceVersion: "195982"
    uid: c5651c34-615f-44fc-a170-1a6578fd5f1c
  spec:
    containers:
    - args:
      - /helper/cleanup.sh
      command:
      - /bin/sh
      env:
      - name: CATTLE_SERVER
        value: https://44.220.0.65.sslip.io
      - name: CATTLE_CA_CHECKSUM
      - name: CATTLE_TOKEN
        value: z8l87xnn8j6xwq4gkk6kb529xv5r9whb48vvqk7kvbpnm5znd2ktbc
      image: rancher/rancher-agent:v2.10.2
      imagePullPolicy: IfNotPresent
      name: rke2-machineconfig-cleanup-pod
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /helper
        name: config-volume
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-zmds6
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: ip-172-31-17-126
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Never
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: rke2-machineconfig-cleanup-sa
    serviceAccountName: rke2-machineconfig-cleanup-sa
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - configMap:
        defaultMode: 420
        name: rke2-machineconfig-cleanup-script
      name: config-volume
    - name: kube-api-access-zmds6
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-03-02T07:31:11Z"
      status: "False"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-03-02T07:30:57Z"
      reason: PodCompleted
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2025-03-02T07:31:09Z"
      reason: PodCompleted
      status: "False"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2025-03-02T07:31:09Z"
      reason: PodCompleted
      status: "False"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-03-02T07:30:57Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://20feabb6618e078bb0cbce05307872d88460bc5772d8b4f665380cff16189552
      image: docker.io/rancher/rancher-agent:v2.10.2
      imageID: docker.io/rancher/rancher-agent@sha256:0678aacdd192768b9112272f00332fa7303ef497b058b46a0f4e1cbbdee47350
      lastState: {}
      name: rke2-machineconfig-cleanup-pod
      ready: false
      restartCount: 0
      started: false
      state:
        terminated:
          containerID: containerd://20feabb6618e078bb0cbce05307872d88460bc5772d8b4f665380cff16189552
          exitCode: 0
          finishedAt: "2025-03-02T07:31:08Z"
          reason: Completed
          startedAt: "2025-03-02T07:30:59Z"
      volumeMounts:
      - mountPath: /helper
        name: config-volume
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-zmds6
        readOnly: true
        recursiveReadOnly: Disabled
    hostIP: 172.31.17.126
    hostIPs:
    - ip: 172.31.17.126
    phase: Succeeded
    qosClass: BestEffort
    startTime: "2025-03-02T07:30:57Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2025-03-03T02:55:28Z"
    generateName: rke2-machineconfig-cleanup-cronjob-29016005-
    labels:
      batch.kubernetes.io/controller-uid: 51e431b0-336c-4b13-a974-2f8fae68aa95
      batch.kubernetes.io/job-name: rke2-machineconfig-cleanup-cronjob-29016005
      controller-uid: 51e431b0-336c-4b13-a974-2f8fae68aa95
      job-name: rke2-machineconfig-cleanup-cronjob-29016005
    name: rke2-machineconfig-cleanup-cronjob-29016005-4nwpb
    namespace: fleet-default
    ownerReferences:
    - apiVersion: batch/v1
      blockOwnerDeletion: true
      controller: true
      kind: Job
      name: rke2-machineconfig-cleanup-cronjob-29016005
      uid: 51e431b0-336c-4b13-a974-2f8fae68aa95
    resourceVersion: "196437"
    uid: 8e5fcaf3-9e50-42fd-acbe-26ced7829fbf
  spec:
    containers:
    - args:
      - /helper/cleanup.sh
      command:
      - /bin/sh
      env:
      - name: CATTLE_SERVER
        value: https://44.220.0.65.sslip.io
      - name: CATTLE_CA_CHECKSUM
      - name: CATTLE_TOKEN
        value: z8l87xnn8j6xwq4gkk6kb529xv5r9whb48vvqk7kvbpnm5znd2ktbc
      image: rancher/rancher-agent:v2.10.2
      imagePullPolicy: IfNotPresent
      name: rke2-machineconfig-cleanup-pod
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /helper
        name: config-volume
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-cggsm
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: ip-172-31-17-126
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Never
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: rke2-machineconfig-cleanup-sa
    serviceAccountName: rke2-machineconfig-cleanup-sa
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - configMap:
        defaultMode: 420
        name: rke2-machineconfig-cleanup-script
      name: config-volume
    - name: kube-api-access-cggsm
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-03-03T02:55:40Z"
      status: "False"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-03-03T02:55:28Z"
      reason: PodCompleted
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2025-03-03T02:55:38Z"
      reason: PodCompleted
      status: "False"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2025-03-03T02:55:38Z"
      reason: PodCompleted
      status: "False"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-03-03T02:55:28Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://a3c218e30cbcdb2dcedbf964109ced66f406a456dde46d898b8d70b2911b1140
      image: docker.io/rancher/rancher-agent:v2.10.2
      imageID: docker.io/rancher/rancher-agent@sha256:0678aacdd192768b9112272f00332fa7303ef497b058b46a0f4e1cbbdee47350
      lastState: {}
      name: rke2-machineconfig-cleanup-pod
      ready: false
      restartCount: 0
      started: false
      state:
        terminated:
          containerID: containerd://a3c218e30cbcdb2dcedbf964109ced66f406a456dde46d898b8d70b2911b1140
          exitCode: 0
          finishedAt: "2025-03-03T02:55:38Z"
          reason: Completed
          startedAt: "2025-03-03T02:55:31Z"
      volumeMounts:
      - mountPath: /helper
        name: config-volume
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-cggsm
        readOnly: true
        recursiveReadOnly: Disabled
    hostIP: 172.31.17.126
    hostIPs:
    - ip: 172.31.17.126
    phase: Succeeded
    podIP: 10.42.0.17
    podIPs:
    - ip: 10.42.0.17
    qosClass: BestEffort
    startTime: "2025-03-03T02:55:28Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      kubectl.kubernetes.io/restartedAt: "2025-02-05T06:42:43Z"
    creationTimestamp: "2025-02-05T06:42:43Z"
    generateName: coredns-f59c78b45-
    labels:
      k8s-app: kube-dns
      pod-template-hash: f59c78b45
    name: coredns-f59c78b45-ttm2h
    namespace: kube-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: coredns-f59c78b45
      uid: bbae2dd5-bb82-4268-b04e-bc8d721a9452
    resourceVersion: "196043"
    uid: 6ef135ef-0716-40f9-a7bf-4c33fdf9af44
  spec:
    containers:
    - args:
      - -conf
      - /etc/coredns/Corefile
      image: rancher/mirrored-coredns-coredns:1.12.0
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 3
        httpGet:
          path: /health
          port: 8080
          scheme: HTTP
        initialDelaySeconds: 60
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      name: coredns
      ports:
      - containerPort: 53
        name: dns
        protocol: UDP
      - containerPort: 53
        name: dns-tcp
        protocol: TCP
      - containerPort: 9153
        name: metrics
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /ready
          port: 8181
          scheme: HTTP
        periodSeconds: 2
        successThreshold: 1
        timeoutSeconds: 1
      resources:
        limits:
          memory: 170Mi
        requests:
          cpu: 100m
          memory: 70Mi
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          add:
          - NET_BIND_SERVICE
          drop:
          - all
        readOnlyRootFilesystem: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/coredns
        name: config-volume
        readOnly: true
      - mountPath: /etc/coredns/custom
        name: custom-config-volume
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-nrgkg
        readOnly: true
    dnsPolicy: Default
    enableServiceLinks: true
    nodeName: ip-172-31-17-126
    nodeSelector:
      kubernetes.io/os: linux
    preemptionPolicy: PreemptLowerPriority
    priority: 2000000000
    priorityClassName: system-cluster-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: coredns
    serviceAccountName: coredns
    terminationGracePeriodSeconds: 30
    tolerations:
    - key: CriticalAddonsOnly
      operator: Exists
    - effect: NoSchedule
      key: node-role.kubernetes.io/control-plane
      operator: Exists
    - effect: NoSchedule
      key: node-role.kubernetes.io/master
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    topologySpreadConstraints:
    - labelSelector:
        matchLabels:
          k8s-app: kube-dns
      maxSkew: 1
      topologyKey: kubernetes.io/hostname
      whenUnsatisfiable: DoNotSchedule
    volumes:
    - configMap:
        defaultMode: 420
        items:
        - key: Corefile
          path: Corefile
        - key: NodeHosts
          path: NodeHosts
        name: coredns
      name: config-volume
    - configMap:
        defaultMode: 420
        name: coredns-custom
        optional: true
      name: custom-config-volume
    - name: kube-api-access-nrgkg
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-03-03T02:55:19Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-02-05T06:42:43Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2025-03-03T02:55:19Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2025-03-03T02:55:19Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-02-05T06:42:43Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://c808e95011f107837bf78670ec03a69757027a740df34dadfcbd262aa8956414
      image: docker.io/rancher/mirrored-coredns-coredns:1.12.0
      imageID: docker.io/rancher/mirrored-coredns-coredns@sha256:82979ddf442c593027a57239ad90616deb874e90c365d1a96ad508c2104bdea5
      lastState:
        terminated:
          containerID: containerd://1f88bd09c68f2b94ed41832b34a51bbd3af8fcec94c44094c0304a5b16a8dd4c
          exitCode: 255
          finishedAt: "2025-03-03T02:54:53Z"
          reason: Unknown
          startedAt: "2025-03-02T07:30:30Z"
      name: coredns
      ready: true
      restartCount: 33
      started: true
      state:
        running:
          startedAt: "2025-03-03T02:55:12Z"
      volumeMounts:
      - mountPath: /etc/coredns
        name: config-volume
        readOnly: true
        recursiveReadOnly: Disabled
      - mountPath: /etc/coredns/custom
        name: custom-config-volume
        readOnly: true
        recursiveReadOnly: Disabled
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-nrgkg
        readOnly: true
        recursiveReadOnly: Disabled
    hostIP: 172.31.17.126
    hostIPs:
    - ip: 172.31.17.126
    phase: Running
    podIP: 10.42.0.6
    podIPs:
    - ip: 10.42.0.6
    qosClass: Burstable
    startTime: "2025-02-05T06:42:43Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      helmcharts.helm.cattle.io/configHash: SHA256=E3B0C44298FC1C149AFBF4C8996FB92427AE41E4649B934CA495991B7852B855
    creationTimestamp: "2025-02-05T04:54:34Z"
    generateName: helm-install-traefik-crd-
    labels:
      batch.kubernetes.io/controller-uid: 627f7bc5-598d-43a7-bc02-d83fb5a0e297
      batch.kubernetes.io/job-name: helm-install-traefik-crd
      controller-uid: 627f7bc5-598d-43a7-bc02-d83fb5a0e297
      helmcharts.helm.cattle.io/chart: traefik-crd
      job-name: helm-install-traefik-crd
    name: helm-install-traefik-crd-2q4c4
    namespace: kube-system
    ownerReferences:
    - apiVersion: batch/v1
      blockOwnerDeletion: true
      controller: true
      kind: Job
      name: helm-install-traefik-crd
      uid: 627f7bc5-598d-43a7-bc02-d83fb5a0e297
    resourceVersion: "1519"
    uid: d5113041-69d5-4c23-a010-9fc8daa3fcaf
  spec:
    containers:
    - args:
      - install
      env:
      - name: NAME
        value: traefik-crd
      - name: VERSION
      - name: REPO
      - name: HELM_DRIVER
        value: secret
      - name: CHART_NAMESPACE
        value: kube-system
      - name: CHART
        value: https://%{KUBERNETES_API}%/static/charts/traefik-crd-27.0.201+up27.0.2.tgz
      - name: HELM_VERSION
      - name: TARGET_NAMESPACE
        value: kube-system
      - name: AUTH_PASS_CREDENTIALS
        value: "false"
      - name: INSECURE_SKIP_TLS_VERIFY
        value: "false"
      - name: PLAIN_HTTP
        value: "false"
      - name: NO_PROXY
        value: .svc,.cluster.local,10.42.0.0/16,10.43.0.0/16
      - name: FAILURE_POLICY
        value: reinstall
      image: rancher/klipper-helm:v0.9.3-build20241008
      imagePullPolicy: IfNotPresent
      name: helm
      resources: {}
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
        readOnlyRootFilesystem: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /home/klipper-helm/.helm
        name: klipper-helm
      - mountPath: /home/klipper-helm/.cache
        name: klipper-cache
      - mountPath: /home/klipper-helm/.config
        name: klipper-config
      - mountPath: /tmp
        name: tmp
      - mountPath: /config
        name: values
      - mountPath: /chart
        name: content
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-2wh8r
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: ip-172-31-17-126
    nodeSelector:
      kubernetes.io/os: linux
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: OnFailure
    schedulerName: default-scheduler
    securityContext:
      runAsNonRoot: true
      seccompProfile:
        type: RuntimeDefault
    serviceAccount: helm-traefik-crd
    serviceAccountName: helm-traefik-crd
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - emptyDir:
        medium: Memory
      name: klipper-helm
    - emptyDir:
        medium: Memory
      name: klipper-cache
    - emptyDir:
        medium: Memory
      name: klipper-config
    - emptyDir:
        medium: Memory
      name: tmp
    - name: values
      secret:
        defaultMode: 420
        secretName: chart-values-traefik-crd
    - configMap:
        defaultMode: 420
        name: chart-content-traefik-crd
      name: content
    - name: kube-api-access-2wh8r
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-02-05T04:54:53Z"
      status: "False"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-02-05T04:54:37Z"
      reason: PodCompleted
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2025-02-05T04:54:51Z"
      reason: PodCompleted
      status: "False"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2025-02-05T04:54:51Z"
      reason: PodCompleted
      status: "False"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-02-05T04:54:37Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://4e2f8846b7fb97582b8f0fefb77cafd502fe4eb1a9dc3d113e1bc402c873104d
      image: docker.io/rancher/klipper-helm:v0.9.3-build20241008
      imageID: docker.io/rancher/klipper-helm@sha256:73ff7ef399717ba8339559054557bd427bdafb47db112165a8c0c358d1ca0283
      lastState: {}
      name: helm
      ready: false
      restartCount: 0
      started: false
      state:
        terminated:
          containerID: containerd://4e2f8846b7fb97582b8f0fefb77cafd502fe4eb1a9dc3d113e1bc402c873104d
          exitCode: 0
          finishedAt: "2025-02-05T04:54:48Z"
          message: |
            Installing helm chart
          reason: Completed
          startedAt: "2025-02-05T04:54:47Z"
      volumeMounts:
      - mountPath: /home/klipper-helm/.helm
        name: klipper-helm
      - mountPath: /home/klipper-helm/.cache
        name: klipper-cache
      - mountPath: /home/klipper-helm/.config
        name: klipper-config
      - mountPath: /tmp
        name: tmp
      - mountPath: /config
        name: values
      - mountPath: /chart
        name: content
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-2wh8r
        readOnly: true
        recursiveReadOnly: Disabled
    hostIP: 172.31.17.126
    hostIPs:
    - ip: 172.31.17.126
    phase: Succeeded
    qosClass: BestEffort
    startTime: "2025-02-05T04:54:37Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      helmcharts.helm.cattle.io/configHash: SHA256=EF658189C81681410C1B7E28CB9F0030170938B99D1CD7EC9E40B82278BB67A8
    creationTimestamp: "2025-02-05T04:54:34Z"
    generateName: helm-install-traefik-
    labels:
      batch.kubernetes.io/controller-uid: f75d2c0c-a946-47c2-be0b-8877e556c103
      batch.kubernetes.io/job-name: helm-install-traefik
      controller-uid: f75d2c0c-a946-47c2-be0b-8877e556c103
      helmcharts.helm.cattle.io/chart: traefik
      job-name: helm-install-traefik
    name: helm-install-traefik-h562n
    namespace: kube-system
    ownerReferences:
    - apiVersion: batch/v1
      blockOwnerDeletion: true
      controller: true
      kind: Job
      name: helm-install-traefik
      uid: f75d2c0c-a946-47c2-be0b-8877e556c103
    resourceVersion: "1547"
    uid: 36831dd6-c041-4815-b569-e1133f1a2e7d
  spec:
    containers:
    - args:
      - install
      - --set-string
      - global.systemDefaultRegistry=
      env:
      - name: NAME
        value: traefik
      - name: VERSION
      - name: REPO
      - name: HELM_DRIVER
        value: secret
      - name: CHART_NAMESPACE
        value: kube-system
      - name: CHART
        value: https://%{KUBERNETES_API}%/static/charts/traefik-27.0.201+up27.0.2.tgz
      - name: HELM_VERSION
      - name: TARGET_NAMESPACE
        value: kube-system
      - name: AUTH_PASS_CREDENTIALS
        value: "false"
      - name: INSECURE_SKIP_TLS_VERIFY
        value: "false"
      - name: PLAIN_HTTP
        value: "false"
      - name: NO_PROXY
        value: .svc,.cluster.local,10.42.0.0/16,10.43.0.0/16
      - name: FAILURE_POLICY
        value: reinstall
      image: rancher/klipper-helm:v0.9.3-build20241008
      imagePullPolicy: IfNotPresent
      name: helm
      resources: {}
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
        readOnlyRootFilesystem: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /home/klipper-helm/.helm
        name: klipper-helm
      - mountPath: /home/klipper-helm/.cache
        name: klipper-cache
      - mountPath: /home/klipper-helm/.config
        name: klipper-config
      - mountPath: /tmp
        name: tmp
      - mountPath: /config
        name: values
      - mountPath: /chart
        name: content
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-m5h92
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: ip-172-31-17-126
    nodeSelector:
      kubernetes.io/os: linux
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: OnFailure
    schedulerName: default-scheduler
    securityContext:
      runAsNonRoot: true
      seccompProfile:
        type: RuntimeDefault
    serviceAccount: helm-traefik
    serviceAccountName: helm-traefik
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - emptyDir:
        medium: Memory
      name: klipper-helm
    - emptyDir:
        medium: Memory
      name: klipper-cache
    - emptyDir:
        medium: Memory
      name: klipper-config
    - emptyDir:
        medium: Memory
      name: tmp
    - name: values
      secret:
        defaultMode: 420
        secretName: chart-values-traefik
    - configMap:
        defaultMode: 420
        name: chart-content-traefik
      name: content
    - name: kube-api-access-m5h92
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-02-05T04:54:54Z"
      status: "False"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-02-05T04:54:37Z"
      reason: PodCompleted
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2025-02-05T04:54:51Z"
      reason: PodCompleted
      status: "False"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2025-02-05T04:54:51Z"
      reason: PodCompleted
      status: "False"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-02-05T04:54:37Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://84547f53dc8299bc01ef5b94c7070bd674eec7a6d59b460260443841f416b49b
      image: docker.io/rancher/klipper-helm:v0.9.3-build20241008
      imageID: docker.io/rancher/klipper-helm@sha256:73ff7ef399717ba8339559054557bd427bdafb47db112165a8c0c358d1ca0283
      lastState: {}
      name: helm
      ready: false
      restartCount: 1
      started: false
      state:
        terminated:
          containerID: containerd://84547f53dc8299bc01ef5b94c7070bd674eec7a6d59b460260443841f416b49b
          exitCode: 0
          finishedAt: "2025-02-05T04:54:52Z"
          message: |
            Installing helm chart
          reason: Completed
          startedAt: "2025-02-05T04:54:51Z"
      volumeMounts:
      - mountPath: /home/klipper-helm/.helm
        name: klipper-helm
      - mountPath: /home/klipper-helm/.cache
        name: klipper-cache
      - mountPath: /home/klipper-helm/.config
        name: klipper-config
      - mountPath: /tmp
        name: tmp
      - mountPath: /config
        name: values
      - mountPath: /chart
        name: content
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-m5h92
        readOnly: true
        recursiveReadOnly: Disabled
    hostIP: 172.31.17.126
    hostIPs:
    - ip: 172.31.17.126
    phase: Succeeded
    qosClass: BestEffort
    startTime: "2025-02-05T04:54:37Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2025-02-05T04:54:34Z"
    generateName: local-path-provisioner-5cf85fd84d-
    labels:
      app: local-path-provisioner
      pod-template-hash: 5cf85fd84d
    name: local-path-provisioner-5cf85fd84d-266t9
    namespace: kube-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: local-path-provisioner-5cf85fd84d
      uid: 1709685b-a763-4213-b0a9-9a4153d2969d
    resourceVersion: "196023"
    uid: 93d3985a-34e9-4008-8191-494568a8ae04
  spec:
    containers:
    - command:
      - local-path-provisioner
      - start
      - --config
      - /etc/config/config.json
      env:
      - name: POD_NAMESPACE
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.namespace
      image: rancher/local-path-provisioner:v0.0.30
      imagePullPolicy: IfNotPresent
      name: local-path-provisioner
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/config/
        name: config-volume
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-hbzbt
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: ip-172-31-17-126
    preemptionPolicy: PreemptLowerPriority
    priority: 2000001000
    priorityClassName: system-node-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: local-path-provisioner-service-account
    serviceAccountName: local-path-provisioner-service-account
    terminationGracePeriodSeconds: 30
    tolerations:
    - key: CriticalAddonsOnly
      operator: Exists
    - effect: NoSchedule
      key: node-role.kubernetes.io/control-plane
      operator: Exists
    - effect: NoSchedule
      key: node-role.kubernetes.io/master
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - configMap:
        defaultMode: 420
        name: local-path-config
      name: config-volume
    - name: kube-api-access-hbzbt
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-03-03T02:55:20Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-02-05T04:54:37Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2025-03-03T02:55:20Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2025-03-03T02:55:20Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-02-05T04:54:37Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://3743c7fe1783aa82855aada5c31d44bbf18c2504ac4041704ef644f59c39b702
      image: docker.io/rancher/local-path-provisioner:v0.0.30
      imageID: docker.io/rancher/local-path-provisioner@sha256:9b914881170048f80ae9302f36e5b99b4a6b18af73a38adc1c66d12f65d360be
      lastState:
        terminated:
          containerID: containerd://ecb7c25f1dc75c6b0c0de852fdfa2a211044030e7000cfa284368cc9eaea182b
          exitCode: 255
          finishedAt: "2025-03-03T02:54:53Z"
          reason: Unknown
          startedAt: "2025-03-02T07:30:32Z"
      name: local-path-provisioner
      ready: true
      restartCount: 34
      started: true
      state:
        running:
          startedAt: "2025-03-03T02:55:12Z"
      volumeMounts:
      - mountPath: /etc/config/
        name: config-volume
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-hbzbt
        readOnly: true
        recursiveReadOnly: Disabled
    hostIP: 172.31.17.126
    hostIPs:
    - ip: 172.31.17.126
    phase: Running
    podIP: 10.42.0.5
    podIPs:
    - ip: 10.42.0.5
    qosClass: BestEffort
    startTime: "2025-02-05T04:54:37Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2025-02-05T04:54:34Z"
    generateName: metrics-server-5985cbc9d7-
    labels:
      k8s-app: metrics-server
      pod-template-hash: 5985cbc9d7
    name: metrics-server-5985cbc9d7-wd9d7
    namespace: kube-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: metrics-server-5985cbc9d7
      uid: e21cfe77-b639-4272-9a53-a05be6a85625
    resourceVersion: "196243"
    uid: 5abed1b2-7b32-4d37-86c6-4a23cc11e681
  spec:
    containers:
    - args:
      - --cert-dir=/tmp
      - --secure-port=10250
      - --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname
      - --kubelet-use-node-status-port
      - --metric-resolution=15s
      - --tls-cipher-suites=TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305
      image: rancher/mirrored-metrics-server:v0.7.2
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 3
        httpGet:
          path: /livez
          port: https
          scheme: HTTPS
        initialDelaySeconds: 60
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      name: metrics-server
      ports:
      - containerPort: 10250
        name: https
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /readyz
          port: https
          scheme: HTTPS
        periodSeconds: 2
        successThreshold: 1
        timeoutSeconds: 1
      resources:
        requests:
          cpu: 100m
          memory: 70Mi
      securityContext:
        allowPrivilegeEscalation: false
        readOnlyRootFilesystem: true
        runAsNonRoot: true
        runAsUser: 1000
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /tmp
        name: tmp-dir
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-m24tr
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: ip-172-31-17-126
    preemptionPolicy: PreemptLowerPriority
    priority: 2000001000
    priorityClassName: system-node-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: metrics-server
    serviceAccountName: metrics-server
    terminationGracePeriodSeconds: 30
    tolerations:
    - key: CriticalAddonsOnly
      operator: Exists
    - effect: NoSchedule
      key: node-role.kubernetes.io/control-plane
      operator: Exists
    - effect: NoSchedule
      key: node-role.kubernetes.io/master
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - emptyDir: {}
      name: tmp-dir
    - name: kube-api-access-m24tr
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-03-03T02:55:16Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-02-05T04:54:37Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2025-03-03T02:55:26Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2025-03-03T02:55:26Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-02-05T04:54:37Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://781fe78eda3b2fe203b1ca9ad3544859545f2629d17e9d5e30a1aaa5dce6e435
      image: docker.io/rancher/mirrored-metrics-server:v0.7.2
      imageID: docker.io/rancher/mirrored-metrics-server@sha256:dccf8474fb910fef261d31d9483d7e4c1df7b86cf4d638fb6a7d7c88bd51600a
      lastState:
        terminated:
          containerID: containerd://e902bd09e538d81c7f0a123718798dcf9825fbe0c860c7be4c054bb25192679a
          exitCode: 255
          finishedAt: "2025-03-03T02:54:53Z"
          reason: Unknown
          startedAt: "2025-03-02T07:30:34Z"
      name: metrics-server
      ready: true
      restartCount: 34
      started: true
      state:
        running:
          startedAt: "2025-03-03T02:55:16Z"
      volumeMounts:
      - mountPath: /tmp
        name: tmp-dir
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-m24tr
        readOnly: true
        recursiveReadOnly: Disabled
    hostIP: 172.31.17.126
    hostIPs:
    - ip: 172.31.17.126
    phase: Running
    podIP: 10.42.0.13
    podIPs:
    - ip: 10.42.0.13
    qosClass: Burstable
    startTime: "2025-02-05T04:54:37Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2025-02-05T04:54:52Z"
    generateName: svclb-traefik-e88bf217-
    labels:
      app: svclb-traefik-e88bf217
      controller-revision-hash: dbf6767f4
      pod-template-generation: "1"
      svccontroller.k3s.cattle.io/svcname: traefik
      svccontroller.k3s.cattle.io/svcnamespace: kube-system
    name: svclb-traefik-e88bf217-2qkpd
    namespace: kube-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: svclb-traefik-e88bf217
      uid: 4f9059e8-6026-472c-a713-e0a5dc5dc244
    resourceVersion: "196053"
    uid: ddbb7710-c7fe-47d8-b85a-8ca3e188a47b
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - ip-172-31-17-126
    automountServiceAccountToken: false
    containers:
    - env:
      - name: SRC_PORT
        value: "80"
      - name: SRC_RANGES
        value: 0.0.0.0/0
      - name: DEST_PROTO
        value: TCP
      - name: DEST_PORT
        value: "80"
      - name: DEST_IPS
        value: 10.43.213.87
      image: rancher/klipper-lb:v0.4.9
      imagePullPolicy: IfNotPresent
      name: lb-tcp-80
      ports:
      - containerPort: 80
        hostPort: 80
        name: lb-tcp-80
        protocol: TCP
      resources: {}
      securityContext:
        capabilities:
          add:
          - NET_ADMIN
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
    - env:
      - name: SRC_PORT
        value: "443"
      - name: SRC_RANGES
        value: 0.0.0.0/0
      - name: DEST_PROTO
        value: TCP
      - name: DEST_PORT
        value: "443"
      - name: DEST_IPS
        value: 10.43.213.87
      image: rancher/klipper-lb:v0.4.9
      imagePullPolicy: IfNotPresent
      name: lb-tcp-443
      ports:
      - containerPort: 443
        hostPort: 443
        name: lb-tcp-443
        protocol: TCP
      resources: {}
      securityContext:
        capabilities:
          add:
          - NET_ADMIN
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: ip-172-31-17-126
    preemptionPolicy: PreemptLowerPriority
    priority: 2000001000
    priorityClassName: system-node-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      sysctls:
      - name: net.ipv4.ip_forward
        value: "1"
    serviceAccount: svclb
    serviceAccountName: svclb
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoSchedule
      key: node-role.kubernetes.io/master
      operator: Exists
    - effect: NoSchedule
      key: node-role.kubernetes.io/control-plane
      operator: Exists
    - key: CriticalAddonsOnly
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-03-03T02:55:17Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-02-05T04:54:52Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2025-03-03T02:55:17Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2025-03-03T02:55:17Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-02-05T04:54:52Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://9bf9c54071be26efa2d0ae7ac03e1b5ecb36b4ada3b92015921d558f8163513e
      image: docker.io/rancher/klipper-lb:v0.4.9
      imageID: docker.io/rancher/klipper-lb@sha256:dd380f5d89a52f2a07853ff17a6048f805c1f8113b50578f3efc3efb9bcf670a
      lastState:
        terminated:
          containerID: containerd://f0271782bd70339b45a78cc22f7e32ebd548b24cc7a31d1c4435c446aa9428ac
          exitCode: 255
          finishedAt: "2025-03-03T02:54:53Z"
          reason: Unknown
          startedAt: "2025-03-02T07:30:26Z"
      name: lb-tcp-443
      ready: true
      restartCount: 34
      started: true
      state:
        running:
          startedAt: "2025-03-03T02:55:16Z"
    - containerID: containerd://5ed50a8d4066a2f2a2bd0095124d1042e8b77cb24f3ba9fc55b0e339a3c8a3e9
      image: docker.io/rancher/klipper-lb:v0.4.9
      imageID: docker.io/rancher/klipper-lb@sha256:dd380f5d89a52f2a07853ff17a6048f805c1f8113b50578f3efc3efb9bcf670a
      lastState:
        terminated:
          containerID: containerd://e870347ebbd99bf95ceaa16284d6874c3bbc6c1aa78a2ca409d08e962cd3b311
          exitCode: 255
          finishedAt: "2025-03-03T02:54:53Z"
          reason: Unknown
          startedAt: "2025-03-02T07:30:25Z"
      name: lb-tcp-80
      ready: true
      restartCount: 34
      started: true
      state:
        running:
          startedAt: "2025-03-03T02:55:15Z"
    hostIP: 172.31.17.126
    hostIPs:
    - ip: 172.31.17.126
    phase: Running
    podIP: 10.42.0.10
    podIPs:
    - ip: 10.42.0.10
    qosClass: BestEffort
    startTime: "2025-02-05T04:54:52Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      prometheus.io/path: /metrics
      prometheus.io/port: "9100"
      prometheus.io/scrape: "true"
    creationTimestamp: "2025-02-05T04:54:52Z"
    generateName: traefik-5d45fc8cc9-
    labels:
      app.kubernetes.io/instance: traefik-kube-system
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: traefik
      helm.sh/chart: traefik-27.0.201_up27.0.2
      pod-template-hash: 5d45fc8cc9
    name: traefik-5d45fc8cc9-h5nd2
    namespace: kube-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: traefik-5d45fc8cc9
      uid: 360ee329-814b-46b4-9a96-61839bfd3820
    resourceVersion: "196183"
    uid: 75be341c-d291-42f5-8bf5-8278bbf15495
  spec:
    containers:
    - args:
      - --global.checknewversion
      - --global.sendanonymoususage
      - --entrypoints.metrics.address=:9100/tcp
      - --entrypoints.traefik.address=:9000/tcp
      - --entrypoints.web.address=:8000/tcp
      - --entrypoints.websecure.address=:8443/tcp
      - --api.dashboard=true
      - --ping=true
      - --metrics.prometheus=true
      - --metrics.prometheus.entrypoint=metrics
      - --providers.kubernetescrd
      - --providers.kubernetesingress
      - --providers.kubernetesingress.ingressendpoint.publishedservice=kube-system/traefik
      - --entrypoints.websecure.http.tls=true
      env:
      - name: POD_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.name
      - name: POD_NAMESPACE
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.namespace
      image: rancher/mirrored-library-traefik:2.11.18
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 3
        httpGet:
          path: /ping
          port: 9000
          scheme: HTTP
        initialDelaySeconds: 2
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 2
      name: traefik
      ports:
      - containerPort: 9100
        name: metrics
        protocol: TCP
      - containerPort: 9000
        name: traefik
        protocol: TCP
      - containerPort: 8000
        name: web
        protocol: TCP
      - containerPort: 8443
        name: websecure
        protocol: TCP
      readinessProbe:
        failureThreshold: 1
        httpGet:
          path: /ping
          port: 9000
          scheme: HTTP
        initialDelaySeconds: 2
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 2
      resources: {}
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
        readOnlyRootFilesystem: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /data
        name: data
      - mountPath: /tmp
        name: tmp
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-6dc45
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: ip-172-31-17-126
    preemptionPolicy: PreemptLowerPriority
    priority: 2000000000
    priorityClassName: system-cluster-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      fsGroupChangePolicy: OnRootMismatch
      runAsGroup: 65532
      runAsNonRoot: true
      runAsUser: 65532
    serviceAccount: traefik
    serviceAccountName: traefik
    terminationGracePeriodSeconds: 60
    tolerations:
    - key: CriticalAddonsOnly
      operator: Exists
    - effect: NoSchedule
      key: node-role.kubernetes.io/control-plane
      operator: Exists
    - effect: NoSchedule
      key: node-role.kubernetes.io/master
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - emptyDir: {}
      name: data
    - emptyDir: {}
      name: tmp
    - name: kube-api-access-6dc45
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-03-03T02:55:23Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-02-05T04:54:52Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2025-03-03T02:55:23Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2025-03-03T02:55:23Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-02-05T04:54:52Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://d8f752e1bdc3f8a493059ddefaacc03254efd3fb97ad751e232f28b81d12d9d0
      image: docker.io/rancher/mirrored-library-traefik:2.11.18
      imageID: docker.io/rancher/mirrored-library-traefik@sha256:25df7bff0b414867f16a74c571c0dc84920404e45cc7780976cec77809230e09
      lastState:
        terminated:
          containerID: containerd://81b48499cbf6cd586aece66e537fb7ade83eadc5c51413260500b0b42b9a7e78
          exitCode: 255
          finishedAt: "2025-03-03T02:54:53Z"
          reason: Unknown
          startedAt: "2025-03-02T07:30:25Z"
      name: traefik
      ready: true
      restartCount: 34
      started: true
      state:
        running:
          startedAt: "2025-03-03T02:55:12Z"
      volumeMounts:
      - mountPath: /data
        name: data
      - mountPath: /tmp
        name: tmp
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-6dc45
        readOnly: true
        recursiveReadOnly: Disabled
    hostIP: 172.31.17.126
    hostIPs:
    - ip: 172.31.17.126
    phase: Running
    podIP: 10.42.0.7
    podIPs:
    - ip: 10.42.0.7
    qosClass: BestEffort
    startTime: "2025-02-05T04:54:52Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2025-03-02T08:12:05Z"
    generateName: flask-lstm-6f8bc77f4f-
    labels:
      app: flask-lstm
      pod-template-hash: 6f8bc77f4f
    name: flask-lstm-6f8bc77f4f-d4ncz
    namespace: monitoring
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: flask-lstm-6f8bc77f4f
      uid: 06759e8e-e00a-409e-b5a7-3ddd8ffe2ae5
    resourceVersion: "196047"
    uid: 692a90f3-cd18-45d8-9636-950ccf7f4490
  spec:
    containers:
    - image: gnh374/lstm-model
      imagePullPolicy: Always
      name: flask-lstm
      ports:
      - containerPort: 5000
        protocol: TCP
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-k4rtg
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: ip-172-31-17-126
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: default
    serviceAccountName: default
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: kube-api-access-k4rtg
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-03-03T02:55:16Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-03-02T08:12:05Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2025-03-03T02:55:16Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2025-03-03T02:55:16Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-03-02T08:12:05Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://41d5605ee455bf7404c07fc2f7cc0cf09bc34e27b0fb746b8e4854fc27e8e927
      image: docker.io/gnh374/lstm-model:latest
      imageID: docker.io/gnh374/lstm-model@sha256:0567574c0330394c0d7e4fac6a9f4532ca3218e6c7bf3d5a2631b82cf24f97c1
      lastState:
        terminated:
          containerID: containerd://3d4168a572493a7991ad5aff12134157ec2f004c2765199cceb38d765d40f745
          exitCode: 255
          finishedAt: "2025-03-03T02:54:53Z"
          reason: Unknown
          startedAt: "2025-03-02T08:12:06Z"
      name: flask-lstm
      ready: true
      restartCount: 1
      started: true
      state:
        running:
          startedAt: "2025-03-03T02:55:05Z"
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-k4rtg
        readOnly: true
        recursiveReadOnly: Disabled
    hostIP: 172.31.17.126
    hostIPs:
    - ip: 172.31.17.126
    phase: Running
    podIP: 10.42.0.249
    podIPs:
    - ip: 10.42.0.249
    qosClass: BestEffort
    startTime: "2025-03-02T08:12:05Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      checksum/config: c2171b6682aaa4b8718c39a16c2736cef1b363bd9b893f8f58254180d449f03f
    creationTimestamp: "2025-03-02T09:00:44Z"
    generateName: prometheus-alertmanager-
    labels:
      app.kubernetes.io/instance: prometheus
      app.kubernetes.io/name: alertmanager
      apps.kubernetes.io/pod-index: "0"
      controller-revision-hash: prometheus-alertmanager-657656587f
      statefulset.kubernetes.io/pod-name: prometheus-alertmanager-0
    name: prometheus-alertmanager-0
    namespace: monitoring
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: StatefulSet
      name: prometheus-alertmanager
      uid: 0258509c-71b2-4644-967f-2f8dde4e4393
    resourceVersion: "196050"
    uid: b51d3acc-97ad-40ab-85db-70dc9421c1a5
  spec:
    automountServiceAccountToken: true
    containers:
    - args:
      - --storage.path=/alertmanager
      - --config.file=/etc/alertmanager/alertmanager.yml
      env:
      - name: POD_IP
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: status.podIP
      image: quay.io/prometheus/alertmanager:v0.28.0
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 3
        httpGet:
          path: /
          port: http
          scheme: HTTP
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      name: alertmanager
      ports:
      - containerPort: 9093
        name: http
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /
          port: http
          scheme: HTTP
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      resources: {}
      securityContext:
        runAsGroup: 65534
        runAsNonRoot: true
        runAsUser: 65534
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/alertmanager
        name: config
      - mountPath: /alertmanager
        name: storage
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-lh8gr
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostname: prometheus-alertmanager-0
    nodeName: ip-172-31-17-126
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      fsGroup: 65534
      runAsGroup: 65534
      runAsNonRoot: true
      runAsUser: 65534
    serviceAccount: prometheus-alertmanager
    serviceAccountName: prometheus-alertmanager
    subdomain: prometheus-alertmanager-headless
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: storage
      persistentVolumeClaim:
        claimName: storage-prometheus-alertmanager-0
    - configMap:
        defaultMode: 420
        name: prometheus-alertmanager
      name: config
    - name: kube-api-access-lh8gr
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-03-03T02:55:21Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-03-02T09:00:44Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2025-03-03T02:55:21Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2025-03-03T02:55:21Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-03-02T09:00:44Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://b53db3ee7ad3d1123bc06cb82c0d6098d2db843d8f1e959ba02e93b6d2526183
      image: quay.io/prometheus/alertmanager:v0.28.0
      imageID: quay.io/prometheus/alertmanager@sha256:d5155cfac40a6d9250ffc97c19db2c5e190c7bc57c6b67125c94903358f8c7d8
      lastState:
        terminated:
          containerID: containerd://70dd1b5a03b396a8144b1588b66f0c3ced284f39bfdb72780a06b786f7f37ddb
          exitCode: 255
          finishedAt: "2025-03-03T02:54:53Z"
          reason: Unknown
          startedAt: "2025-03-02T09:00:46Z"
      name: alertmanager
      ready: true
      restartCount: 1
      started: true
      state:
        running:
          startedAt: "2025-03-03T02:55:12Z"
      volumeMounts:
      - mountPath: /etc/alertmanager
        name: config
      - mountPath: /alertmanager
        name: storage
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-lh8gr
        readOnly: true
        recursiveReadOnly: Disabled
    hostIP: 172.31.17.126
    hostIPs:
    - ip: 172.31.17.126
    phase: Running
    podIP: 10.42.0.4
    podIPs:
    - ip: 10.42.0.4
    qosClass: BestEffort
    startTime: "2025-03-02T09:00:44Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      kubectl.kubernetes.io/restartedAt: "2025-02-13T12:06:57Z"
    creationTimestamp: "2025-02-26T10:32:51Z"
    labels:
      app.kubernetes.io/component: metrics
      app.kubernetes.io/instance: prometheus
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: kube-state-metrics
      app.kubernetes.io/part-of: kube-state-metrics
      app.kubernetes.io/version: 2.14.0
      helm.sh/chart: kube-state-metrics-5.28.1
      pod-template-hash: 75b5b65f7f
      velero.io/backup-name: monitoring-backup
      velero.io/restore-name: monitoring-backup-20250226103249
    name: prometheus-kube-state-metrics-75b5b65f7f-hshmj
    namespace: monitoring
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: prometheus-kube-state-metrics-75b5b65f7f
      uid: 4ff56e3b-ab3e-4869-a50d-dfc7c573b099
    resourceVersion: "196209"
    uid: ce95b77c-3d3e-48af-ae76-94f6fd89a3b6
  spec:
    automountServiceAccountToken: true
    containers:
    - args:
      - --port=8080
      - --resources=certificatesigningrequests,configmaps,cronjobs,daemonsets,deployments,endpoints,horizontalpodautoscalers,ingresses,jobs,leases,limitranges,mutatingwebhookconfigurations,namespaces,networkpolicies,nodes,persistentvolumeclaims,persistentvolumes,poddisruptionbudgets,pods,replicasets,replicationcontrollers,resourcequotas,secrets,services,statefulsets,storageclasses,validatingwebhookconfigurations,volumeattachments
      image: registry.k8s.io/kube-state-metrics/kube-state-metrics:v2.14.0
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 3
        httpGet:
          path: /livez
          port: 8080
          scheme: HTTP
        initialDelaySeconds: 5
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 5
      name: kube-state-metrics
      ports:
      - containerPort: 8080
        name: http
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /readyz
          port: 8081
          scheme: HTTP
        initialDelaySeconds: 5
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 5
      resources: {}
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
        readOnlyRootFilesystem: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-kjhrq
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: ip-172-31-17-126
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      fsGroup: 65534
      runAsGroup: 65534
      runAsNonRoot: true
      runAsUser: 65534
      seccompProfile:
        type: RuntimeDefault
    serviceAccount: prometheus-kube-state-metrics
    serviceAccountName: prometheus-kube-state-metrics
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: kube-api-access-kjhrq
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-03-03T02:55:22Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-02-26T10:32:51Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2025-03-03T02:55:22Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2025-03-03T02:55:22Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-02-26T10:32:51Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://fab093d9c08afb4f452cf29521890250a16ae2ebc8dc3c9fcd340096df284748
      image: registry.k8s.io/kube-state-metrics/kube-state-metrics:v2.14.0
      imageID: registry.k8s.io/kube-state-metrics/kube-state-metrics@sha256:37d841299325c23b56e5951176ce8ef317d537447c0f1b2d2437dddbb1f51165
      lastState:
        terminated:
          containerID: containerd://a76d5ffd4d3e1acb3c6fa52c07287013df1167d10df3dbe0c1de25919bb7926a
          exitCode: 255
          finishedAt: "2025-03-03T02:54:53Z"
          reason: Unknown
          startedAt: "2025-03-02T07:30:29Z"
      name: kube-state-metrics
      ready: true
      restartCount: 7
      started: true
      state:
        running:
          startedAt: "2025-03-03T02:55:06Z"
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-kjhrq
        readOnly: true
        recursiveReadOnly: Disabled
    hostIP: 172.31.17.126
    hostIPs:
    - ip: 172.31.17.126
    phase: Running
    podIP: 10.42.0.252
    podIPs:
    - ip: 10.42.0.252
    qosClass: BestEffort
    startTime: "2025-02-26T10:32:51Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      cluster-autoscaler.kubernetes.io/safe-to-evict: "true"
    creationTimestamp: "2025-02-26T10:33:04Z"
    generateName: prometheus-prometheus-node-exporter-
    labels:
      app.kubernetes.io/component: metrics
      app.kubernetes.io/instance: prometheus
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: prometheus-node-exporter
      app.kubernetes.io/part-of: prometheus-node-exporter
      app.kubernetes.io/version: 1.8.2
      controller-revision-hash: f66b975f4
      helm.sh/chart: prometheus-node-exporter-4.43.1
      pod-template-generation: "1"
    name: prometheus-prometheus-node-exporter-z5q4v
    namespace: monitoring
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: prometheus-prometheus-node-exporter
      uid: 9a003536-1a78-492e-b653-fa6e7003213c
    resourceVersion: "196030"
    uid: 62dd79b1-90f7-4ab5-8f50-12256b69b2dd
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - ip-172-31-17-126
    automountServiceAccountToken: false
    containers:
    - args:
      - --path.procfs=/host/proc
      - --path.sysfs=/host/sys
      - --path.rootfs=/host/root
      - --path.udev.data=/host/root/run/udev/data
      - --web.listen-address=[$(HOST_IP)]:9100
      env:
      - name: HOST_IP
        value: 0.0.0.0
      image: quay.io/prometheus/node-exporter:v1.8.2
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 3
        httpGet:
          path: /
          port: 9100
          scheme: HTTP
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      name: node-exporter
      ports:
      - containerPort: 9100
        hostPort: 9100
        name: metrics
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /
          port: 9100
          scheme: HTTP
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      resources: {}
      securityContext:
        allowPrivilegeEscalation: false
        readOnlyRootFilesystem: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /host/proc
        name: proc
        readOnly: true
      - mountPath: /host/sys
        name: sys
        readOnly: true
      - mountPath: /host/root
        mountPropagation: HostToContainer
        name: root
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostNetwork: true
    hostPID: true
    nodeName: ip-172-31-17-126
    nodeSelector:
      kubernetes.io/os: linux
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      fsGroup: 65534
      runAsGroup: 65534
      runAsNonRoot: true
      runAsUser: 65534
    serviceAccount: prometheus-prometheus-node-exporter
    serviceAccountName: prometheus-prometheus-node-exporter
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoSchedule
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
    volumes:
    - hostPath:
        path: /proc
        type: ""
      name: proc
    - hostPath:
        path: /sys
        type: ""
      name: sys
    - hostPath:
        path: /
        type: ""
      name: root
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-03-03T02:55:19Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-02-26T10:33:05Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2025-03-03T02:55:19Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2025-03-03T02:55:19Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-02-26T10:33:04Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://5d6167203bd4e4ff9cbc3f828a5d115ce4b0e41c123595bde32cc5eb188c87ef
      image: quay.io/prometheus/node-exporter:v1.8.2
      imageID: quay.io/prometheus/node-exporter@sha256:4032c6d5bfd752342c3e631c2f1de93ba6b86c41db6b167b9a35372c139e7706
      lastState:
        terminated:
          containerID: containerd://3a027ae18a35a31575327616fca13f9c93ba94610af57d23cefb3d53b6a468b0
          exitCode: 255
          finishedAt: "2025-03-03T02:54:53Z"
          reason: Unknown
          startedAt: "2025-03-02T07:30:29Z"
      name: node-exporter
      ready: true
      restartCount: 7
      started: true
      state:
        running:
          startedAt: "2025-03-03T02:55:02Z"
      volumeMounts:
      - mountPath: /host/proc
        name: proc
        readOnly: true
        recursiveReadOnly: Disabled
      - mountPath: /host/sys
        name: sys
        readOnly: true
        recursiveReadOnly: Disabled
      - mountPath: /host/root
        name: root
        readOnly: true
        recursiveReadOnly: Disabled
    hostIP: 172.31.17.126
    hostIPs:
    - ip: 172.31.17.126
    phase: Running
    podIP: 172.31.17.126
    podIPs:
    - ip: 172.31.17.126
    qosClass: BestEffort
    startTime: "2025-02-26T10:33:05Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      kubectl.kubernetes.io/restartedAt: "2025-02-13T12:06:57Z"
    creationTimestamp: "2025-02-26T10:32:52Z"
    labels:
      app.kubernetes.io/instance: prometheus
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: prometheus-pushgateway
      app.kubernetes.io/version: v1.11.0
      helm.sh/chart: prometheus-pushgateway-2.17.0
      pod-template-hash: d5964d9df
      velero.io/backup-name: monitoring-backup
      velero.io/restore-name: monitoring-backup-20250226103249
    name: prometheus-prometheus-pushgateway-d5964d9df-9w5wv
    namespace: monitoring
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: prometheus-prometheus-pushgateway-d5964d9df
      uid: 48d32cf6-583c-45d5-a29b-695c16f1bedf
    resourceVersion: "196230"
    uid: 6d3db464-4ad6-4816-af42-b9b6ebe86fd2
  spec:
    automountServiceAccountToken: true
    containers:
    - image: quay.io/prometheus/pushgateway:v1.11.0
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 3
        httpGet:
          path: /-/healthy
          port: 9091
          scheme: HTTP
        initialDelaySeconds: 10
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 10
      name: pushgateway
      ports:
      - containerPort: 9091
        name: metrics
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /-/ready
          port: 9091
          scheme: HTTP
        initialDelaySeconds: 10
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 10
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /data
        name: storage-volume
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-vr4xg
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: ip-172-31-17-126
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      fsGroup: 65534
      runAsNonRoot: true
      runAsUser: 65534
    serviceAccount: prometheus-prometheus-pushgateway
    serviceAccountName: prometheus-prometheus-pushgateway
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - emptyDir: {}
      name: storage-volume
    - name: kube-api-access-vr4xg
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-03-03T02:55:23Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-02-26T10:32:52Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2025-03-03T02:55:23Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2025-03-03T02:55:23Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-02-26T10:32:52Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://2d7805def485a334ef062fb80cd042890521ec83059ebce72b78ff57d18eee8f
      image: quay.io/prometheus/pushgateway:v1.11.0
      imageID: quay.io/prometheus/pushgateway@sha256:99392035ae99754b40e579088710df184b6a730b77670148f44102ba9ee01d2f
      lastState:
        terminated:
          containerID: containerd://3b3f6a3750598be5aa208e65c14d694b3a6359ae282b65af0d899c7be27ec43d
          exitCode: 255
          finishedAt: "2025-03-03T02:54:53Z"
          reason: Unknown
          startedAt: "2025-03-02T07:30:32Z"
      name: pushgateway
      ready: true
      restartCount: 7
      started: true
      state:
        running:
          startedAt: "2025-03-03T02:55:12Z"
      volumeMounts:
      - mountPath: /data
        name: storage-volume
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-vr4xg
        readOnly: true
        recursiveReadOnly: Disabled
    hostIP: 172.31.17.126
    hostIPs:
    - ip: 172.31.17.126
    phase: Running
    podIP: 10.42.0.8
    podIPs:
    - ip: 10.42.0.8
    qosClass: BestEffort
    startTime: "2025-02-26T10:32:52Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      kubectl.kubernetes.io/restartedAt: "2025-02-13T12:06:57Z"
    creationTimestamp: "2025-02-26T10:32:52Z"
    labels:
      app.kubernetes.io/component: server
      app.kubernetes.io/instance: prometheus
      app.kubernetes.io/name: prometheus
      app.kubernetes.io/part-of: prometheus
      app.kubernetes.io/version: v3.1.0
      helm.sh/chart: prometheus-27.3.0
      pod-template-hash: d7d4585c5
      velero.io/backup-name: monitoring-backup
      velero.io/restore-name: monitoring-backup-20250226103249
    name: prometheus-server-d7d4585c5-57dxn
    namespace: monitoring
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: prometheus-server-d7d4585c5
      uid: 75cbd753-bfa2-44ae-9e3e-121839b72e0a
    resourceVersion: "196324"
    uid: ab288887-99d5-4e03-b136-e854bb62801e
  spec:
    containers:
    - args:
      - --watched-dir=/etc/config
      - --listen-address=0.0.0.0:8080
      - --reload-url=http://127.0.0.1:9090/-/reload
      image: quay.io/prometheus-operator/prometheus-config-reloader:v0.79.2
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 3
        httpGet:
          path: /healthz
          port: metrics
          scheme: HTTP
        initialDelaySeconds: 2
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      name: prometheus-server-configmap-reload
      ports:
      - containerPort: 8080
        name: metrics
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /healthz
          port: metrics
          scheme: HTTP
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/config
        name: config-volume
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-qpwhj
        readOnly: true
    - args:
      - --storage.tsdb.retention.time=15d
      - --config.file=/etc/config/prometheus.yml
      - --storage.tsdb.path=/data
      - --web.console.libraries=/etc/prometheus/console_libraries
      - --web.console.templates=/etc/prometheus/consoles
      - --web.enable-lifecycle
      image: quay.io/prometheus/prometheus:v3.1.0
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 3
        httpGet:
          path: /-/healthy
          port: 9090
          scheme: HTTP
        initialDelaySeconds: 30
        periodSeconds: 15
        successThreshold: 1
        timeoutSeconds: 10
      name: prometheus-server
      ports:
      - containerPort: 9090
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /-/ready
          port: 9090
          scheme: HTTP
        initialDelaySeconds: 30
        periodSeconds: 5
        successThreshold: 1
        timeoutSeconds: 4
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/config
        name: config-volume
      - mountPath: /data
        name: storage-volume
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-qpwhj
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: ip-172-31-17-126
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      fsGroup: 65534
      runAsGroup: 65534
      runAsNonRoot: true
      runAsUser: 65534
    serviceAccount: prometheus-server
    serviceAccountName: prometheus-server
    terminationGracePeriodSeconds: 300
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - configMap:
        defaultMode: 420
        name: prometheus-server
      name: config-volume
    - name: storage-volume
      persistentVolumeClaim:
        claimName: prometheus-server
    - name: kube-api-access-qpwhj
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-03-03T02:55:22Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-02-26T10:32:59Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2025-03-03T02:55:37Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2025-03-03T02:55:37Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-02-26T10:32:59Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://f6f87e813e989fea112b2911f57a4775837682ff705c6d9bda4102ea6378e4de
      image: quay.io/prometheus/prometheus:v3.1.0
      imageID: quay.io/prometheus/prometheus@sha256:6559acbd5d770b15bb3c954629ce190ac3cbbdb2b7f1c30f0385c4e05104e218
      lastState:
        terminated:
          containerID: containerd://c839b0b4a3e40790200bcb591301b422decdc98f15c0b26ab05535cb66eb3477
          exitCode: 255
          finishedAt: "2025-03-03T02:54:53Z"
          reason: Unknown
          startedAt: "2025-03-02T07:30:32Z"
      name: prometheus-server
      ready: true
      restartCount: 7
      started: true
      state:
        running:
          startedAt: "2025-03-03T02:55:07Z"
      volumeMounts:
      - mountPath: /etc/config
        name: config-volume
      - mountPath: /data
        name: storage-volume
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-qpwhj
        readOnly: true
        recursiveReadOnly: Disabled
    - containerID: containerd://1a8e9c757032ecbbdb781bd8f0ca2e3d9b7393552eea696b73216579039b3946
      image: quay.io/prometheus-operator/prometheus-config-reloader:v0.79.2
      imageID: quay.io/prometheus-operator/prometheus-config-reloader@sha256:193280a33bc1acad9bc1956c5d986e3da6950882bda89311bac317998dcebf30
      lastState:
        terminated:
          containerID: containerd://1c72751db70b02c333baf08ceafef28c0624b81339314691ffde9d479cc6dea7
          exitCode: 255
          finishedAt: "2025-03-03T02:54:53Z"
          reason: Unknown
          startedAt: "2025-03-02T07:30:29Z"
      name: prometheus-server-configmap-reload
      ready: true
      restartCount: 7
      started: true
      state:
        running:
          startedAt: "2025-03-03T02:55:06Z"
      volumeMounts:
      - mountPath: /etc/config
        name: config-volume
        readOnly: true
        recursiveReadOnly: Disabled
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-qpwhj
        readOnly: true
        recursiveReadOnly: Disabled
    hostIP: 172.31.17.126
    hostIPs:
    - ip: 172.31.17.126
    phase: Running
    podIP: 10.42.0.253
    podIPs:
    - ip: 10.42.0.253
    qosClass: BestEffort
    startTime: "2025-02-26T10:32:59Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      kubectl.kubernetes.io/restartedAt: "2025-02-13T12:06:57Z"
    creationTimestamp: "2025-02-26T10:32:52Z"
    labels:
      app: velero-runner
      pod-template-hash: 54974bb85d
      velero.io/backup-name: monitoring-backup
      velero.io/restore-name: monitoring-backup-20250226103249
    name: velero-runner-54974bb85d-p4c58
    namespace: monitoring
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: velero-runner-54974bb85d
      uid: 0c1e3709-3326-4c5b-80b2-f6be3901b525
    resourceVersion: "197343"
    uid: 4f1cc45a-0b4c-441a-9b84-e077c2909eb8
  spec:
    containers:
    - command:
      - python
      - /app/webhook.py
      image: python:3.9
      imagePullPolicy: IfNotPresent
      name: velero-runner
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /app
        name: app
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-rz4ds
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: ip-172-31-17-126
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: default
    serviceAccountName: default
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - configMap:
        defaultMode: 420
        name: webhook-config
      name: app
    - name: kube-api-access-rz4ds
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-03-03T02:55:33Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-02-26T10:32:52Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2025-03-03T03:01:06Z"
      message: 'containers with unready status: [velero-runner]'
      reason: ContainersNotReady
      status: "False"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2025-03-03T03:01:06Z"
      message: 'containers with unready status: [velero-runner]'
      reason: ContainersNotReady
      status: "False"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-02-26T10:32:52Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://b379218ccbf06a6b70058ba8fda307ed29727f4d36b735973ab8e58fd27e681b
      image: docker.io/library/python:3.9
      imageID: docker.io/library/python@sha256:3316bd11b41f8b80fdfd9036eb4c6f98c2da2404ef42c4c384a471259ee0e3ba
      lastState:
        terminated:
          containerID: containerd://b379218ccbf06a6b70058ba8fda307ed29727f4d36b735973ab8e58fd27e681b
          exitCode: 1
          finishedAt: "2025-03-03T03:01:05Z"
          reason: Error
          startedAt: "2025-03-03T03:01:05Z"
      name: velero-runner
      ready: false
      restartCount: 141
      started: false
      state:
        waiting:
          message: back-off 5m0s restarting failed container=velero-runner pod=velero-runner-54974bb85d-p4c58_monitoring(4f1cc45a-0b4c-441a-9b84-e077c2909eb8)
          reason: CrashLoopBackOff
      volumeMounts:
      - mountPath: /app
        name: app
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-rz4ds
        readOnly: true
        recursiveReadOnly: Disabled
    hostIP: 172.31.17.126
    hostIPs:
    - ip: 172.31.17.126
    phase: Running
    podIP: 10.42.0.14
    podIPs:
    - ip: 10.42.0.14
    qosClass: BestEffort
    startTime: "2025-02-26T10:32:52Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      kubectl.kubernetes.io/restartedAt: "2025-02-13T12:06:57Z"
    creationTimestamp: "2025-02-26T10:32:53Z"
    labels:
      app: webhook
      pod-template-hash: 64d4fb9bcc
      velero.io/backup-name: monitoring-backup
      velero.io/restore-name: monitoring-backup-20250226103249
    name: webhook-deployment-64d4fb9bcc-s7x2n
    namespace: monitoring
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: webhook-deployment-64d4fb9bcc
      uid: 0ec5bc5d-b9e5-45c7-9b02-18ec3268c1c7
    resourceVersion: "196201"
    uid: 9b492abf-436b-432d-8713-1a617172fac7
  spec:
    containers:
    - image: gnh374/webhook-express:latest
      imagePullPolicy: Always
      name: webhook
      ports:
      - containerPort: 3000
        protocol: TCP
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-wblxd
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: ip-172-31-17-126
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: webhook-sa
    serviceAccountName: webhook-sa
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: kube-api-access-wblxd
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-03-03T02:55:24Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-02-26T10:32:53Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2025-03-03T02:55:24Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2025-03-03T02:55:24Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-02-26T10:32:53Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://8ea5d4c37b5595ae3cc5f71a69c928209ce0e0cb2952f878f340cacd526787ac
      image: docker.io/gnh374/webhook-express:latest
      imageID: docker.io/gnh374/webhook-express@sha256:7f3d4f924c93d35298d7dfb4aba3bd45e54bec772f7e8a802f72803dcdf5dea7
      lastState:
        terminated:
          containerID: containerd://10575df886dd54e845af1f30e2462a4aef90406145549f5f92a79dd8dbf9eef5
          exitCode: 255
          finishedAt: "2025-03-03T02:54:53Z"
          reason: Unknown
          startedAt: "2025-03-02T07:30:33Z"
      name: webhook
      ready: true
      restartCount: 7
      started: true
      state:
        running:
          startedAt: "2025-03-03T02:55:09Z"
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-wblxd
        readOnly: true
        recursiveReadOnly: Disabled
    hostIP: 172.31.17.126
    hostIPs:
    - ip: 172.31.17.126
    phase: Running
    podIP: 10.42.0.254
    podIPs:
    - ip: 10.42.0.254
    qosClass: BestEffort
    startTime: "2025-02-26T10:32:53Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2025-02-06T14:32:37Z"
    generateName: node-agent-
    labels:
      component: velero
      controller-revision-hash: 9dcbb9b8d
      name: node-agent
      pod-template-generation: "1"
    name: node-agent-xmdpt
    namespace: velero
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: node-agent
      uid: e81668b1-2c5e-4aab-9365-640c72e4975d
    resourceVersion: "196256"
    uid: b3580f76-3053-4bb4-bcc2-2404bb7a9fb1
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - ip-172-31-17-126
    containers:
    - args:
      - node-agent
      - server
      - --features=
      command:
      - /velero
      env:
      - name: NODE_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      - name: VELERO_NAMESPACE
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.namespace
      - name: VELERO_SCRATCH_DIR
        value: /scratch
      image: velero/velero:v1.15.2
      imagePullPolicy: IfNotPresent
      name: node-agent
      ports:
      - containerPort: 8085
        name: metrics
        protocol: TCP
      resources: {}
      securityContext:
        privileged: false
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /host_pods
        mountPropagation: HostToContainer
        name: host-pods
      - mountPath: /var/lib/kubelet/plugins
        mountPropagation: HostToContainer
        name: host-plugins
      - mountPath: /scratch
        name: scratch
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-48x46
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: ip-172-31-17-126
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      runAsUser: 0
    serviceAccount: velero
    serviceAccountName: velero
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    volumes:
    - hostPath:
        path: /var/lib/kubelet/pods
        type: ""
      name: host-pods
    - hostPath:
        path: /var/lib/kubelet/plugins
        type: ""
      name: host-plugins
    - emptyDir: {}
      name: scratch
    - name: kube-api-access-48x46
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-03-03T02:55:27Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-02-06T14:32:37Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2025-03-03T02:55:27Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2025-03-03T02:55:27Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-02-06T14:32:37Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://b6c851da13f644de4b8c5bbb847a41e9f07af68038e89af5be00ca7d1598b7e9
      image: docker.io/velero/velero:v1.15.2
      imageID: docker.io/velero/velero@sha256:668467fdf39f3a610ed6f27431f38a6fbb6143a2ab302ad3e839b0074aaeba39
      lastState:
        terminated:
          containerID: containerd://86dafcd71cea22f1ce51b9810a3b3c266bcc8d31fd7099e7280fc1ea96910d37
          exitCode: 255
          finishedAt: "2025-03-03T02:54:53Z"
          reason: Unknown
          startedAt: "2025-03-02T07:30:28Z"
      name: node-agent
      ready: true
      restartCount: 25
      started: true
      state:
        running:
          startedAt: "2025-03-03T02:55:04Z"
      volumeMounts:
      - mountPath: /host_pods
        name: host-pods
      - mountPath: /var/lib/kubelet/plugins
        name: host-plugins
      - mountPath: /scratch
        name: scratch
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-48x46
        readOnly: true
        recursiveReadOnly: Disabled
    hostIP: 172.31.17.126
    hostIPs:
    - ip: 172.31.17.126
    phase: Running
    podIP: 10.42.0.250
    podIPs:
    - ip: 10.42.0.250
    qosClass: BestEffort
    startTime: "2025-02-06T14:32:37Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      prometheus.io/path: /metrics
      prometheus.io/port: "8085"
      prometheus.io/scrape: "true"
    creationTimestamp: "2025-02-06T14:32:37Z"
    generateName: velero-57fdbd48db-
    labels:
      component: velero
      deploy: velero
      pod-template-hash: 57fdbd48db
    name: velero-57fdbd48db-ldltc
    namespace: velero
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: velero-57fdbd48db
      uid: 1da66ef8-ddcf-49ab-b15e-981515303dd3
    resourceVersion: "196319"
    uid: aa8e726a-7754-40c1-8e9e-95bfaf976303
  spec:
    containers:
    - args:
      - server
      - --features=
      - --uploader-type=kopia
      command:
      - /velero
      env:
      - name: VELERO_SCRATCH_DIR
        value: /scratch
      - name: VELERO_NAMESPACE
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.namespace
      - name: LD_LIBRARY_PATH
        value: /plugins
      image: velero/velero:v1.15.2
      imagePullPolicy: IfNotPresent
      name: velero
      ports:
      - containerPort: 8085
        name: metrics
        protocol: TCP
      resources:
        limits:
          cpu: "1"
          memory: 512Mi
        requests:
          cpu: 500m
          memory: 128Mi
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /plugins
        name: plugins
      - mountPath: /scratch
        name: scratch
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-8bbtt
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    initContainers:
    - image: velero/velero-plugin-for-aws:v1.10.0
      imagePullPolicy: IfNotPresent
      name: velero-velero-plugin-for-aws
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /target
        name: plugins
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-8bbtt
        readOnly: true
    nodeName: ip-172-31-17-126
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: velero
    serviceAccountName: velero
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - emptyDir: {}
      name: plugins
    - emptyDir: {}
      name: scratch
    - name: kube-api-access-8bbtt
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-03-03T02:55:17Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-02-06T14:32:45Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2025-03-03T02:55:37Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2025-03-03T02:55:37Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-02-06T14:32:37Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://669d54290731e78451c40f7ac0bca321a0790eefb274e74b3cb1f8d4c5fa5ad3
      image: docker.io/velero/velero:v1.15.2
      imageID: docker.io/velero/velero@sha256:668467fdf39f3a610ed6f27431f38a6fbb6143a2ab302ad3e839b0074aaeba39
      lastState:
        terminated:
          containerID: containerd://698bcd025053cd66536785737e31c9e8accb2e4f7b72d5ae8e9551461548ae95
          exitCode: 255
          finishedAt: "2025-03-03T02:54:53Z"
          reason: Unknown
          startedAt: "2025-03-02T07:30:38Z"
      name: velero
      ready: true
      restartCount: 25
      started: true
      state:
        running:
          startedAt: "2025-03-03T02:55:30Z"
      volumeMounts:
      - mountPath: /plugins
        name: plugins
      - mountPath: /scratch
        name: scratch
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-8bbtt
        readOnly: true
        recursiveReadOnly: Disabled
    hostIP: 172.31.17.126
    hostIPs:
    - ip: 172.31.17.126
    initContainerStatuses:
    - containerID: containerd://9a95f629862ac40f97199891c63205c2dbee533f23faea055a91ec23fa096624
      image: docker.io/velero/velero-plugin-for-aws:v1.10.0
      imageID: docker.io/velero/velero-plugin-for-aws@sha256:80f84902d9f644aecce043908867a6629cc89b7ddb8c6633ad0e8bbe2c364e7e
      lastState: {}
      name: velero-velero-plugin-for-aws
      ready: true
      restartCount: 5
      started: false
      state:
        terminated:
          containerID: containerd://9a95f629862ac40f97199891c63205c2dbee533f23faea055a91ec23fa096624
          exitCode: 0
          finishedAt: "2025-03-03T02:55:15Z"
          reason: Completed
          startedAt: "2025-03-03T02:55:14Z"
      volumeMounts:
      - mountPath: /target
        name: plugins
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-8bbtt
        readOnly: true
        recursiveReadOnly: Disabled
    phase: Running
    podIP: 10.42.0.11
    podIPs:
    - ip: 10.42.0.11
    qosClass: Burstable
    startTime: "2025-02-06T14:32:37Z"
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      meta.helm.sh/release-name: fleet-agent-local
      meta.helm.sh/release-namespace: cattle-fleet-local-system
      objectset.rio.cattle.io/applied: H4sIAAAAAAAA/3yQPU7EQAyF7+I6Ccvs5reEigZRcAFn4rBBzngUO8VqlbujSSQkKLa0n/29Z99hJsMBDaG7A4YghjZJ0FRK/03elKxYJik8mjEVkzxNA3QwMpHl+EXB8l7E1BaM+TGUHyKLR871pkYzbBn4hXb45zSTGs4RurAyZ8DYEz+0vKJeoYNz5U7uPNalo3p8bpt2aPqqdHRybT84X5UNUu0vl+QWcKa/OeFoakSflIdZNZJPgTyvarS8fUAH7xIIMlBi8ibL/rEY/3lsGdgtJv7r72riGdq6n8iCwwsyBk8JsW3bTwAAAP//NSWMEIUBAAA
      objectset.rio.cattle.io/id: fleet-agent-bootstrap-cattle-fleet-local-system
    creationTimestamp: "2025-02-05T05:34:45Z"
    labels:
      app.kubernetes.io/managed-by: Helm
      objectset.rio.cattle.io/hash: 362023f752e7f1989d8b652e029bd2c658ae7c44
    name: fleet-agent
    namespace: cattle-fleet-local-system
    resourceVersion: "5176"
    uid: 5fa52124-fa7c-4603-9001-bd05bf92aaf0
  spec:
    clusterIP: None
    clusterIPs:
    - None
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    selector:
      app: fleet-agent
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      meta.helm.sh/release-name: fleet
      meta.helm.sh/release-namespace: cattle-fleet-system
    creationTimestamp: "2025-02-05T05:33:54Z"
    labels:
      app.kubernetes.io/managed-by: Helm
    name: gitjob
    namespace: cattle-fleet-system
    resourceVersion: "4500"
    uid: 4adba9c5-70e4-4fa3-8e9b-33dc566b9ecd
  spec:
    clusterIP: 10.43.216.170
    clusterIPs:
    - 10.43.216.170
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: http-80
      port: 80
      protocol: TCP
      targetPort: 8080
    selector:
      app: gitjob
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      meta.helm.sh/release-name: fleet
      meta.helm.sh/release-namespace: cattle-fleet-system
    creationTimestamp: "2025-02-05T05:33:54Z"
    labels:
      app: fleet-controller
      app.kubernetes.io/managed-by: Helm
    name: monitoring-fleet-controller
    namespace: cattle-fleet-system
    resourceVersion: "4498"
    uid: 1461b282-651e-4b4c-92d1-0f47477c13b4
  spec:
    clusterIP: 10.43.180.171
    clusterIPs:
    - 10.43.180.171
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: metrics
      port: 8080
      protocol: TCP
      targetPort: 8080
    selector:
      app: fleet-controller
      fleet.cattle.io/shard-default: "true"
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      meta.helm.sh/release-name: fleet
      meta.helm.sh/release-namespace: cattle-fleet-system
    creationTimestamp: "2025-02-05T05:33:54Z"
    labels:
      app: gitjob
      app.kubernetes.io/managed-by: Helm
    name: monitoring-gitjob
    namespace: cattle-fleet-system
    resourceVersion: "4505"
    uid: c32e0d2c-6008-4679-bd53-6bd76a8ce434
  spec:
    clusterIP: 10.43.188.68
    clusterIPs:
    - 10.43.188.68
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: metrics
      port: 8081
      protocol: TCP
      targetPort: 8081
    selector:
      app: gitjob
      fleet.cattle.io/shard-default: "true"
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      meta.helm.sh/release-name: rancher-provisioning-capi
      meta.helm.sh/release-namespace: cattle-provisioning-capi-system
      need-a-cert.cattle.io/secret-name: capi-webhook-service-cert
    creationTimestamp: "2025-02-05T05:35:00Z"
    labels:
      app.kubernetes.io/managed-by: Helm
      cluster.x-k8s.io/provider: cluster-api
    name: capi-webhook-service
    namespace: cattle-provisioning-capi-system
    resourceVersion: "5209"
    uid: 7b2d6ec9-3ee4-4b39-9a82-65d72cdfcc5e
  spec:
    clusterIP: 10.43.140.188
    clusterIPs:
    - 10.43.140.188
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - port: 443
      protocol: TCP
      targetPort: webhook-server
    selector:
      cluster.x-k8s.io/provider: cluster-api
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      field.cattle.io/publicEndpoints: '[{"port":32493,"protocol":"TCP","serviceName":"cattle-system:rancher","allNodes":true},{"port":30199,"protocol":"TCP","serviceName":"cattle-system:rancher","allNodes":true}]'
      meta.helm.sh/release-name: rancher
      meta.helm.sh/release-namespace: cattle-system
    creationTimestamp: "2025-02-05T05:31:08Z"
    labels:
      app: rancher
      app.kubernetes.io/managed-by: Helm
      chart: rancher-2.10.2
      heritage: Helm
      release: rancher
    name: rancher
    namespace: cattle-system
    resourceVersion: "33048"
    uid: 25862798-352f-4b8f-a679-af6cb66ab6c8
  spec:
    clusterIP: 10.43.154.121
    clusterIPs:
    - 10.43.154.121
    externalTrafficPolicy: Cluster
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: http
      nodePort: 32493
      port: 80
      protocol: TCP
      targetPort: 80
    - name: https-internal
      nodePort: 30199
      port: 443
      protocol: TCP
      targetPort: 444
    selector:
      app: rancher
    sessionAffinity: None
    type: NodePort
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      field.cattle.io/publicEndpoints: '[{"port":30449,"protocol":"TCP","serviceName":"cattle-system:rancher-webhook","allNodes":true}]'
      meta.helm.sh/release-name: rancher-webhook
      meta.helm.sh/release-namespace: cattle-system
    creationTimestamp: "2025-02-05T05:34:32Z"
    labels:
      app.kubernetes.io/managed-by: Helm
    name: rancher-webhook
    namespace: cattle-system
    resourceVersion: "137470"
    uid: f6cdd55b-b090-4d93-b406-2965ead7f21c
  spec:
    clusterIP: 10.43.7.193
    clusterIPs:
    - 10.43.7.193
    externalTrafficPolicy: Cluster
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: https
      nodePort: 30449
      port: 443
      protocol: TCP
      targetPort: 9443
    selector:
      app: rancher-webhook
    sessionAffinity: None
    type: NodePort
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      meta.helm.sh/release-name: cert-manager
      meta.helm.sh/release-namespace: cert-manager
    creationTimestamp: "2025-02-05T04:55:46Z"
    labels:
      app: cert-manager
      app.kubernetes.io/component: controller
      app.kubernetes.io/instance: cert-manager
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: cert-manager
      app.kubernetes.io/version: v1.17.0
      helm.sh/chart: cert-manager-v1.17.0
    name: cert-manager
    namespace: cert-manager
    resourceVersion: "747"
    uid: 932bbaa5-c7a0-44e9-acad-edae3c5a70e5
  spec:
    clusterIP: 10.43.173.152
    clusterIPs:
    - 10.43.173.152
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: tcp-prometheus-servicemonitor
      port: 9402
      protocol: TCP
      targetPort: 9402
    selector:
      app.kubernetes.io/component: controller
      app.kubernetes.io/instance: cert-manager
      app.kubernetes.io/name: cert-manager
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      meta.helm.sh/release-name: cert-manager
      meta.helm.sh/release-namespace: cert-manager
    creationTimestamp: "2025-02-05T04:55:46Z"
    labels:
      app: cainjector
      app.kubernetes.io/component: cainjector
      app.kubernetes.io/instance: cert-manager
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: cainjector
      app.kubernetes.io/version: v1.17.0
      helm.sh/chart: cert-manager-v1.17.0
    name: cert-manager-cainjector
    namespace: cert-manager
    resourceVersion: "750"
    uid: d38ca25d-bfb5-4464-a283-23366800269e
  spec:
    clusterIP: 10.43.148.22
    clusterIPs:
    - 10.43.148.22
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: http-metrics
      port: 9402
      protocol: TCP
      targetPort: 9402
    selector:
      app.kubernetes.io/component: cainjector
      app.kubernetes.io/instance: cert-manager
      app.kubernetes.io/name: cainjector
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      meta.helm.sh/release-name: cert-manager
      meta.helm.sh/release-namespace: cert-manager
    creationTimestamp: "2025-02-05T04:55:46Z"
    labels:
      app: webhook
      app.kubernetes.io/component: webhook
      app.kubernetes.io/instance: cert-manager
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: webhook
      app.kubernetes.io/version: v1.17.0
      helm.sh/chart: cert-manager-v1.17.0
    name: cert-manager-webhook
    namespace: cert-manager
    resourceVersion: "756"
    uid: 77d3f81f-2bb0-48db-bdf0-8e677f1a75c1
  spec:
    clusterIP: 10.43.131.214
    clusterIPs:
    - 10.43.131.214
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: https
      port: 443
      protocol: TCP
      targetPort: https
    - name: metrics
      port: 9402
      protocol: TCP
      targetPort: http-metrics
    selector:
      app.kubernetes.io/component: webhook
      app.kubernetes.io/instance: cert-manager
      app.kubernetes.io/name: webhook
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      field.cattle.io/publicEndpoints: '[{"port":32131,"protocol":"TCP","serviceName":"default:flask-lstm-service","allNodes":true}]'
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"v1","kind":"Service","metadata":{"annotations":{},"name":"flask-lstm-service","namespace":"default"},"spec":{"ports":[{"port":5000,"protocol":"TCP","targetPort":5000}],"selector":{"app":"flask-lstm"},"type":"NodePort"}}
    creationTimestamp: "2025-02-27T12:30:54Z"
    name: flask-lstm-service
    namespace: default
    resourceVersion: "166480"
    uid: 92ebf691-8417-466e-b0ca-24f50ca93eff
  spec:
    clusterIP: 10.43.253.133
    clusterIPs:
    - 10.43.253.133
    externalTrafficPolicy: Cluster
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - nodePort: 32131
      port: 5000
      protocol: TCP
      targetPort: 5000
    selector:
      app: flask-lstm
    sessionAffinity: None
    type: NodePort
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    creationTimestamp: "2025-02-05T04:54:27Z"
    labels:
      component: apiserver
      provider: kubernetes
    name: kubernetes
    namespace: default
    resourceVersion: "195"
    uid: e3770546-98a5-4b02-a527-65d375e10dd9
  spec:
    clusterIP: 10.43.0.1
    clusterIPs:
    - 10.43.0.1
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: https
      port: 443
      protocol: TCP
      targetPort: 6443
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      objectset.rio.cattle.io/applied: H4sIAAAAAAAA/4ySQYvbMBCF/0p5Z9m142TjFfRQdimUQgmk7aXsQZYnG9W2JKRJSgj+70WJl00b0vZm8958vHmjI5Q33yhE4ywk9iUEOmNbSKwp7I0mCAzEqlWsII9Q1jpWbJyN6dc1P0hzJM6DcblWzD3lxr01iQBxU3c/LYXsed9BoqvihbIvxZtPxrbv3rets/9EWDUQJLQL1Nr4X/bolU4z3a6hLB4i0wABH9xAvKVdTG7vAkPivlxUV1rUQfkE4LAjjAK9aqg/1dHVMVPev8DPidJnsMR0mtb9LjKFLE71Tpg/bdNeDy7Q4+f1X/baqriFRKNpVlez+7ouy+W8UkVV36lmURab2eZuSZvlfDYv9GKZ8k7si4i3ahkFoiedVptyf1xBoizyeZUXeVlAvAoR8vul9CRg/Ac1mP6wcr3Rh/SojH3uac1Kd6lXFzhNHV8indOcy19Up+LZaddD4uvjCqO4dGas/S33l4ff3ANxMPqVne567X8SiNSTZhduHHMcx18BAAD//5X9LCMyAwAA
      objectset.rio.cattle.io/id: ""
      objectset.rio.cattle.io/owner-gvk: k3s.cattle.io/v1, Kind=Addon
      objectset.rio.cattle.io/owner-name: coredns
      objectset.rio.cattle.io/owner-namespace: kube-system
      prometheus.io/port: "9153"
      prometheus.io/scrape: "true"
    creationTimestamp: "2025-02-05T04:54:31Z"
    labels:
      k8s-app: kube-dns
      kubernetes.io/cluster-service: "true"
      kubernetes.io/name: CoreDNS
      objectset.rio.cattle.io/hash: bce283298811743a0386ab510f2f67ef74240c57
    name: kube-dns
    namespace: kube-system
    resourceVersion: "272"
    uid: 5eb60266-98e8-4609-bdb4-a7707b56092a
  spec:
    clusterIP: 10.43.0.10
    clusterIPs:
    - 10.43.0.10
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: dns
      port: 53
      protocol: UDP
      targetPort: 53
    - name: dns-tcp
      port: 53
      protocol: TCP
      targetPort: 53
    - name: metrics
      port: 9153
      protocol: TCP
      targetPort: 9153
    selector:
      k8s-app: kube-dns
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      objectset.rio.cattle.io/applied: H4sIAAAAAAAA/4SQQWsbMRCF/0p5Z9nNep04FfRQWnopBUNKL6WHWe04VleWhGa8xZj970UbFxLaJCchvZn3vqczKPvvXMSnCIuxgcHgYw+LOy6jdwyDAyv1pAR7BsWYlNSnKPWaul/sVFiXxaelI9XAS5/e+uoA86yefkcui/txgMXQyiNlbMybLz727z/0fYqvWkQ6MGxFLN7JQriMXObjgf31bcnkqsVw7HghJ1E+YDII1HGYO1ahRFaWuujCUfRRhIWWY016Onbh+vqE6wWePckeFnTdt527uWrc7abhZtXuqF11q83uev2uu2HabK46t1tTJfxvdTy8P1NKMrtayefPdPDhtE3BuxMstoV3XD4dKdwpuQEGORUV2B/nvzl71SwXAXa9bg1ySZpcCrD49nELA6Vyz7qdJy4L008D4cBOU5l/81YWlPO/4NM0/QkAAP//sKxN444CAAA
      objectset.rio.cattle.io/id: ""
      objectset.rio.cattle.io/owner-gvk: k3s.cattle.io/v1, Kind=Addon
      objectset.rio.cattle.io/owner-name: metrics-server-service
      objectset.rio.cattle.io/owner-namespace: kube-system
    creationTimestamp: "2025-02-05T04:54:31Z"
    labels:
      kubernetes.io/cluster-service: "true"
      kubernetes.io/name: Metrics-server
      objectset.rio.cattle.io/hash: a5d3bc601c871e123fa32b27f549b6ea770bcf4a
    name: metrics-server
    namespace: kube-system
    resourceVersion: "316"
    uid: 182471b0-67e4-4ddd-81e4-a8059f19d541
  spec:
    clusterIP: 10.43.146.153
    clusterIPs:
    - 10.43.146.153
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: PreferDualStack
    ports:
    - name: https
      port: 443
      protocol: TCP
      targetPort: https
    selector:
      k8s-app: metrics-server
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      field.cattle.io/publicEndpoints: '[{"addresses":["172.31.17.126"],"port":80,"protocol":"TCP","serviceName":"kube-system:traefik","allNodes":false},{"addresses":["172.31.17.126"],"port":443,"protocol":"TCP","serviceName":"kube-system:traefik","allNodes":false}]'
      meta.helm.sh/release-name: traefik
      meta.helm.sh/release-namespace: kube-system
    creationTimestamp: "2025-02-05T04:54:52Z"
    finalizers:
    - service.kubernetes.io/load-balancer-cleanup
    labels:
      app.kubernetes.io/instance: traefik-kube-system
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: traefik
      helm.sh/chart: traefik-27.0.201_up27.0.2
    name: traefik
    namespace: kube-system
    resourceVersion: "196073"
    uid: e88bf217-4aec-4655-bfd7-5434015d5e3d
  spec:
    allocateLoadBalancerNodePorts: true
    clusterIP: 10.43.213.87
    clusterIPs:
    - 10.43.213.87
    externalTrafficPolicy: Cluster
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: PreferDualStack
    ports:
    - name: web
      nodePort: 32747
      port: 80
      protocol: TCP
      targetPort: web
    - name: websecure
      nodePort: 31307
      port: 443
      protocol: TCP
      targetPort: websecure
    selector:
      app.kubernetes.io/instance: traefik-kube-system
      app.kubernetes.io/name: traefik
    sessionAffinity: None
    type: LoadBalancer
  status:
    loadBalancer:
      ingress:
      - ip: 172.31.17.126
        ipMode: VIP
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      meta.helm.sh/release-name: prometheus
      meta.helm.sh/release-namespace: monitoring
    creationTimestamp: "2025-02-26T10:33:03Z"
    labels:
      app.kubernetes.io/instance: prometheus
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: alertmanager
      app.kubernetes.io/version: v0.28.0
      helm.sh/chart: alertmanager-1.14.0
      velero.io/backup-name: monitoring-backup
      velero.io/restore-name: monitoring-backup-20250226103249
    name: prometheus-alertmanager
    namespace: monitoring
    resourceVersion: "124602"
    uid: 952a9d9d-4905-4146-9ef7-3f3d64f2f551
  spec:
    clusterIP: 10.43.163.207
    clusterIPs:
    - 10.43.163.207
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: http
      port: 9093
      protocol: TCP
      targetPort: http
    selector:
      app.kubernetes.io/instance: prometheus
      app.kubernetes.io/name: alertmanager
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      meta.helm.sh/release-name: prometheus
      meta.helm.sh/release-namespace: monitoring
    creationTimestamp: "2025-02-26T10:33:03Z"
    labels:
      app.kubernetes.io/instance: prometheus
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: alertmanager
      app.kubernetes.io/version: v0.28.0
      helm.sh/chart: alertmanager-1.14.0
      velero.io/backup-name: monitoring-backup
      velero.io/restore-name: monitoring-backup-20250226103249
    name: prometheus-alertmanager-headless
    namespace: monitoring
    resourceVersion: "124599"
    uid: 35ea0941-fb3b-4640-84e0-58fa1479429b
  spec:
    clusterIP: None
    clusterIPs:
    - None
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: http
      port: 9093
      protocol: TCP
      targetPort: http
    selector:
      app.kubernetes.io/instance: prometheus
      app.kubernetes.io/name: alertmanager
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      meta.helm.sh/release-name: prometheus
      meta.helm.sh/release-namespace: monitoring
      prometheus.io/scrape: "true"
    creationTimestamp: "2025-02-26T10:33:03Z"
    labels:
      app.kubernetes.io/component: metrics
      app.kubernetes.io/instance: prometheus
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: kube-state-metrics
      app.kubernetes.io/part-of: kube-state-metrics
      app.kubernetes.io/version: 2.14.0
      helm.sh/chart: kube-state-metrics-5.28.1
      velero.io/backup-name: monitoring-backup
      velero.io/restore-name: monitoring-backup-20250226103249
    name: prometheus-kube-state-metrics
    namespace: monitoring
    resourceVersion: "124609"
    uid: 2f3b776c-0f82-4010-a72a-18d3cd29c44f
  spec:
    clusterIP: 10.43.113.11
    clusterIPs:
    - 10.43.113.11
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: http
      port: 8080
      protocol: TCP
      targetPort: 8080
    selector:
      app.kubernetes.io/instance: prometheus
      app.kubernetes.io/name: kube-state-metrics
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      meta.helm.sh/release-name: prometheus
      meta.helm.sh/release-namespace: monitoring
      prometheus.io/scrape: "true"
    creationTimestamp: "2025-02-26T10:33:03Z"
    labels:
      app.kubernetes.io/component: metrics
      app.kubernetes.io/instance: prometheus
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: prometheus-node-exporter
      app.kubernetes.io/part-of: prometheus-node-exporter
      app.kubernetes.io/version: 1.8.2
      helm.sh/chart: prometheus-node-exporter-4.43.1
      velero.io/backup-name: monitoring-backup
      velero.io/restore-name: monitoring-backup-20250226103249
    name: prometheus-prometheus-node-exporter
    namespace: monitoring
    resourceVersion: "124614"
    uid: 7e2b33d4-5b2a-42db-a23a-c91686b458a6
  spec:
    clusterIP: 10.43.226.22
    clusterIPs:
    - 10.43.226.22
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: metrics
      port: 9100
      protocol: TCP
      targetPort: 9100
    selector:
      app.kubernetes.io/instance: prometheus
      app.kubernetes.io/name: prometheus-node-exporter
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      meta.helm.sh/release-name: prometheus
      meta.helm.sh/release-namespace: monitoring
      prometheus.io/probe: pushgateway
    creationTimestamp: "2025-02-26T10:33:03Z"
    labels:
      app.kubernetes.io/instance: prometheus
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: prometheus-pushgateway
      app.kubernetes.io/version: v1.11.0
      helm.sh/chart: prometheus-pushgateway-2.17.0
      velero.io/backup-name: monitoring-backup
      velero.io/restore-name: monitoring-backup-20250226103249
    name: prometheus-prometheus-pushgateway
    namespace: monitoring
    resourceVersion: "124617"
    uid: b2e623fc-0731-4218-a7f6-bb962a582f26
  spec:
    clusterIP: 10.43.202.144
    clusterIPs:
    - 10.43.202.144
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: http
      port: 9091
      protocol: TCP
      targetPort: 9091
    selector:
      app.kubernetes.io/instance: prometheus
      app.kubernetes.io/name: prometheus-pushgateway
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      field.cattle.io/publicEndpoints: "null"
      meta.helm.sh/release-name: prometheus
      meta.helm.sh/release-namespace: monitoring
    creationTimestamp: "2025-02-26T10:33:04Z"
    labels:
      app.kubernetes.io/component: server
      app.kubernetes.io/instance: prometheus
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: prometheus
      app.kubernetes.io/part-of: prometheus
      app.kubernetes.io/version: v3.1.0
      helm.sh/chart: prometheus-27.3.0
      velero.io/backup-name: monitoring-backup
      velero.io/restore-name: monitoring-backup-20250226103249
    name: prometheus-server
    namespace: monitoring
    resourceVersion: "124623"
    uid: d9822508-3218-4ba4-be17-59087d399a96
  spec:
    clusterIP: 10.43.2.149
    clusterIPs:
    - 10.43.2.149
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - port: 80
      protocol: TCP
      targetPort: 8080
    selector:
      app.kubernetes.io/component: server
      app.kubernetes.io/instance: prometheus
      app.kubernetes.io/name: prometheus
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      field.cattle.io/publicEndpoints: '[{"port":30089,"protocol":"TCP","serviceName":"monitoring:velero-runner","allNodes":true}]'
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"v1","kind":"Service","metadata":{"annotations":{},"name":"velero-runner","namespace":"monitoring"},"spec":{"ports":[{"nodePort":30089,"port":80,"protocol":"TCP","targetPort":5000}],"selector":{"app":"velero-runner"},"type":"NodePort"}}
    creationTimestamp: "2025-02-26T10:33:04Z"
    labels:
      velero.io/backup-name: monitoring-backup
      velero.io/restore-name: monitoring-backup-20250226103249
    name: velero-runner
    namespace: monitoring
    resourceVersion: "124628"
    uid: 9c631e27-fad3-4c19-8e8b-27722a092cc0
  spec:
    clusterIP: 10.43.147.14
    clusterIPs:
    - 10.43.147.14
    externalTrafficPolicy: Cluster
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - nodePort: 30089
      port: 80
      protocol: TCP
      targetPort: 5000
    selector:
      app: velero-runner
    sessionAffinity: None
    type: NodePort
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      field.cattle.io/publicEndpoints: '[{"port":30080,"protocol":"TCP","serviceName":"monitoring:webhook-service","allNodes":true}]'
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"v1","kind":"Service","metadata":{"annotations":{},"name":"webhook-service","namespace":"monitoring"},"spec":{"ports":[{"nodePort":30080,"port":80,"protocol":"TCP","targetPort":3000}],"selector":{"app":"webhook"},"type":"NodePort"}}
    creationTimestamp: "2025-02-26T10:33:04Z"
    labels:
      velero.io/backup-name: monitoring-backup
      velero.io/restore-name: monitoring-backup-20250226103249
    name: webhook-service
    namespace: monitoring
    resourceVersion: "124637"
    uid: 5948578c-eb6e-491c-8f81-5238af7f7a16
  spec:
    clusterIP: 10.43.80.224
    clusterIPs:
    - 10.43.80.224
    externalTrafficPolicy: Cluster
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - nodePort: 30080
      port: 80
      protocol: TCP
      targetPort: 3000
    selector:
      app: webhook
    sessionAffinity: None
    type: NodePort
  status:
    loadBalancer: {}
- apiVersion: apps/v1
  kind: DaemonSet
  metadata:
    annotations:
      deprecated.daemonset.template.generation: "1"
      field.cattle.io/publicEndpoints: '[{"nodeName":":ip-172-31-17-126","addresses":["172.31.17.126"],"port":80,"protocol":"TCP","podName":"kube-system:svclb-traefik-e88bf217-2qkpd","allNodes":false},{"nodeName":":ip-172-31-17-126","addresses":["172.31.17.126"],"port":443,"protocol":"TCP","podName":"kube-system:svclb-traefik-e88bf217-2qkpd","allNodes":false}]'
      objectset.rio.cattle.io/applied: H4sIAAAAAAAA/8RVwW7jNhD9lWLOsmJH3sQR0EOQBEXQrmPY3l4WRjCiRjFriiTIkTZGoH8vSDtZpWsnQYFu4YNBcuZx+N7ozROglX+S89JoyAGt9SftCBLYSF1CDtdItdELYkigJsYSGSF/AtTaMLI02oelKf4iwZ44ddKkApkVpdKcyIABydFz802TGzy0G8jhpB0lv/wudfnrglwrBb2bp7EmyIEdUiU3Hwr3FkXI2TQFDfzWM9XQJSAcxccsZU2esbaQ60apBBQWpN584hr9GnKYZGeVKAQNT0/LTyXSpyI7O7sYVzQirLLJhbjIsDovBSTgWyGMZmeUIpduMt9D06YkT4oEGwc5VKg8vZPiW/EDER+IP8LEHsq3QhWDPeCAJpOiOh2dw+78SKq3JAJT3+t/ghpZrP94IRGtPQ7edQkw1VYhU8zt9dsHBHoT++dR2CMCGza1aTTvG/pSiLBamg1pyKO2CYRLUGpyHvKvT0C6jf/7ehbzq/vZ3XwJCbSomrA1GUKXvAqYX05/u1n0QoZp/J28iry+WSzvZ/O75V0vcnk1+zHmrftixO2sf9tomI6z9HSUpZNz6FYJyBofwoFDLdbkTjZKWktuoIq8Habj9AL2MbNGqZlRUmwhh9tqanjmyJNmeGnEIKawg8kQErDG8Y6lF9JmxjHkk2ECa+P5++pQtjNshFHPr14l4MibxgkK/RN0I9E4ydsro5keOfYdWiykkixp12RlCflXmN4s7y+vP99OYdV1gZ33ZRuPs5+r2z8u/J+EC1W8odx4nPWli8uDAP+ZeKsALk1MVej9dG+A8XMeBD8eCCdZClRw8Ba/9YKV78uviVNp23Eq7X1l3Dd0ZZ926Fax4L4pTHu+CwmwUeSe52uwhaoiwZDD1CzEmspGhbGwocB/rNEZRWkwIqeJyQeXqtEzuTAWbcCKA+XmUXr2sS/+DeTeEQdWoaajyDuMqz1rl2VptL/Tans4YRUss7ElMi3YIdPDNtAajFfqhy/xYDdKHr9obFEqLBRBPgrjYmsDa/NXsdGCGbmJoovGOdI8beqC3PNDS8iHCZTkpaPy0JGOe5+l9we254TlFvJh1/0dAAD//xqmdT4/CQAA
      objectset.rio.cattle.io/id: ""
      objectset.rio.cattle.io/owner-gvk: /v1, Kind=Service
      objectset.rio.cattle.io/owner-name: traefik
      objectset.rio.cattle.io/owner-namespace: kube-system
    creationTimestamp: "2025-02-05T04:54:52Z"
    generation: 1
    labels:
      objectset.rio.cattle.io/hash: 836fcbce022d5dae5b36694fe1eaf389c93af7dc
      svccontroller.k3s.cattle.io/nodeselector: "false"
      svccontroller.k3s.cattle.io/svcname: traefik
      svccontroller.k3s.cattle.io/svcnamespace: kube-system
    name: svclb-traefik-e88bf217
    namespace: kube-system
    resourceVersion: "196087"
    uid: 4f9059e8-6026-472c-a713-e0a5dc5dc244
  spec:
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app: svclb-traefik-e88bf217
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: svclb-traefik-e88bf217
          svccontroller.k3s.cattle.io/svcname: traefik
          svccontroller.k3s.cattle.io/svcnamespace: kube-system
      spec:
        automountServiceAccountToken: false
        containers:
        - env:
          - name: SRC_PORT
            value: "80"
          - name: SRC_RANGES
            value: 0.0.0.0/0
          - name: DEST_PROTO
            value: TCP
          - name: DEST_PORT
            value: "80"
          - name: DEST_IPS
            value: 10.43.213.87
          image: rancher/klipper-lb:v0.4.9
          imagePullPolicy: IfNotPresent
          name: lb-tcp-80
          ports:
          - containerPort: 80
            hostPort: 80
            name: lb-tcp-80
            protocol: TCP
          resources: {}
          securityContext:
            capabilities:
              add:
              - NET_ADMIN
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        - env:
          - name: SRC_PORT
            value: "443"
          - name: SRC_RANGES
            value: 0.0.0.0/0
          - name: DEST_PROTO
            value: TCP
          - name: DEST_PORT
            value: "443"
          - name: DEST_IPS
            value: 10.43.213.87
          image: rancher/klipper-lb:v0.4.9
          imagePullPolicy: IfNotPresent
          name: lb-tcp-443
          ports:
          - containerPort: 443
            hostPort: 443
            name: lb-tcp-443
            protocol: TCP
          resources: {}
          securityContext:
            capabilities:
              add:
              - NET_ADMIN
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        priorityClassName: system-node-critical
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          sysctls:
          - name: net.ipv4.ip_forward
            value: "1"
        serviceAccount: svclb
        serviceAccountName: svclb
        terminationGracePeriodSeconds: 30
        tolerations:
        - effect: NoSchedule
          key: node-role.kubernetes.io/master
          operator: Exists
        - effect: NoSchedule
          key: node-role.kubernetes.io/control-plane
          operator: Exists
        - key: CriticalAddonsOnly
          operator: Exists
    updateStrategy:
      rollingUpdate:
        maxSurge: 0
        maxUnavailable: 1
      type: RollingUpdate
  status:
    currentNumberScheduled: 1
    desiredNumberScheduled: 1
    numberAvailable: 1
    numberMisscheduled: 0
    numberReady: 1
    observedGeneration: 1
    updatedNumberScheduled: 1
- apiVersion: apps/v1
  kind: DaemonSet
  metadata:
    annotations:
      deprecated.daemonset.template.generation: "1"
      field.cattle.io/publicEndpoints: '[{"nodeName":":ip-172-31-17-126","addresses":["172.31.17.126"],"port":9100,"protocol":"TCP","podName":"monitoring:prometheus-prometheus-node-exporter-z5q4v","allNodes":false}]'
      meta.helm.sh/release-name: prometheus
      meta.helm.sh/release-namespace: monitoring
    creationTimestamp: "2025-02-26T10:33:04Z"
    generation: 1
    labels:
      app.kubernetes.io/component: metrics
      app.kubernetes.io/instance: prometheus
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: prometheus-node-exporter
      app.kubernetes.io/part-of: prometheus-node-exporter
      app.kubernetes.io/version: 1.8.2
      helm.sh/chart: prometheus-node-exporter-4.43.1
      velero.io/backup-name: monitoring-backup
      velero.io/restore-name: monitoring-backup-20250226103249
    name: prometheus-prometheus-node-exporter
    namespace: monitoring
    resourceVersion: "162540"
    uid: 9a003536-1a78-492e-b653-fa6e7003213c
  spec:
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app.kubernetes.io/instance: prometheus
        app.kubernetes.io/name: prometheus-node-exporter
    template:
      metadata:
        annotations:
          cluster-autoscaler.kubernetes.io/safe-to-evict: "true"
        creationTimestamp: null
        labels:
          app.kubernetes.io/component: metrics
          app.kubernetes.io/instance: prometheus
          app.kubernetes.io/managed-by: Helm
          app.kubernetes.io/name: prometheus-node-exporter
          app.kubernetes.io/part-of: prometheus-node-exporter
          app.kubernetes.io/version: 1.8.2
          helm.sh/chart: prometheus-node-exporter-4.43.1
      spec:
        affinity:
          nodeAffinity:
            requiredDuringSchedulingIgnoredDuringExecution:
              nodeSelectorTerms:
              - matchExpressions:
                - key: eks.amazonaws.com/compute-type
                  operator: NotIn
                  values:
                  - fargate
                - key: type
                  operator: NotIn
                  values:
                  - virtual-kubelet
        automountServiceAccountToken: false
        containers:
        - args:
          - --path.procfs=/host/proc
          - --path.sysfs=/host/sys
          - --path.rootfs=/host/root
          - --path.udev.data=/host/root/run/udev/data
          - --web.listen-address=[$(HOST_IP)]:9100
          env:
          - name: HOST_IP
            value: 0.0.0.0
          image: quay.io/prometheus/node-exporter:v1.8.2
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /
              port: 9100
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          name: node-exporter
          ports:
          - containerPort: 9100
            name: metrics
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /
              port: 9100
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          resources: {}
          securityContext:
            allowPrivilegeEscalation: false
            readOnlyRootFilesystem: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /host/proc
            name: proc
            readOnly: true
          - mountPath: /host/sys
            name: sys
            readOnly: true
          - mountPath: /host/root
            mountPropagation: HostToContainer
            name: root
            readOnly: true
        dnsPolicy: ClusterFirst
        hostNetwork: true
        hostPID: true
        nodeSelector:
          kubernetes.io/os: linux
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 65534
          runAsGroup: 65534
          runAsNonRoot: true
          runAsUser: 65534
        serviceAccount: prometheus-prometheus-node-exporter
        serviceAccountName: prometheus-prometheus-node-exporter
        terminationGracePeriodSeconds: 30
        tolerations:
        - effect: NoSchedule
          operator: Exists
        volumes:
        - hostPath:
            path: /proc
            type: ""
          name: proc
        - hostPath:
            path: /sys
            type: ""
          name: sys
        - hostPath:
            path: /
            type: ""
          name: root
    updateStrategy:
      rollingUpdate:
        maxSurge: 0
        maxUnavailable: 1
      type: RollingUpdate
  status:
    currentNumberScheduled: 1
    desiredNumberScheduled: 1
    numberAvailable: 1
    numberMisscheduled: 0
    numberReady: 1
    observedGeneration: 1
    updatedNumberScheduled: 1
- apiVersion: apps/v1
  kind: DaemonSet
  metadata:
    annotations:
      deprecated.daemonset.template.generation: "1"
    creationTimestamp: "2025-02-06T14:32:37Z"
    generation: 1
    labels:
      component: velero
    name: node-agent
    namespace: velero
    resourceVersion: "196260"
    uid: e81668b1-2c5e-4aab-9365-640c72e4975d
  spec:
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        name: node-agent
    template:
      metadata:
        creationTimestamp: null
        labels:
          component: velero
          name: node-agent
      spec:
        containers:
        - args:
          - node-agent
          - server
          - --features=
          command:
          - /velero
          env:
          - name: NODE_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: spec.nodeName
          - name: VELERO_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          - name: VELERO_SCRATCH_DIR
            value: /scratch
          image: velero/velero:v1.15.2
          imagePullPolicy: IfNotPresent
          name: node-agent
          ports:
          - containerPort: 8085
            name: metrics
            protocol: TCP
          resources: {}
          securityContext:
            privileged: false
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /host_pods
            mountPropagation: HostToContainer
            name: host-pods
          - mountPath: /var/lib/kubelet/plugins
            mountPropagation: HostToContainer
            name: host-plugins
          - mountPath: /scratch
            name: scratch
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          runAsUser: 0
        serviceAccount: velero
        serviceAccountName: velero
        terminationGracePeriodSeconds: 30
        volumes:
        - hostPath:
            path: /var/lib/kubelet/pods
            type: ""
          name: host-pods
        - hostPath:
            path: /var/lib/kubelet/plugins
            type: ""
          name: host-plugins
        - emptyDir: {}
          name: scratch
    updateStrategy:
      rollingUpdate:
        maxSurge: 0
        maxUnavailable: 1
      type: RollingUpdate
  status:
    currentNumberScheduled: 1
    desiredNumberScheduled: 1
    numberAvailable: 1
    numberMisscheduled: 0
    numberReady: 1
    observedGeneration: 1
    updatedNumberScheduled: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "1"
      meta.helm.sh/release-name: fleet
      meta.helm.sh/release-namespace: cattle-fleet-system
    creationTimestamp: "2025-02-05T05:33:55Z"
    generation: 1
    labels:
      app.kubernetes.io/managed-by: Helm
    name: fleet-controller
    namespace: cattle-fleet-system
    resourceVersion: "162474"
    uid: 104d03cc-34b5-4d7c-ac7e-8bcf237b9a01
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app: fleet-controller
    strategy:
      rollingUpdate:
        maxSurge: 25%
        maxUnavailable: 25%
      type: RollingUpdate
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: fleet-controller
          fleet.cattle.io/shard-default: "true"
          fleet.cattle.io/shard-id: ""
      spec:
        containers:
        - command:
          - fleetcontroller
          env:
          - name: NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          - name: CATTLE_ELECTION_LEASE_DURATION
            value: 30s
          - name: CATTLE_ELECTION_RETRY_PERIOD
            value: 10s
          - name: CATTLE_ELECTION_RENEW_DEADLINE
            value: 25s
          - name: BUNDLE_RECONCILER_WORKERS
            value: "50"
          - name: BUNDLEDEPLOYMENT_RECONCILER_WORKERS
            value: "50"
          - name: CLUSTER_RECONCILER_WORKERS
            value: "50"
          - name: CLUSTERGROUP_RECONCILER_WORKERS
            value: "50"
          - name: IMAGESCAN_RECONCILER_WORKERS
            value: "50"
          image: rancher/fleet:v0.11.3
          imagePullPolicy: IfNotPresent
          name: fleet-controller
          ports:
          - containerPort: 8080
            name: metrics
            protocol: TCP
          resources: {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            privileged: false
            readOnlyRootFilesystem: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /tmp
            name: tmp
        - command:
          - fleetcontroller
          - cleanup
          env:
          - name: NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          - name: CATTLE_ELECTION_LEASE_DURATION
            value: 30s
          - name: CATTLE_ELECTION_RETRY_PERIOD
            value: 10s
          - name: CATTLE_ELECTION_RENEW_DEADLINE
            value: 25s
          image: rancher/fleet:v0.11.3
          imagePullPolicy: IfNotPresent
          name: fleet-cleanup
          resources: {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            privileged: false
            readOnlyRootFilesystem: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        - command:
          - fleetcontroller
          - agentmanagement
          - --disable-bootstrap
          env:
          - name: NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          - name: FLEET_PROPAGATE_DEBUG_SETTINGS_TO_AGENTS
            value: "true"
          - name: FLEET_DEBUG_DISABLE_SECURITY_CONTEXT
            value: "false"
          - name: CATTLE_ELECTION_LEASE_DURATION
            value: 30s
          - name: CATTLE_ELECTION_RETRY_PERIOD
            value: 10s
          - name: CATTLE_ELECTION_RENEW_DEADLINE
            value: 25s
          image: rancher/fleet:v0.11.3
          imagePullPolicy: IfNotPresent
          name: fleet-agentmanagement
          resources: {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            privileged: false
            readOnlyRootFilesystem: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        nodeSelector:
          kubernetes.io/os: linux
        priorityClassName: rancher-critical
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          runAsGroup: 1000
          runAsNonRoot: true
          runAsUser: 1000
        serviceAccount: fleet-controller
        serviceAccountName: fleet-controller
        terminationGracePeriodSeconds: 30
        tolerations:
        - effect: NoSchedule
          key: cattle.io/os
          operator: Equal
          value: linux
        volumes:
        - emptyDir: {}
          name: tmp
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2025-02-05T05:33:55Z"
      lastUpdateTime: "2025-02-05T05:34:02Z"
      message: ReplicaSet "fleet-controller-86bfd4bff" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    - lastTransitionTime: "2025-02-27T11:59:07Z"
      lastUpdateTime: "2025-02-27T11:59:07Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "1"
      meta.helm.sh/release-name: fleet
      meta.helm.sh/release-namespace: cattle-fleet-system
    creationTimestamp: "2025-02-05T05:33:55Z"
    generation: 1
    labels:
      app.kubernetes.io/managed-by: Helm
    name: gitjob
    namespace: cattle-fleet-system
    resourceVersion: "196274"
    uid: c903e399-d293-4097-9bbb-6e8f1710ef8a
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app: gitjob
    strategy:
      rollingUpdate:
        maxSurge: 25%
        maxUnavailable: 25%
      type: RollingUpdate
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: gitjob
          fleet.cattle.io/shard-default: "true"
          fleet.cattle.io/shard-id: ""
      spec:
        containers:
        - args:
          - fleetcontroller
          - gitjob
          - --gitjob-image
          - rancher/fleet:v0.11.3
          env:
          - name: NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          - name: CATTLE_ELECTION_LEASE_DURATION
            value: 30s
          - name: CATTLE_ELECTION_RETRY_PERIOD
            value: 10s
          - name: CATTLE_ELECTION_RENEW_DEADLINE
            value: 25s
          - name: GITREPO_SYNC_PERIOD
            value: 2h
          - name: GITREPO_RECONCILER_WORKERS
            value: "50"
          image: rancher/fleet:v0.11.3
          imagePullPolicy: IfNotPresent
          name: gitjob
          ports:
          - containerPort: 8081
            name: metrics
            protocol: TCP
          resources: {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            privileged: false
            readOnlyRootFilesystem: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /tmp
            name: tmp
        dnsPolicy: ClusterFirst
        nodeSelector:
          kubernetes.io/os: linux
        priorityClassName: rancher-critical
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          runAsGroup: 1000
          runAsNonRoot: true
          runAsUser: 1000
        serviceAccount: gitjob
        serviceAccountName: gitjob
        terminationGracePeriodSeconds: 30
        tolerations:
        - effect: NoSchedule
          key: cattle.io/os
          operator: Equal
          value: linux
        volumes:
        - emptyDir: {}
          name: tmp
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2025-02-05T05:33:55Z"
      lastUpdateTime: "2025-02-05T05:34:02Z"
      message: ReplicaSet "gitjob-547d87f97f" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    - lastTransitionTime: "2025-03-03T02:55:37Z"
      lastUpdateTime: "2025-03-03T02:55:37Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "1"
      meta.helm.sh/release-name: rancher-provisioning-capi
      meta.helm.sh/release-namespace: cattle-provisioning-capi-system
    creationTimestamp: "2025-02-05T05:35:00Z"
    generation: 1
    labels:
      app.kubernetes.io/managed-by: Helm
      cluster.x-k8s.io/provider: cluster-api
      control-plane: controller-manager
    name: capi-controller-manager
    namespace: cattle-provisioning-capi-system
    resourceVersion: "162517"
    uid: 552ce176-6e04-4294-a371-a3dc3d60d4ec
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        cluster.x-k8s.io/provider: cluster-api
        control-plane: controller-manager
    strategy:
      rollingUpdate:
        maxSurge: 25%
        maxUnavailable: 25%
      type: RollingUpdate
    template:
      metadata:
        creationTimestamp: null
        labels:
          cluster.x-k8s.io/provider: cluster-api
          control-plane: controller-manager
      spec:
        containers:
        - args:
          - --leader-elect
          - --metrics-bind-addr=localhost:8080
          - --feature-gates=MachinePool=false,ClusterResourceSet=false,ClusterTopology=false,RuntimeSDK=false
          command:
          - /manager
          env:
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: POD_UID
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.uid
          image: rancher/mirrored-cluster-api-controller:v1.8.3
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /healthz
              port: healthz
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          name: manager
          ports:
          - containerPort: 9443
            name: webhook-server
            protocol: TCP
          - containerPort: 9440
            name: healthz
            protocol: TCP
          - containerPort: 8443
            name: metrics
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /readyz
              port: healthz
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          resources: {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            privileged: false
            runAsGroup: 65532
            runAsUser: 65532
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: FallbackToLogsOnError
          volumeMounts:
          - mountPath: /tmp/k8s-webhook-server/serving-certs
            name: cert
            readOnly: true
        dnsPolicy: ClusterFirst
        nodeSelector:
          kubernetes.io/os: linux
        priorityClassName: rancher-critical
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          runAsNonRoot: true
          seccompProfile:
            type: RuntimeDefault
        serviceAccount: capi-manager
        serviceAccountName: capi-manager
        terminationGracePeriodSeconds: 10
        tolerations:
        - effect: NoSchedule
          key: cattle.io/os
          operator: Equal
          value: linux
        - effect: NoSchedule
          key: node-role.kubernetes.io/controlplane
          value: "true"
        - effect: NoSchedule
          key: node-role.kubernetes.io/control-plane
          operator: Exists
        - effect: NoSchedule
          key: node-role.kubernetes.io/master
          operator: Exists
        - effect: NoExecute
          key: node-role.kubernetes.io/etcd
          operator: Exists
        volumes:
        - name: cert
          secret:
            defaultMode: 420
            secretName: capi-webhook-service-cert
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2025-02-05T05:35:00Z"
      lastUpdateTime: "2025-02-05T05:35:07Z"
      message: ReplicaSet "capi-controller-manager-cc78bcdb8" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    - lastTransitionTime: "2025-02-27T11:59:12Z"
      lastUpdateTime: "2025-02-27T11:59:12Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "2"
      field.cattle.io/publicEndpoints: '[{"port":32493,"protocol":"TCP","serviceName":"cattle-system:rancher","allNodes":true},{"port":30199,"protocol":"TCP","serviceName":"cattle-system:rancher","allNodes":true},{"addresses":["172.31.17.126"],"port":443,"protocol":"HTTPS","serviceName":"cattle-system:rancher","ingressName":"cattle-system:rancher","hostname":"44.220.0.65.sslip.io","path":"/","allNodes":false}]'
      meta.helm.sh/release-name: rancher
      meta.helm.sh/release-namespace: cattle-system
    creationTimestamp: "2025-02-05T05:31:08Z"
    generation: 4
    labels:
      app: rancher
      app.kubernetes.io/managed-by: Helm
      chart: rancher-2.10.2
      heritage: Helm
      release: rancher
    name: rancher
    namespace: cattle-system
    resourceVersion: "196591"
    uid: 356f8ee7-4b23-4dce-b5aa-049d29d0a6b8
  spec:
    progressDeadlineSeconds: 600
    replicas: 3
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app: rancher
    strategy:
      rollingUpdate:
        maxSurge: 1
        maxUnavailable: 1
      type: RollingUpdate
    template:
      metadata:
        annotations:
          kubectl.kubernetes.io/restartedAt: "2025-02-05T06:43:35Z"
        creationTimestamp: null
        labels:
          app: rancher
          release: rancher
      spec:
        affinity:
          nodeAffinity:
            requiredDuringSchedulingIgnoredDuringExecution:
              nodeSelectorTerms:
              - matchExpressions:
                - key: kubernetes.io/os
                  operator: NotIn
                  values:
                  - windows
          podAntiAffinity:
            preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchExpressions:
                  - key: app
                    operator: In
                    values:
                    - rancher
                topologyKey: kubernetes.io/hostname
              weight: 100
        containers:
        - args:
          - --no-cacerts
          - --http-listen-port=80
          - --https-listen-port=443
          - --add-local=true
          env:
          - name: CATTLE_NAMESPACE
            value: cattle-system
          - name: CATTLE_PEER_SERVICE
            value: rancher
          image: rancher/rancher:v2.10.2
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 5
            httpGet:
              path: /healthz
              port: 80
              scheme: HTTP
            periodSeconds: 30
            successThreshold: 1
            timeoutSeconds: 5
          name: rancher
          ports:
          - containerPort: 80
            protocol: TCP
          readinessProbe:
            failureThreshold: 5
            httpGet:
              path: /healthz
              port: 80
              scheme: HTTP
            periodSeconds: 30
            successThreshold: 1
            timeoutSeconds: 5
          resources: {}
          startupProbe:
            failureThreshold: 12
            httpGet:
              path: /healthz
              port: 80
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        priorityClassName: rancher-critical
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: rancher
        serviceAccountName: rancher
        terminationGracePeriodSeconds: 30
        tolerations:
        - effect: NoSchedule
          key: cattle.io/os
          operator: Equal
          value: linux
  status:
    availableReplicas: 3
    conditions:
    - lastTransitionTime: "2025-02-05T05:31:08Z"
      lastUpdateTime: "2025-02-05T06:44:48Z"
      message: ReplicaSet "rancher-c6b4fd756" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    - lastTransitionTime: "2025-03-03T02:56:18Z"
      lastUpdateTime: "2025-03-03T02:56:18Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    observedGeneration: 4
    readyReplicas: 3
    replicas: 3
    updatedReplicas: 3
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "1"
      field.cattle.io/publicEndpoints: '[{"port":30449,"protocol":"TCP","serviceName":"cattle-system:rancher-webhook","allNodes":true}]'
      meta.helm.sh/release-name: rancher-webhook
      meta.helm.sh/release-namespace: cattle-system
    creationTimestamp: "2025-02-05T05:34:32Z"
    generation: 2
    labels:
      app.kubernetes.io/managed-by: Helm
    name: rancher-webhook
    namespace: cattle-system
    resourceVersion: "196313"
    uid: cec28531-fe69-4562-8264-6858bb9659bb
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app: rancher-webhook
    strategy:
      rollingUpdate:
        maxSurge: 25%
        maxUnavailable: 25%
      type: RollingUpdate
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: rancher-webhook
      spec:
        containers:
        - env:
          - name: STAMP
          - name: ENABLE_MCM
            value: "true"
          - name: CATTLE_PORT
            value: "9443"
          - name: NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          image: rancher/rancher-webhook:v0.6.3
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /healthz
              port: https
              scheme: HTTPS
            periodSeconds: 5
            successThreshold: 1
            timeoutSeconds: 1
          name: rancher-webhook
          ports:
          - containerPort: 9443
            name: https
            protocol: TCP
          resources: {}
          startupProbe:
            failureThreshold: 60
            httpGet:
              path: /healthz
              port: https
              scheme: HTTPS
            periodSeconds: 5
            successThreshold: 1
            timeoutSeconds: 1
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        nodeSelector:
          kubernetes.io/os: linux
        priorityClassName: rancher-critical
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: rancher-webhook
        serviceAccountName: rancher-webhook
        terminationGracePeriodSeconds: 30
        tolerations:
        - effect: NoSchedule
          key: cattle.io/os
          operator: Equal
          value: linux
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2025-02-05T05:34:32Z"
      lastUpdateTime: "2025-02-05T05:34:53Z"
      message: ReplicaSet "rancher-webhook-554ffd94d8" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    - lastTransitionTime: "2025-03-03T02:55:38Z"
      lastUpdateTime: "2025-03-03T02:55:38Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    observedGeneration: 2
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "1"
      meta.helm.sh/release-name: system-upgrade-controller
      meta.helm.sh/release-namespace: cattle-system
    creationTimestamp: "2025-02-05T05:35:18Z"
    generation: 1
    labels:
      app.kubernetes.io/managed-by: Helm
    name: system-upgrade-controller
    namespace: cattle-system
    resourceVersion: "196245"
    uid: 709a66bf-6f03-4474-8362-d957132dcbcb
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        upgrade.cattle.io/controller: system-upgrade-controller
    strategy:
      rollingUpdate:
        maxSurge: 25%
        maxUnavailable: 25%
      type: RollingUpdate
    template:
      metadata:
        creationTimestamp: null
        labels:
          upgrade.cattle.io/controller: system-upgrade-controller
      spec:
        affinity:
          nodeAffinity:
            preferredDuringSchedulingIgnoredDuringExecution:
            - preference:
                matchExpressions:
                - key: node-role.kubernetes.io/control-plane
                  operator: In
                  values:
                  - "true"
              weight: 100
            - preference:
                matchExpressions:
                - key: node-role.kubernetes.io/master
                  operator: In
                  values:
                  - "true"
              weight: 100
            requiredDuringSchedulingIgnoredDuringExecution:
              nodeSelectorTerms:
              - matchExpressions:
                - key: kubernetes.io/os
                  operator: NotIn
                  values:
                  - windows
        containers:
        - env:
          - name: SYSTEM_UPGRADE_CONTROLLER_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.labels['upgrade.cattle.io/controller']
          - name: SYSTEM_UPGRADE_CONTROLLER_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          envFrom:
          - configMapRef:
              name: system-upgrade-controller-config
          image: rancher/system-upgrade-controller:v0.14.2
          imagePullPolicy: IfNotPresent
          name: system-upgrade-controller
          resources: {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            runAsGroup: 65534
            runAsNonRoot: true
            runAsUser: 65534
            seccompProfile:
              type: RuntimeDefault
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/ssl
            name: etc-ssl
            readOnly: true
          - mountPath: /etc/pki
            name: etc-pki
            readOnly: true
          - mountPath: /etc/ca-certificates
            name: etc-ca-certificates
            readOnly: true
          - mountPath: /tmp
            name: tmp
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: system-upgrade-controller
        serviceAccountName: system-upgrade-controller
        terminationGracePeriodSeconds: 30
        tolerations:
        - operator: Exists
        volumes:
        - hostPath:
            path: /etc/ssl
            type: DirectoryOrCreate
          name: etc-ssl
        - hostPath:
            path: /etc/pki
            type: DirectoryOrCreate
          name: etc-pki
        - hostPath:
            path: /etc/ca-certificates
            type: DirectoryOrCreate
          name: etc-ca-certificates
        - emptyDir: {}
          name: tmp
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2025-02-05T05:35:18Z"
      lastUpdateTime: "2025-02-05T05:35:21Z"
      message: ReplicaSet "system-upgrade-controller-5fb67f585d" has successfully
        progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    - lastTransitionTime: "2025-03-03T02:55:35Z"
      lastUpdateTime: "2025-03-03T02:55:35Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "1"
      meta.helm.sh/release-name: cert-manager
      meta.helm.sh/release-namespace: cert-manager
    creationTimestamp: "2025-02-05T04:55:46Z"
    generation: 1
    labels:
      app: cert-manager
      app.kubernetes.io/component: controller
      app.kubernetes.io/instance: cert-manager
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: cert-manager
      app.kubernetes.io/version: v1.17.0
      helm.sh/chart: cert-manager-v1.17.0
    name: cert-manager
    namespace: cert-manager
    resourceVersion: "196231"
    uid: 05281867-d295-40d0-9c7b-d8c22a054fd9
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app.kubernetes.io/component: controller
        app.kubernetes.io/instance: cert-manager
        app.kubernetes.io/name: cert-manager
    strategy:
      rollingUpdate:
        maxSurge: 25%
        maxUnavailable: 25%
      type: RollingUpdate
    template:
      metadata:
        annotations:
          prometheus.io/path: /metrics
          prometheus.io/port: "9402"
          prometheus.io/scrape: "true"
        creationTimestamp: null
        labels:
          app: cert-manager
          app.kubernetes.io/component: controller
          app.kubernetes.io/instance: cert-manager
          app.kubernetes.io/managed-by: Helm
          app.kubernetes.io/name: cert-manager
          app.kubernetes.io/version: v1.17.0
          helm.sh/chart: cert-manager-v1.17.0
      spec:
        containers:
        - args:
          - --v=2
          - --cluster-resource-namespace=$(POD_NAMESPACE)
          - --leader-election-namespace=kube-system
          - --acme-http01-solver-image=quay.io/jetstack/cert-manager-acmesolver:v1.17.0
          - --max-concurrent-challenges=60
          env:
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          image: quay.io/jetstack/cert-manager-controller:v1.17.0
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 8
            httpGet:
              path: /livez
              port: http-healthz
              scheme: HTTP
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 15
          name: cert-manager-controller
          ports:
          - containerPort: 9402
            name: http-metrics
            protocol: TCP
          - containerPort: 9403
            name: http-healthz
            protocol: TCP
          resources: {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        enableServiceLinks: false
        nodeSelector:
          kubernetes.io/os: linux
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          runAsNonRoot: true
          seccompProfile:
            type: RuntimeDefault
        serviceAccount: cert-manager
        serviceAccountName: cert-manager
        terminationGracePeriodSeconds: 30
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2025-02-05T04:55:46Z"
      lastUpdateTime: "2025-02-05T04:55:52Z"
      message: ReplicaSet "cert-manager-6846f9d58c" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    - lastTransitionTime: "2025-03-03T02:55:34Z"
      lastUpdateTime: "2025-03-03T02:55:34Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "1"
      meta.helm.sh/release-name: cert-manager
      meta.helm.sh/release-namespace: cert-manager
    creationTimestamp: "2025-02-05T04:55:46Z"
    generation: 1
    labels:
      app: cainjector
      app.kubernetes.io/component: cainjector
      app.kubernetes.io/instance: cert-manager
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: cainjector
      app.kubernetes.io/version: v1.17.0
      helm.sh/chart: cert-manager-v1.17.0
    name: cert-manager-cainjector
    namespace: cert-manager
    resourceVersion: "196283"
    uid: 20cddda3-c743-4ac7-80e6-048092358b97
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app.kubernetes.io/component: cainjector
        app.kubernetes.io/instance: cert-manager
        app.kubernetes.io/name: cainjector
    strategy:
      rollingUpdate:
        maxSurge: 25%
        maxUnavailable: 25%
      type: RollingUpdate
    template:
      metadata:
        annotations:
          prometheus.io/path: /metrics
          prometheus.io/port: "9402"
          prometheus.io/scrape: "true"
        creationTimestamp: null
        labels:
          app: cainjector
          app.kubernetes.io/component: cainjector
          app.kubernetes.io/instance: cert-manager
          app.kubernetes.io/managed-by: Helm
          app.kubernetes.io/name: cainjector
          app.kubernetes.io/version: v1.17.0
          helm.sh/chart: cert-manager-v1.17.0
      spec:
        containers:
        - args:
          - --v=2
          - --leader-election-namespace=kube-system
          env:
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          image: quay.io/jetstack/cert-manager-cainjector:v1.17.0
          imagePullPolicy: IfNotPresent
          name: cert-manager-cainjector
          ports:
          - containerPort: 9402
            name: http-metrics
            protocol: TCP
          resources: {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        enableServiceLinks: false
        nodeSelector:
          kubernetes.io/os: linux
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          runAsNonRoot: true
          seccompProfile:
            type: RuntimeDefault
        serviceAccount: cert-manager-cainjector
        serviceAccountName: cert-manager-cainjector
        terminationGracePeriodSeconds: 30
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2025-02-05T04:55:46Z"
      lastUpdateTime: "2025-02-05T04:55:50Z"
      message: ReplicaSet "cert-manager-cainjector-6fc95694fb" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    - lastTransitionTime: "2025-03-03T02:55:37Z"
      lastUpdateTime: "2025-03-03T02:55:37Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "1"
      meta.helm.sh/release-name: cert-manager
      meta.helm.sh/release-namespace: cert-manager
    creationTimestamp: "2025-02-05T04:55:46Z"
    generation: 1
    labels:
      app: webhook
      app.kubernetes.io/component: webhook
      app.kubernetes.io/instance: cert-manager
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: webhook
      app.kubernetes.io/version: v1.17.0
      helm.sh/chart: cert-manager-v1.17.0
    name: cert-manager-webhook
    namespace: cert-manager
    resourceVersion: "196226"
    uid: d35fbb5f-5cbf-4274-8003-b02c57c407cb
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app.kubernetes.io/component: webhook
        app.kubernetes.io/instance: cert-manager
        app.kubernetes.io/name: webhook
    strategy:
      rollingUpdate:
        maxSurge: 25%
        maxUnavailable: 25%
      type: RollingUpdate
    template:
      metadata:
        annotations:
          prometheus.io/path: /metrics
          prometheus.io/port: "9402"
          prometheus.io/scrape: "true"
        creationTimestamp: null
        labels:
          app: webhook
          app.kubernetes.io/component: webhook
          app.kubernetes.io/instance: cert-manager
          app.kubernetes.io/managed-by: Helm
          app.kubernetes.io/name: webhook
          app.kubernetes.io/version: v1.17.0
          helm.sh/chart: cert-manager-v1.17.0
      spec:
        containers:
        - args:
          - --v=2
          - --secure-port=10250
          - --dynamic-serving-ca-secret-namespace=$(POD_NAMESPACE)
          - --dynamic-serving-ca-secret-name=cert-manager-webhook-ca
          - --dynamic-serving-dns-names=cert-manager-webhook
          - --dynamic-serving-dns-names=cert-manager-webhook.$(POD_NAMESPACE)
          - --dynamic-serving-dns-names=cert-manager-webhook.$(POD_NAMESPACE).svc
          env:
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          image: quay.io/jetstack/cert-manager-webhook:v1.17.0
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /livez
              port: 6080
              scheme: HTTP
            initialDelaySeconds: 60
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          name: cert-manager-webhook
          ports:
          - containerPort: 10250
            name: https
            protocol: TCP
          - containerPort: 6080
            name: healthcheck
            protocol: TCP
          - containerPort: 9402
            name: http-metrics
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /healthz
              port: 6080
              scheme: HTTP
            initialDelaySeconds: 5
            periodSeconds: 5
            successThreshold: 1
            timeoutSeconds: 1
          resources: {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        enableServiceLinks: false
        nodeSelector:
          kubernetes.io/os: linux
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          runAsNonRoot: true
          seccompProfile:
            type: RuntimeDefault
        serviceAccount: cert-manager-webhook
        serviceAccountName: cert-manager-webhook
        terminationGracePeriodSeconds: 30
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2025-02-05T04:55:46Z"
      lastUpdateTime: "2025-02-05T04:55:56Z"
      message: ReplicaSet "cert-manager-webhook-66f876d88c" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    - lastTransitionTime: "2025-03-03T02:55:34Z"
      lastUpdateTime: "2025-03-03T02:55:34Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "1"
      field.cattle.io/publicEndpoints: '[{"port":32131,"protocol":"TCP","serviceName":"default:flask-lstm-service","allNodes":true}]'
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"apps/v1","kind":"Deployment","metadata":{"annotations":{},"name":"flask-lstm","namespace":"default"},"spec":{"replicas":1,"selector":{"matchLabels":{"app":"flask-lstm"}},"template":{"metadata":{"labels":{"app":"flask-lstm"}},"spec":{"containers":[{"image":"gnh374/lstm-model","name":"flask-lstm","ports":[{"containerPort":5000}]}]}}}}
    creationTimestamp: "2025-02-27T12:30:53Z"
    generation: 2
    name: flask-lstm
    namespace: default
    resourceVersion: "196165"
    uid: c0c3952a-5854-46cf-bf14-b753db5ed606
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app: flask-lstm
    strategy:
      rollingUpdate:
        maxSurge: 25%
        maxUnavailable: 25%
      type: RollingUpdate
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: flask-lstm
      spec:
        containers:
        - image: gnh374/lstm-model
          imagePullPolicy: Always
          name: flask-lstm
          ports:
          - containerPort: 5000
            protocol: TCP
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2025-03-02T07:32:14Z"
      lastUpdateTime: "2025-03-02T07:32:14Z"
      message: ReplicaSet "flask-lstm-6f8bc77f4f" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    - lastTransitionTime: "2025-03-03T02:55:28Z"
      lastUpdateTime: "2025-03-03T02:55:28Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    observedGeneration: 2
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "2"
      objectset.rio.cattle.io/applied: H4sIAAAAAAAA/6xVQW/bOBP9Kx/mLMVW0jaugO/QjbPboq3XqJNeiqCgqZHFNcXhkiMnRqD/vhhJduw2TdrFniyTb4ZvHucN70F58xlDNOQgB+V9HG0ySGBtXAE5TNFb2tboGBKokVWhWEF+D8o5YsWGXJS/tPwLNUfkk2DoRCtmiyeGRkaSQPLDfbp1GNLVZg05rM/iwc4mS/733rji/2+KgtyzKZyqEXLQFLBw8afg0SstMetmiWncRsYa2gSsWqLtilpPYqq830H6vPIZHDJGyTYce0EBp7PFE8dWKlaQw1Lj6eTs9PVkkmXnL87U+GzySi1fZuPytHx1juX5i9MXY/3yXIh8V9ITpKNHLZQDbozc5VsTmcL2g6kNQz5OIKJFzRQEVCvW1YenymwlJQfFuNp2acla41bXvlCMfYq7a6c2yli1tAh51ibAWy/MPh1hZR1rb3dxBy30pNDtQVGaHCvjMETIv9yDCiv5gFSTKyGBEbIeDSqN5CZKYxFuEjC1WgmjoJyuMIxqE4LA0gG8+82zk+z0ZAxDxLyxdk7W6C3k8K6cEc8Dxt4C1mzQYYzzQMuuoFIZ2wS8qgLGimwB+VkCFbP/A1n2vWK591GFynIFCXgKDPlkPJFL0RV2d/z26mouUhln2Cg7Rau2C9Tkigj5q3ECHoOhYr+USXCjNcZ4cHKWAJsaqeEH4GN9JBR6KffKzjtWL8/26AEZiEmThRyup8LwmZCUtT8Ou7p4NOx1dhBYIwej4yOBNwkEVIX5V5JL5PZB8WyS/azi3wt++gt6B4zUBI1da1txYOxbv6YgLZWdjz8a6IB/Nxj7Xe0b2RqP627QDtAeKVZA3QTD2wtyjHddmcpaup0HszEWV3gZtbLdPIa8VDZiAlp5tTTWsOmpqKIQ28wur77+9m42/bq4/PT53cWlOKUI5GVPWQs3bS/6n85uPxHx78biMGhyDg22CWzINjV+pMYNfVTL53zQ/cCOcNB9rjSrtI+EhxN2OX+cY6SbyFQfpOr+p89kvJHmKVzcO3mKpWqsmNhRgYuDeXg80ilCDta45k7uyAdDnfBWxTjrCfRqpNo2kTGkOhg2WlmQawobo/GN1lLM7FvjMVkMu0fzyz2sUYhdDPHdQxe7EhIgL0jhB5d3RppENMKyRM2Qw4wWusKisVJ5n0aqSgNZPDmuR5wXyKbeKof/aeZaSf2Pp7yRaj1ZWm0XXq7mgpy8KGbXMt30X/zyq1Sru8Uab3vzDQe871gec6soctcvCdxW6K5dVGxiafrnCqY0I94XKmz7PtqPxdKsPiovRAxjfXRduxcm2U2a/YoI2YNmVOBbEiX2qIclOe6bodz+wCjD6HxgcxyX7r1BXtpK2b1HnzJLe9O2bftPAAAA//8kyNqv/AkAAA
      objectset.rio.cattle.io/id: ""
      objectset.rio.cattle.io/owner-gvk: k3s.cattle.io/v1, Kind=Addon
      objectset.rio.cattle.io/owner-name: coredns
      objectset.rio.cattle.io/owner-namespace: kube-system
    creationTimestamp: "2025-02-05T04:54:31Z"
    generation: 2
    labels:
      k8s-app: kube-dns
      kubernetes.io/name: CoreDNS
      objectset.rio.cattle.io/hash: bce283298811743a0386ab510f2f67ef74240c57
    name: coredns
    namespace: kube-system
    resourceVersion: "167548"
    uid: cbf9da15-e940-4b11-bda3-3f28d45d8409
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 0
    selector:
      matchLabels:
        k8s-app: kube-dns
    strategy:
      rollingUpdate:
        maxSurge: 25%
        maxUnavailable: 1
      type: RollingUpdate
    template:
      metadata:
        annotations:
          kubectl.kubernetes.io/restartedAt: "2025-02-05T06:42:43Z"
        creationTimestamp: null
        labels:
          k8s-app: kube-dns
      spec:
        containers:
        - args:
          - -conf
          - /etc/coredns/Corefile
          image: rancher/mirrored-coredns-coredns:1.12.0
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /health
              port: 8080
              scheme: HTTP
            initialDelaySeconds: 60
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          name: coredns
          ports:
          - containerPort: 53
            name: dns
            protocol: UDP
          - containerPort: 53
            name: dns-tcp
            protocol: TCP
          - containerPort: 9153
            name: metrics
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /ready
              port: 8181
              scheme: HTTP
            periodSeconds: 2
            successThreshold: 1
            timeoutSeconds: 1
          resources:
            limits:
              memory: 170Mi
            requests:
              cpu: 100m
              memory: 70Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              add:
              - NET_BIND_SERVICE
              drop:
              - all
            readOnlyRootFilesystem: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/coredns
            name: config-volume
            readOnly: true
          - mountPath: /etc/coredns/custom
            name: custom-config-volume
            readOnly: true
        dnsPolicy: Default
        nodeSelector:
          kubernetes.io/os: linux
        priorityClassName: system-cluster-critical
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: coredns
        serviceAccountName: coredns
        terminationGracePeriodSeconds: 30
        tolerations:
        - key: CriticalAddonsOnly
          operator: Exists
        - effect: NoSchedule
          key: node-role.kubernetes.io/control-plane
          operator: Exists
        - effect: NoSchedule
          key: node-role.kubernetes.io/master
          operator: Exists
        topologySpreadConstraints:
        - labelSelector:
            matchLabels:
              k8s-app: kube-dns
          maxSkew: 1
          topologyKey: kubernetes.io/hostname
          whenUnsatisfiable: DoNotSchedule
        volumes:
        - configMap:
            defaultMode: 420
            items:
            - key: Corefile
              path: Corefile
            - key: NodeHosts
              path: NodeHosts
            name: coredns
          name: config-volume
        - configMap:
            defaultMode: 420
            name: coredns-custom
            optional: true
          name: custom-config-volume
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2025-02-05T04:54:34Z"
      lastUpdateTime: "2025-02-05T04:54:34Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    - lastTransitionTime: "2025-02-05T04:54:34Z"
      lastUpdateTime: "2025-02-05T06:42:45Z"
      message: ReplicaSet "coredns-f59c78b45" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    observedGeneration: 2
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "1"
      objectset.rio.cattle.io/applied: H4sIAAAAAAAA/6xUXWvrRhD9K2WeJdmuE2MEfTBJSksTxySkL8GU8Wpkb7zaXXbHaoTRf7+MJeeDGyf3wn3SfsycPTPnjPaAXv9LIWpnIQf0Pg7qESSw1baAHC7JG9dUZBkSqIixQEbI94DWOkbWzkbZutUTKY7EWdAuU8hsKNNuoAUEkpP37n9LIV3XW8hhO45vbupR8ts/2hZ/zIrC2S8hLFYEORin0KSRXcA1/VBS9Kgkc7tbURqbyFRBm4DBFZlPS9tg3EAOo+m4HJ+ryXlZrtR4ODmbDMfl2bgcnU+HxVRNpvh7gaviTEDfkfTIm9QHV2tpPgXo7k/wiZ6UsAnUxf+lpcjmWleaIR8mEMmQYhckqEJWm+uXCtD706+2As4BmdbN4QFnjLbrB18gUwf2/GCxRm1wZQjyUZsAN1443r2LlXOqvDnmvXGL+QkufaHKWUZtKUTIH2VbVSiWfDzdvsgYxKdpqpwt9RoSGBCrQbfrP9lTdBaWCZCtD8i9KIvby//ms5ur+8Xs4goSqNHs6M/gKiFTajLFHZUv6wWyiH+sMXtVrm3bZQK6Ev/lENCqDYXBx5zzepgNs/EQ+oTFzpiFM1o1kMPf5dzxIlDshu8r79TO7Cq6cTvLXccqWfY837bhFas7SLtMaJdC3AftgubmwmCM8y6uc2FqXUGpCpq1QiPtplBrRTOl5KX5Z/zSPjbFLhgSYGcoHH8gj3vYkhR90cMfhj7eWtPIEHuJFGvD1bOOHKFN9kBlSYohh7m7VxsqdkYGvoM5UA3OUCZjFCwxRZlZMVVwJvUGLf1S5AojH3T4AHJ5VOdoZWn7DXpx0/ey9t5tT8vUtu23AAAA///s6eu+uAUAAA
      objectset.rio.cattle.io/id: ""
      objectset.rio.cattle.io/owner-gvk: k3s.cattle.io/v1, Kind=Addon
      objectset.rio.cattle.io/owner-name: local-storage
      objectset.rio.cattle.io/owner-namespace: kube-system
    creationTimestamp: "2025-02-05T04:54:31Z"
    generation: 1
    labels:
      objectset.rio.cattle.io/hash: 183f35c65ffbc3064603f43f1580d8c68a2dabd4
    name: local-path-provisioner
    namespace: kube-system
    resourceVersion: "162570"
    uid: ac77729a-8823-4097-818a-621e5aa7f967
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 0
    selector:
      matchLabels:
        app: local-path-provisioner
    strategy:
      rollingUpdate:
        maxSurge: 25%
        maxUnavailable: 1
      type: RollingUpdate
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: local-path-provisioner
      spec:
        containers:
        - command:
          - local-path-provisioner
          - start
          - --config
          - /etc/config/config.json
          env:
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          image: rancher/local-path-provisioner:v0.0.30
          imagePullPolicy: IfNotPresent
          name: local-path-provisioner
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/config/
            name: config-volume
        dnsPolicy: ClusterFirst
        priorityClassName: system-node-critical
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: local-path-provisioner-service-account
        serviceAccountName: local-path-provisioner-service-account
        terminationGracePeriodSeconds: 30
        tolerations:
        - key: CriticalAddonsOnly
          operator: Exists
        - effect: NoSchedule
          key: node-role.kubernetes.io/control-plane
          operator: Exists
        - effect: NoSchedule
          key: node-role.kubernetes.io/master
          operator: Exists
        volumes:
        - configMap:
            defaultMode: 420
            name: local-path-config
          name: config-volume
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2025-02-05T04:54:34Z"
      lastUpdateTime: "2025-02-05T04:54:34Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    - lastTransitionTime: "2025-02-05T04:54:34Z"
      lastUpdateTime: "2025-02-05T04:54:44Z"
      message: ReplicaSet "local-path-provisioner-5cf85fd84d" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "1"
      objectset.rio.cattle.io/applied: H4sIAAAAAAAA/6xV4W8auRP9V36az7vJLoHQrsQHRPiV6tIUFXqnUxUh4x3Ah9f2eWZpuIj//TS7SUovpelV9wUt9vPzmzfjmXtQwfyKkYx3UIAKgc53OSSwNa6EAq4wWL+v0DEkUCGrUrGC4h6Uc54VG+9I/vrlH6iZkM+i8WdaMVs8M/7cCAkkJ/f9Z4cxXe+2UMD2go52dnnyv1+MKwfDsvTuRQqnKoRCJEajKSWMO4xpeSz/ZQIKSgvLtl5iSntirOCQgFVLtE2Y21eUqhCeXfQd9o2iDRSAeYadbg/zC+xeqn6np1+XZVl2V69x2bl4vcq7/X6/uyrlvm/GAu36CYkUUIvAiDsjuZwYYh/316YyDEWWAKFFzT4KqFKsN9cvB3UQYo6Kcb1vyL21xq0/hlIxtkR3H53aKWPV0iIU+SEB3gfR9+ErrKxjFezjuaNC+gFzT1pyFLj2jpVxGAmKT/eg4lo+IE01Rk5LEwfnXAVIIE0JdR0xDT7yIM86vaxZFUMtchoirjBGLFNVlhGJUomIBm8dY3TKvp0m47unz4knbrQdU9SEqfMlpsSKa2puagCt/DQieVvL2xnkPWp22FKqTdhgTKk2jDSYX88W49HVZCy/s+Hit7fzyWI4ni06vcvFm9G7xWwyvHjVTb7gPvwQ6h9seefVI67TuzzFdhJ1xDaaDEeTYSdbTN9f/55fZL1vkT0DwW0CplJryW5UTm8wnlcmRi8Z+DrdxS476591IAFrduiQaBr9simolTK2jjjfRKSNtyUUFwlsmMMbZNkPiuURnsvBvyCBJiNFgxD/SW+wqa/JfD6dSVkZZ9goe4VW7WeovSsJisssgYDR+PJpKZenVWuNREeX5wmwqdDX/AX4nXctatqyfariaSOwqc6nc49qQ/TstbdQwHw0hcNtAhFVaX7KETm5/3lLnjvS+ReGyEOoo0ZqW9efNRI33zrUUECeZVUzdiof91BAP3tn2qYkL9jwfuQd410Tj7LWf55GszMW1zgmrWwznaBYKUvYWvTe2f0H7/n/xuJD7yw41rJbuyHdeCe7X619JIySiCw7JLDztq7wna/dQ74q+Zw+WNn2l4dkcRWk68DhVvITovGNYKuIblpEK6BtFDoaNlpZMR7jzmgcai3cNydKhr3F+Dh+P93DFsWg0QNNMzJJopXBFAQpnR/Gd0YMPiT3gKsVakn4jZ/pDZa1lR7W0jSSord4Jh0tOmQkGWVSndHbNFjl8D9lrhRxO0WfU94++t5GilXg/ZWRQXb4ltuHw+HvAAAA//9PFN5y1QgAAA
      objectset.rio.cattle.io/id: ""
      objectset.rio.cattle.io/owner-gvk: k3s.cattle.io/v1, Kind=Addon
      objectset.rio.cattle.io/owner-name: metrics-server-deployment
      objectset.rio.cattle.io/owner-namespace: kube-system
    creationTimestamp: "2025-02-05T04:54:31Z"
    generation: 1
    labels:
      k8s-app: metrics-server
      objectset.rio.cattle.io/hash: e10e245e13e46a725c9dddd4f9eb239f147774fd
    name: metrics-server
    namespace: kube-system
    resourceVersion: "196252"
    uid: 1fcca16f-9222-4390-8ab2-a13cd57356c9
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 0
    selector:
      matchLabels:
        k8s-app: metrics-server
    strategy:
      rollingUpdate:
        maxSurge: 25%
        maxUnavailable: 1
      type: RollingUpdate
    template:
      metadata:
        creationTimestamp: null
        labels:
          k8s-app: metrics-server
        name: metrics-server
      spec:
        containers:
        - args:
          - --cert-dir=/tmp
          - --secure-port=10250
          - --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname
          - --kubelet-use-node-status-port
          - --metric-resolution=15s
          - --tls-cipher-suites=TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305
          image: rancher/mirrored-metrics-server:v0.7.2
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /livez
              port: https
              scheme: HTTPS
            initialDelaySeconds: 60
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          name: metrics-server
          ports:
          - containerPort: 10250
            name: https
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /readyz
              port: https
              scheme: HTTPS
            periodSeconds: 2
            successThreshold: 1
            timeoutSeconds: 1
          resources:
            requests:
              cpu: 100m
              memory: 70Mi
          securityContext:
            allowPrivilegeEscalation: false
            readOnlyRootFilesystem: true
            runAsNonRoot: true
            runAsUser: 1000
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /tmp
            name: tmp-dir
        dnsPolicy: ClusterFirst
        priorityClassName: system-node-critical
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: metrics-server
        serviceAccountName: metrics-server
        terminationGracePeriodSeconds: 30
        tolerations:
        - key: CriticalAddonsOnly
          operator: Exists
        - effect: NoSchedule
          key: node-role.kubernetes.io/control-plane
          operator: Exists
        - effect: NoSchedule
          key: node-role.kubernetes.io/master
          operator: Exists
        volumes:
        - emptyDir: {}
          name: tmp-dir
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2025-02-05T04:54:34Z"
      lastUpdateTime: "2025-02-05T04:54:34Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    - lastTransitionTime: "2025-02-05T04:54:34Z"
      lastUpdateTime: "2025-02-05T04:55:01Z"
      message: ReplicaSet "metrics-server-5985cbc9d7" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "1"
      field.cattle.io/publicEndpoints: '[{"addresses":["172.31.17.126"],"port":80,"protocol":"TCP","serviceName":"kube-system:traefik","allNodes":false},{"addresses":["172.31.17.126"],"port":443,"protocol":"TCP","serviceName":"kube-system:traefik","allNodes":false}]'
      meta.helm.sh/release-name: traefik
      meta.helm.sh/release-namespace: kube-system
    creationTimestamp: "2025-02-05T04:54:52Z"
    generation: 2
    labels:
      app.kubernetes.io/instance: traefik-kube-system
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: traefik
      helm.sh/chart: traefik-27.0.201_up27.0.2
    name: traefik
    namespace: kube-system
    resourceVersion: "196191"
    uid: 5da96425-00d5-45f4-b331-92d96315e0a7
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app.kubernetes.io/instance: traefik-kube-system
        app.kubernetes.io/name: traefik
    strategy:
      rollingUpdate:
        maxSurge: 1
        maxUnavailable: 0
      type: RollingUpdate
    template:
      metadata:
        annotations:
          prometheus.io/path: /metrics
          prometheus.io/port: "9100"
          prometheus.io/scrape: "true"
        creationTimestamp: null
        labels:
          app.kubernetes.io/instance: traefik-kube-system
          app.kubernetes.io/managed-by: Helm
          app.kubernetes.io/name: traefik
          helm.sh/chart: traefik-27.0.201_up27.0.2
      spec:
        containers:
        - args:
          - --global.checknewversion
          - --global.sendanonymoususage
          - --entrypoints.metrics.address=:9100/tcp
          - --entrypoints.traefik.address=:9000/tcp
          - --entrypoints.web.address=:8000/tcp
          - --entrypoints.websecure.address=:8443/tcp
          - --api.dashboard=true
          - --ping=true
          - --metrics.prometheus=true
          - --metrics.prometheus.entrypoint=metrics
          - --providers.kubernetescrd
          - --providers.kubernetesingress
          - --providers.kubernetesingress.ingressendpoint.publishedservice=kube-system/traefik
          - --entrypoints.websecure.http.tls=true
          env:
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          image: rancher/mirrored-library-traefik:2.11.18
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /ping
              port: 9000
              scheme: HTTP
            initialDelaySeconds: 2
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 2
          name: traefik
          ports:
          - containerPort: 9100
            name: metrics
            protocol: TCP
          - containerPort: 9000
            name: traefik
            protocol: TCP
          - containerPort: 8000
            name: web
            protocol: TCP
          - containerPort: 8443
            name: websecure
            protocol: TCP
          readinessProbe:
            failureThreshold: 1
            httpGet:
              path: /ping
              port: 9000
              scheme: HTTP
            initialDelaySeconds: 2
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 2
          resources: {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /data
            name: data
          - mountPath: /tmp
            name: tmp
        dnsPolicy: ClusterFirst
        priorityClassName: system-cluster-critical
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroupChangePolicy: OnRootMismatch
          runAsGroup: 65532
          runAsNonRoot: true
          runAsUser: 65532
        serviceAccount: traefik
        serviceAccountName: traefik
        terminationGracePeriodSeconds: 60
        tolerations:
        - key: CriticalAddonsOnly
          operator: Exists
        - effect: NoSchedule
          key: node-role.kubernetes.io/control-plane
          operator: Exists
        - effect: NoSchedule
          key: node-role.kubernetes.io/master
          operator: Exists
        volumes:
        - emptyDir: {}
          name: data
        - emptyDir: {}
          name: tmp
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2025-02-05T04:54:52Z"
      lastUpdateTime: "2025-02-05T04:54:58Z"
      message: ReplicaSet "traefik-5d45fc8cc9" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    - lastTransitionTime: "2025-03-03T02:55:30Z"
      lastUpdateTime: "2025-03-03T02:55:30Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    observedGeneration: 2
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "1"
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"apps/v1","kind":"Deployment","metadata":{"annotations":{},"name":"flask-lstm","namespace":"monitoring"},"spec":{"replicas":1,"selector":{"matchLabels":{"app":"flask-lstm"}},"template":{"metadata":{"labels":{"app":"flask-lstm"}},"spec":{"containers":[{"image":"gnh374/lstm-model","name":"flask-lstm","ports":[{"containerPort":5000}]}]}}}}
    creationTimestamp: "2025-03-02T08:12:05Z"
    generation: 1
    name: flask-lstm
    namespace: monitoring
    resourceVersion: "172692"
    uid: a69f9991-0dc9-4231-8979-080af48ca49f
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app: flask-lstm
    strategy:
      rollingUpdate:
        maxSurge: 25%
        maxUnavailable: 25%
      type: RollingUpdate
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: flask-lstm
      spec:
        containers:
        - image: gnh374/lstm-model
          imagePullPolicy: Always
          name: flask-lstm
          ports:
          - containerPort: 5000
            protocol: TCP
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2025-03-02T08:12:07Z"
      lastUpdateTime: "2025-03-02T08:12:07Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    - lastTransitionTime: "2025-03-02T08:12:05Z"
      lastUpdateTime: "2025-03-02T08:12:07Z"
      message: ReplicaSet "flask-lstm-6f8bc77f4f" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "28"
      meta.helm.sh/release-name: prometheus
      meta.helm.sh/release-namespace: monitoring
    creationTimestamp: "2025-02-26T10:33:04Z"
    generation: 1
    labels:
      app.kubernetes.io/component: metrics
      app.kubernetes.io/instance: prometheus
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: kube-state-metrics
      app.kubernetes.io/part-of: kube-state-metrics
      app.kubernetes.io/version: 2.14.0
      helm.sh/chart: kube-state-metrics-5.28.1
      velero.io/backup-name: monitoring-backup
      velero.io/restore-name: monitoring-backup-20250226103249
    name: prometheus-kube-state-metrics
    namespace: monitoring
    resourceVersion: "196216"
    uid: b10b918b-2a9c-43b7-ab4d-a96a9c8326d4
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app.kubernetes.io/instance: prometheus
        app.kubernetes.io/name: kube-state-metrics
    strategy:
      rollingUpdate:
        maxSurge: 25%
        maxUnavailable: 25%
      type: RollingUpdate
    template:
      metadata:
        annotations:
          kubectl.kubernetes.io/restartedAt: "2025-02-13T12:06:57Z"
        creationTimestamp: null
        labels:
          app.kubernetes.io/component: metrics
          app.kubernetes.io/instance: prometheus
          app.kubernetes.io/managed-by: Helm
          app.kubernetes.io/name: kube-state-metrics
          app.kubernetes.io/part-of: kube-state-metrics
          app.kubernetes.io/version: 2.14.0
          helm.sh/chart: kube-state-metrics-5.28.1
      spec:
        automountServiceAccountToken: true
        containers:
        - args:
          - --port=8080
          - --resources=certificatesigningrequests,configmaps,cronjobs,daemonsets,deployments,endpoints,horizontalpodautoscalers,ingresses,jobs,leases,limitranges,mutatingwebhookconfigurations,namespaces,networkpolicies,nodes,persistentvolumeclaims,persistentvolumes,poddisruptionbudgets,pods,replicasets,replicationcontrollers,resourcequotas,secrets,services,statefulsets,storageclasses,validatingwebhookconfigurations,volumeattachments
          image: registry.k8s.io/kube-state-metrics/kube-state-metrics:v2.14.0
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /livez
              port: 8080
              scheme: HTTP
            initialDelaySeconds: 5
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          name: kube-state-metrics
          ports:
          - containerPort: 8080
            name: http
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /readyz
              port: 8081
              scheme: HTTP
            initialDelaySeconds: 5
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          resources: {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 65534
          runAsGroup: 65534
          runAsNonRoot: true
          runAsUser: 65534
          seccompProfile:
            type: RuntimeDefault
        serviceAccount: prometheus-kube-state-metrics
        serviceAccountName: prometheus-kube-state-metrics
        terminationGracePeriodSeconds: 30
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2025-02-26T10:33:06Z"
      lastUpdateTime: "2025-02-26T10:33:12Z"
      message: ReplicaSet "prometheus-kube-state-metrics-75b5b65f7f" has successfully
        progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    - lastTransitionTime: "2025-03-03T02:55:33Z"
      lastUpdateTime: "2025-03-03T02:55:33Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "28"
      meta.helm.sh/release-name: prometheus
      meta.helm.sh/release-namespace: monitoring
    creationTimestamp: "2025-02-26T10:33:05Z"
    generation: 1
    labels:
      app.kubernetes.io/instance: prometheus
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: prometheus-pushgateway
      app.kubernetes.io/version: v1.11.0
      helm.sh/chart: prometheus-pushgateway-2.17.0
      velero.io/backup-name: monitoring-backup
      velero.io/restore-name: monitoring-backup-20250226103249
    name: prometheus-prometheus-pushgateway
    namespace: monitoring
    resourceVersion: "196240"
    uid: d0c6bed6-b65d-4d9b-bac8-04261c7bb2cd
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app.kubernetes.io/instance: prometheus
        app.kubernetes.io/name: prometheus-pushgateway
    strategy:
      type: Recreate
    template:
      metadata:
        annotations:
          kubectl.kubernetes.io/restartedAt: "2025-02-13T12:06:57Z"
        creationTimestamp: null
        labels:
          app.kubernetes.io/instance: prometheus
          app.kubernetes.io/managed-by: Helm
          app.kubernetes.io/name: prometheus-pushgateway
          app.kubernetes.io/version: v1.11.0
          helm.sh/chart: prometheus-pushgateway-2.17.0
      spec:
        automountServiceAccountToken: true
        containers:
        - image: quay.io/prometheus/pushgateway:v1.11.0
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /-/healthy
              port: 9091
              scheme: HTTP
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 10
          name: pushgateway
          ports:
          - containerPort: 9091
            name: metrics
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /-/ready
              port: 9091
              scheme: HTTP
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 10
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /data
            name: storage-volume
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 65534
          runAsNonRoot: true
          runAsUser: 65534
        serviceAccount: prometheus-prometheus-pushgateway
        serviceAccountName: prometheus-prometheus-pushgateway
        terminationGracePeriodSeconds: 30
        volumes:
        - emptyDir: {}
          name: storage-volume
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2025-02-26T10:33:06Z"
      lastUpdateTime: "2025-02-26T10:33:12Z"
      message: ReplicaSet "prometheus-prometheus-pushgateway-d5964d9df" has successfully
        progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    - lastTransitionTime: "2025-03-03T02:55:35Z"
      lastUpdateTime: "2025-03-03T02:55:35Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "29"
      field.cattle.io/publicEndpoints: "null"
      meta.helm.sh/release-name: prometheus
      meta.helm.sh/release-namespace: monitoring
    creationTimestamp: "2025-02-26T10:33:05Z"
    generation: 1
    labels:
      app.kubernetes.io/component: server
      app.kubernetes.io/instance: prometheus
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: prometheus
      app.kubernetes.io/part-of: prometheus
      app.kubernetes.io/version: v3.1.0
      helm.sh/chart: prometheus-27.3.0
      velero.io/backup-name: monitoring-backup
      velero.io/restore-name: monitoring-backup-20250226103249
    name: prometheus-server
    namespace: monitoring
    resourceVersion: "196331"
    uid: 9811a1b1-4c9b-43d9-82a8-2f566b280ff0
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app.kubernetes.io/component: server
        app.kubernetes.io/instance: prometheus
        app.kubernetes.io/name: prometheus
    strategy:
      type: Recreate
    template:
      metadata:
        annotations:
          kubectl.kubernetes.io/restartedAt: "2025-02-13T12:06:57Z"
        creationTimestamp: null
        labels:
          app.kubernetes.io/component: server
          app.kubernetes.io/instance: prometheus
          app.kubernetes.io/name: prometheus
          app.kubernetes.io/part-of: prometheus
          app.kubernetes.io/version: v3.1.0
          helm.sh/chart: prometheus-27.3.0
      spec:
        containers:
        - args:
          - --watched-dir=/etc/config
          - --listen-address=0.0.0.0:8080
          - --reload-url=http://127.0.0.1:9090/-/reload
          image: quay.io/prometheus-operator/prometheus-config-reloader:v0.79.2
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /healthz
              port: metrics
              scheme: HTTP
            initialDelaySeconds: 2
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          name: prometheus-server-configmap-reload
          ports:
          - containerPort: 8080
            name: metrics
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /healthz
              port: metrics
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/config
            name: config-volume
            readOnly: true
        - args:
          - --storage.tsdb.retention.time=15d
          - --config.file=/etc/config/prometheus.yml
          - --storage.tsdb.path=/data
          - --web.console.libraries=/etc/prometheus/console_libraries
          - --web.console.templates=/etc/prometheus/consoles
          - --web.enable-lifecycle
          image: quay.io/prometheus/prometheus:v3.1.0
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /-/healthy
              port: 9090
              scheme: HTTP
            initialDelaySeconds: 30
            periodSeconds: 15
            successThreshold: 1
            timeoutSeconds: 10
          name: prometheus-server
          ports:
          - containerPort: 9090
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /-/ready
              port: 9090
              scheme: HTTP
            initialDelaySeconds: 30
            periodSeconds: 5
            successThreshold: 1
            timeoutSeconds: 4
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/config
            name: config-volume
          - mountPath: /data
            name: storage-volume
        dnsPolicy: ClusterFirst
        enableServiceLinks: true
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 65534
          runAsGroup: 65534
          runAsNonRoot: true
          runAsUser: 65534
        serviceAccount: prometheus-server
        serviceAccountName: prometheus-server
        terminationGracePeriodSeconds: 300
        volumes:
        - configMap:
            defaultMode: 420
            name: prometheus-server
          name: config-volume
        - name: storage-volume
          persistentVolumeClaim:
            claimName: prometheus-server
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2025-02-26T10:33:06Z"
      lastUpdateTime: "2025-02-26T10:33:55Z"
      message: ReplicaSet "prometheus-server-d7d4585c5" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    - lastTransitionTime: "2025-03-03T02:55:38Z"
      lastUpdateTime: "2025-03-03T02:55:38Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "18"
      field.cattle.io/publicEndpoints: '[{"port":30089,"protocol":"TCP","serviceName":"monitoring:velero-runner","allNodes":true}]'
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"apps/v1","kind":"Deployment","metadata":{"annotations":{},"name":"velero-runner","namespace":"monitoring"},"spec":{"replicas":1,"selector":{"matchLabels":{"app":"velero-runner"}},"template":{"metadata":{"labels":{"app":"velero-runner"}},"spec":{"containers":[{"command":["python","/app/webhook.py"],"image":"python:3.9","name":"velero-runner","volumeMounts":[{"mountPath":"/app","name":"app"}]}],"volumes":[{"configMap":{"name":"webhook-config"},"name":"app"}]}}}}
    creationTimestamp: "2025-02-26T10:33:05Z"
    generation: 1
    labels:
      velero.io/backup-name: monitoring-backup
      velero.io/restore-name: monitoring-backup-20250226103249
    name: velero-runner
    namespace: monitoring
    resourceVersion: "197323"
    uid: 70edf5cb-849c-44d2-b360-47a942d14fa9
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app: velero-runner
    strategy:
      rollingUpdate:
        maxSurge: 25%
        maxUnavailable: 25%
      type: RollingUpdate
    template:
      metadata:
        annotations:
          kubectl.kubernetes.io/restartedAt: "2025-02-13T12:06:57Z"
        creationTimestamp: null
        labels:
          app: velero-runner
      spec:
        containers:
        - command:
          - python
          - /app/webhook.py
          image: python:3.9
          imagePullPolicy: IfNotPresent
          name: velero-runner
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /app
            name: app
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
        volumes:
        - configMap:
            defaultMode: 420
            name: webhook-config
          name: app
  status:
    conditions:
    - lastTransitionTime: "2025-02-26T10:33:07Z"
      lastUpdateTime: "2025-02-26T10:33:43Z"
      message: ReplicaSet "velero-runner-54974bb85d" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    - lastTransitionTime: "2025-03-03T03:01:06Z"
      lastUpdateTime: "2025-03-03T03:01:06Z"
      message: Deployment does not have minimum availability.
      reason: MinimumReplicasUnavailable
      status: "False"
      type: Available
    observedGeneration: 1
    replicas: 1
    unavailableReplicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "32"
      field.cattle.io/publicEndpoints: '[{"port":30080,"protocol":"TCP","serviceName":"monitoring:webhook-service","allNodes":true}]'
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"apps/v1","kind":"Deployment","metadata":{"annotations":{},"name":"webhook-deployment","namespace":"monitoring"},"spec":{"replicas":1,"selector":{"matchLabels":{"app":"webhook"}},"template":{"metadata":{"labels":{"app":"webhook"}},"spec":{"containers":[{"image":"gnh374/webhook-express:latest","name":"webhook","ports":[{"containerPort":3000}]}],"serviceAccountName":"webhook-sa"}}}}
    creationTimestamp: "2025-02-26T10:33:06Z"
    generation: 1
    labels:
      velero.io/backup-name: monitoring-backup
      velero.io/restore-name: monitoring-backup-20250226103249
    name: webhook-deployment
    namespace: monitoring
    resourceVersion: "196210"
    uid: 1ef53948-3f7e-4489-8902-5b431f11b6a1
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app: webhook
    strategy:
      rollingUpdate:
        maxSurge: 25%
        maxUnavailable: 25%
      type: RollingUpdate
    template:
      metadata:
        annotations:
          kubectl.kubernetes.io/restartedAt: "2025-02-13T12:06:57Z"
        creationTimestamp: null
        labels:
          app: webhook
      spec:
        containers:
        - image: gnh374/webhook-express:latest
          imagePullPolicy: Always
          name: webhook
          ports:
          - containerPort: 3000
            protocol: TCP
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: webhook-sa
        serviceAccountName: webhook-sa
        terminationGracePeriodSeconds: 30
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2025-02-26T10:33:08Z"
      lastUpdateTime: "2025-02-26T10:33:46Z"
      message: ReplicaSet "webhook-deployment-64d4fb9bcc" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    - lastTransitionTime: "2025-03-03T02:55:33Z"
      lastUpdateTime: "2025-03-03T02:55:33Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "1"
    creationTimestamp: "2025-02-06T14:32:37Z"
    generation: 1
    labels:
      component: velero
    name: velero
    namespace: velero
    resourceVersion: "196326"
    uid: 958dac77-e6fe-4e0c-a880-b2980d992d7e
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        deploy: velero
    strategy:
      rollingUpdate:
        maxSurge: 25%
        maxUnavailable: 25%
      type: RollingUpdate
    template:
      metadata:
        annotations:
          prometheus.io/path: /metrics
          prometheus.io/port: "8085"
          prometheus.io/scrape: "true"
        creationTimestamp: null
        labels:
          component: velero
          deploy: velero
      spec:
        containers:
        - args:
          - server
          - --features=
          - --uploader-type=kopia
          command:
          - /velero
          env:
          - name: VELERO_SCRATCH_DIR
            value: /scratch
          - name: VELERO_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          - name: LD_LIBRARY_PATH
            value: /plugins
          image: velero/velero:v1.15.2
          imagePullPolicy: IfNotPresent
          name: velero
          ports:
          - containerPort: 8085
            name: metrics
            protocol: TCP
          resources:
            limits:
              cpu: "1"
              memory: 512Mi
            requests:
              cpu: 500m
              memory: 128Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /plugins
            name: plugins
          - mountPath: /scratch
            name: scratch
        dnsPolicy: ClusterFirst
        initContainers:
        - image: velero/velero-plugin-for-aws:v1.10.0
          imagePullPolicy: IfNotPresent
          name: velero-velero-plugin-for-aws
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /target
            name: plugins
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: velero
        serviceAccountName: velero
        terminationGracePeriodSeconds: 30
        volumes:
        - emptyDir: {}
          name: plugins
        - emptyDir: {}
          name: scratch
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2025-02-06T14:32:37Z"
      lastUpdateTime: "2025-02-06T14:32:46Z"
      message: ReplicaSet "velero-57fdbd48db" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    - lastTransitionTime: "2025-03-03T02:55:38Z"
      lastUpdateTime: "2025-03-03T02:55:38Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "1"
      meta.helm.sh/release-name: fleet
      meta.helm.sh/release-namespace: cattle-fleet-system
    creationTimestamp: "2025-02-05T05:33:55Z"
    generation: 1
    labels:
      app: fleet-controller
      fleet.cattle.io/shard-default: "true"
      fleet.cattle.io/shard-id: ""
      pod-template-hash: 86bfd4bff
    name: fleet-controller-86bfd4bff
    namespace: cattle-fleet-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: fleet-controller
      uid: 104d03cc-34b5-4d7c-ac7e-8bcf237b9a01
    resourceVersion: "162462"
    uid: b54bd895-59b7-4c0e-805e-fe80cc675e33
  spec:
    replicas: 1
    selector:
      matchLabels:
        app: fleet-controller
        pod-template-hash: 86bfd4bff
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: fleet-controller
          fleet.cattle.io/shard-default: "true"
          fleet.cattle.io/shard-id: ""
          pod-template-hash: 86bfd4bff
      spec:
        containers:
        - command:
          - fleetcontroller
          env:
          - name: NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          - name: CATTLE_ELECTION_LEASE_DURATION
            value: 30s
          - name: CATTLE_ELECTION_RETRY_PERIOD
            value: 10s
          - name: CATTLE_ELECTION_RENEW_DEADLINE
            value: 25s
          - name: BUNDLE_RECONCILER_WORKERS
            value: "50"
          - name: BUNDLEDEPLOYMENT_RECONCILER_WORKERS
            value: "50"
          - name: CLUSTER_RECONCILER_WORKERS
            value: "50"
          - name: CLUSTERGROUP_RECONCILER_WORKERS
            value: "50"
          - name: IMAGESCAN_RECONCILER_WORKERS
            value: "50"
          image: rancher/fleet:v0.11.3
          imagePullPolicy: IfNotPresent
          name: fleet-controller
          ports:
          - containerPort: 8080
            name: metrics
            protocol: TCP
          resources: {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            privileged: false
            readOnlyRootFilesystem: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /tmp
            name: tmp
        - command:
          - fleetcontroller
          - cleanup
          env:
          - name: NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          - name: CATTLE_ELECTION_LEASE_DURATION
            value: 30s
          - name: CATTLE_ELECTION_RETRY_PERIOD
            value: 10s
          - name: CATTLE_ELECTION_RENEW_DEADLINE
            value: 25s
          image: rancher/fleet:v0.11.3
          imagePullPolicy: IfNotPresent
          name: fleet-cleanup
          resources: {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            privileged: false
            readOnlyRootFilesystem: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        - command:
          - fleetcontroller
          - agentmanagement
          - --disable-bootstrap
          env:
          - name: NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          - name: FLEET_PROPAGATE_DEBUG_SETTINGS_TO_AGENTS
            value: "true"
          - name: FLEET_DEBUG_DISABLE_SECURITY_CONTEXT
            value: "false"
          - name: CATTLE_ELECTION_LEASE_DURATION
            value: 30s
          - name: CATTLE_ELECTION_RETRY_PERIOD
            value: 10s
          - name: CATTLE_ELECTION_RENEW_DEADLINE
            value: 25s
          image: rancher/fleet:v0.11.3
          imagePullPolicy: IfNotPresent
          name: fleet-agentmanagement
          resources: {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            privileged: false
            readOnlyRootFilesystem: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        nodeSelector:
          kubernetes.io/os: linux
        priorityClassName: rancher-critical
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          runAsGroup: 1000
          runAsNonRoot: true
          runAsUser: 1000
        serviceAccount: fleet-controller
        serviceAccountName: fleet-controller
        terminationGracePeriodSeconds: 30
        tolerations:
        - effect: NoSchedule
          key: cattle.io/os
          operator: Equal
          value: linux
        volumes:
        - emptyDir: {}
          name: tmp
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "1"
      meta.helm.sh/release-name: fleet
      meta.helm.sh/release-namespace: cattle-fleet-system
    creationTimestamp: "2025-02-05T05:33:55Z"
    generation: 1
    labels:
      app: gitjob
      fleet.cattle.io/shard-default: "true"
      fleet.cattle.io/shard-id: ""
      pod-template-hash: 547d87f97f
    name: gitjob-547d87f97f
    namespace: cattle-fleet-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: gitjob
      uid: c903e399-d293-4097-9bbb-6e8f1710ef8a
    resourceVersion: "196269"
    uid: 62877784-4e30-4fbc-91cd-0b615a6c601e
  spec:
    replicas: 1
    selector:
      matchLabels:
        app: gitjob
        pod-template-hash: 547d87f97f
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: gitjob
          fleet.cattle.io/shard-default: "true"
          fleet.cattle.io/shard-id: ""
          pod-template-hash: 547d87f97f
      spec:
        containers:
        - args:
          - fleetcontroller
          - gitjob
          - --gitjob-image
          - rancher/fleet:v0.11.3
          env:
          - name: NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          - name: CATTLE_ELECTION_LEASE_DURATION
            value: 30s
          - name: CATTLE_ELECTION_RETRY_PERIOD
            value: 10s
          - name: CATTLE_ELECTION_RENEW_DEADLINE
            value: 25s
          - name: GITREPO_SYNC_PERIOD
            value: 2h
          - name: GITREPO_RECONCILER_WORKERS
            value: "50"
          image: rancher/fleet:v0.11.3
          imagePullPolicy: IfNotPresent
          name: gitjob
          ports:
          - containerPort: 8081
            name: metrics
            protocol: TCP
          resources: {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            privileged: false
            readOnlyRootFilesystem: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /tmp
            name: tmp
        dnsPolicy: ClusterFirst
        nodeSelector:
          kubernetes.io/os: linux
        priorityClassName: rancher-critical
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          runAsGroup: 1000
          runAsNonRoot: true
          runAsUser: 1000
        serviceAccount: gitjob
        serviceAccountName: gitjob
        terminationGracePeriodSeconds: 30
        tolerations:
        - effect: NoSchedule
          key: cattle.io/os
          operator: Equal
          value: linux
        volumes:
        - emptyDir: {}
          name: tmp
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "1"
      meta.helm.sh/release-name: rancher-provisioning-capi
      meta.helm.sh/release-namespace: cattle-provisioning-capi-system
    creationTimestamp: "2025-02-05T05:35:00Z"
    generation: 1
    labels:
      cluster.x-k8s.io/provider: cluster-api
      control-plane: controller-manager
      pod-template-hash: cc78bcdb8
    name: capi-controller-manager-cc78bcdb8
    namespace: cattle-provisioning-capi-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: capi-controller-manager
      uid: 552ce176-6e04-4294-a371-a3dc3d60d4ec
    resourceVersion: "162513"
    uid: 96880f01-e8bc-4f80-adb8-ca5139f82d39
  spec:
    replicas: 1
    selector:
      matchLabels:
        cluster.x-k8s.io/provider: cluster-api
        control-plane: controller-manager
        pod-template-hash: cc78bcdb8
    template:
      metadata:
        creationTimestamp: null
        labels:
          cluster.x-k8s.io/provider: cluster-api
          control-plane: controller-manager
          pod-template-hash: cc78bcdb8
      spec:
        containers:
        - args:
          - --leader-elect
          - --metrics-bind-addr=localhost:8080
          - --feature-gates=MachinePool=false,ClusterResourceSet=false,ClusterTopology=false,RuntimeSDK=false
          command:
          - /manager
          env:
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: POD_UID
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.uid
          image: rancher/mirrored-cluster-api-controller:v1.8.3
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /healthz
              port: healthz
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          name: manager
          ports:
          - containerPort: 9443
            name: webhook-server
            protocol: TCP
          - containerPort: 9440
            name: healthz
            protocol: TCP
          - containerPort: 8443
            name: metrics
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /readyz
              port: healthz
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          resources: {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            privileged: false
            runAsGroup: 65532
            runAsUser: 65532
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: FallbackToLogsOnError
          volumeMounts:
          - mountPath: /tmp/k8s-webhook-server/serving-certs
            name: cert
            readOnly: true
        dnsPolicy: ClusterFirst
        nodeSelector:
          kubernetes.io/os: linux
        priorityClassName: rancher-critical
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          runAsNonRoot: true
          seccompProfile:
            type: RuntimeDefault
        serviceAccount: capi-manager
        serviceAccountName: capi-manager
        terminationGracePeriodSeconds: 10
        tolerations:
        - effect: NoSchedule
          key: cattle.io/os
          operator: Equal
          value: linux
        - effect: NoSchedule
          key: node-role.kubernetes.io/controlplane
          value: "true"
        - effect: NoSchedule
          key: node-role.kubernetes.io/control-plane
          operator: Exists
        - effect: NoSchedule
          key: node-role.kubernetes.io/master
          operator: Exists
        - effect: NoExecute
          key: node-role.kubernetes.io/etcd
          operator: Exists
        volumes:
        - name: cert
          secret:
            defaultMode: 420
            secretName: capi-webhook-service-cert
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "3"
      deployment.kubernetes.io/max-replicas: "4"
      deployment.kubernetes.io/revision: "1"
      field.cattle.io/publicEndpoints: '[{"addresses":["172.31.17.126"],"port":443,"protocol":"HTTPS","serviceName":"cattle-system:rancher","ingressName":"cattle-system:rancher","hostname":"44.220.0.65.sslip.io","path":"/","allNodes":false}]'
      meta.helm.sh/release-name: rancher
      meta.helm.sh/release-namespace: cattle-system
    creationTimestamp: "2025-02-05T05:31:08Z"
    generation: 4
    labels:
      app: rancher
      pod-template-hash: 5b9874696b
      release: rancher
    name: rancher-5b9874696b
    namespace: cattle-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: rancher
      uid: 356f8ee7-4b23-4dce-b5aa-049d29d0a6b8
    resourceVersion: "14088"
    uid: 30998f54-5ffc-4320-a7c2-5ac00d8a6c9a
  spec:
    replicas: 0
    selector:
      matchLabels:
        app: rancher
        pod-template-hash: 5b9874696b
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: rancher
          pod-template-hash: 5b9874696b
          release: rancher
      spec:
        affinity:
          nodeAffinity:
            requiredDuringSchedulingIgnoredDuringExecution:
              nodeSelectorTerms:
              - matchExpressions:
                - key: kubernetes.io/os
                  operator: NotIn
                  values:
                  - windows
          podAntiAffinity:
            preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchExpressions:
                  - key: app
                    operator: In
                    values:
                    - rancher
                topologyKey: kubernetes.io/hostname
              weight: 100
        containers:
        - args:
          - --no-cacerts
          - --http-listen-port=80
          - --https-listen-port=443
          - --add-local=true
          env:
          - name: CATTLE_NAMESPACE
            value: cattle-system
          - name: CATTLE_PEER_SERVICE
            value: rancher
          image: rancher/rancher:v2.10.2
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 5
            httpGet:
              path: /healthz
              port: 80
              scheme: HTTP
            periodSeconds: 30
            successThreshold: 1
            timeoutSeconds: 5
          name: rancher
          ports:
          - containerPort: 80
            protocol: TCP
          readinessProbe:
            failureThreshold: 5
            httpGet:
              path: /healthz
              port: 80
              scheme: HTTP
            periodSeconds: 30
            successThreshold: 1
            timeoutSeconds: 5
          resources: {}
          startupProbe:
            failureThreshold: 12
            httpGet:
              path: /healthz
              port: 80
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        priorityClassName: rancher-critical
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: rancher
        serviceAccountName: rancher
        terminationGracePeriodSeconds: 30
        tolerations:
        - effect: NoSchedule
          key: cattle.io/os
          operator: Equal
          value: linux
  status:
    observedGeneration: 4
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "3"
      deployment.kubernetes.io/max-replicas: "4"
      deployment.kubernetes.io/revision: "2"
      field.cattle.io/publicEndpoints: '[{"port":32493,"protocol":"TCP","serviceName":"cattle-system:rancher","allNodes":true},{"port":30199,"protocol":"TCP","serviceName":"cattle-system:rancher","allNodes":true},{"addresses":["172.31.17.126"],"port":443,"protocol":"HTTPS","serviceName":"cattle-system:rancher","ingressName":"cattle-system:rancher","hostname":"44.220.0.65.sslip.io","path":"/","allNodes":false}]'
      meta.helm.sh/release-name: rancher
      meta.helm.sh/release-namespace: cattle-system
    creationTimestamp: "2025-02-05T06:43:35Z"
    generation: 3
    labels:
      app: rancher
      pod-template-hash: c6b4fd756
      release: rancher
    name: rancher-c6b4fd756
    namespace: cattle-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: rancher
      uid: 356f8ee7-4b23-4dce-b5aa-049d29d0a6b8
    resourceVersion: "196590"
    uid: 898e6c1f-00b4-4dcc-aad6-e05639c4eb7c
  spec:
    replicas: 3
    selector:
      matchLabels:
        app: rancher
        pod-template-hash: c6b4fd756
    template:
      metadata:
        annotations:
          kubectl.kubernetes.io/restartedAt: "2025-02-05T06:43:35Z"
        creationTimestamp: null
        labels:
          app: rancher
          pod-template-hash: c6b4fd756
          release: rancher
      spec:
        affinity:
          nodeAffinity:
            requiredDuringSchedulingIgnoredDuringExecution:
              nodeSelectorTerms:
              - matchExpressions:
                - key: kubernetes.io/os
                  operator: NotIn
                  values:
                  - windows
          podAntiAffinity:
            preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchExpressions:
                  - key: app
                    operator: In
                    values:
                    - rancher
                topologyKey: kubernetes.io/hostname
              weight: 100
        containers:
        - args:
          - --no-cacerts
          - --http-listen-port=80
          - --https-listen-port=443
          - --add-local=true
          env:
          - name: CATTLE_NAMESPACE
            value: cattle-system
          - name: CATTLE_PEER_SERVICE
            value: rancher
          image: rancher/rancher:v2.10.2
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 5
            httpGet:
              path: /healthz
              port: 80
              scheme: HTTP
            periodSeconds: 30
            successThreshold: 1
            timeoutSeconds: 5
          name: rancher
          ports:
          - containerPort: 80
            protocol: TCP
          readinessProbe:
            failureThreshold: 5
            httpGet:
              path: /healthz
              port: 80
              scheme: HTTP
            periodSeconds: 30
            successThreshold: 1
            timeoutSeconds: 5
          resources: {}
          startupProbe:
            failureThreshold: 12
            httpGet:
              path: /healthz
              port: 80
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        priorityClassName: rancher-critical
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: rancher
        serviceAccountName: rancher
        terminationGracePeriodSeconds: 30
        tolerations:
        - effect: NoSchedule
          key: cattle.io/os
          operator: Equal
          value: linux
  status:
    availableReplicas: 3
    fullyLabeledReplicas: 3
    observedGeneration: 3
    readyReplicas: 3
    replicas: 3
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "1"
      field.cattle.io/publicEndpoints: '[{"port":30449,"protocol":"TCP","serviceName":"cattle-system:rancher-webhook","allNodes":true}]'
      meta.helm.sh/release-name: rancher-webhook
      meta.helm.sh/release-namespace: cattle-system
    creationTimestamp: "2025-02-05T05:34:32Z"
    generation: 1
    labels:
      app: rancher-webhook
      pod-template-hash: 554ffd94d8
    name: rancher-webhook-554ffd94d8
    namespace: cattle-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: rancher-webhook
      uid: cec28531-fe69-4562-8264-6858bb9659bb
    resourceVersion: "196309"
    uid: 9689e049-3c6a-4881-828f-6405469c70ba
  spec:
    replicas: 1
    selector:
      matchLabels:
        app: rancher-webhook
        pod-template-hash: 554ffd94d8
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: rancher-webhook
          pod-template-hash: 554ffd94d8
      spec:
        containers:
        - env:
          - name: STAMP
          - name: ENABLE_MCM
            value: "true"
          - name: CATTLE_PORT
            value: "9443"
          - name: NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          image: rancher/rancher-webhook:v0.6.3
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /healthz
              port: https
              scheme: HTTPS
            periodSeconds: 5
            successThreshold: 1
            timeoutSeconds: 1
          name: rancher-webhook
          ports:
          - containerPort: 9443
            name: https
            protocol: TCP
          resources: {}
          startupProbe:
            failureThreshold: 60
            httpGet:
              path: /healthz
              port: https
              scheme: HTTPS
            periodSeconds: 5
            successThreshold: 1
            timeoutSeconds: 1
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        nodeSelector:
          kubernetes.io/os: linux
        priorityClassName: rancher-critical
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: rancher-webhook
        serviceAccountName: rancher-webhook
        terminationGracePeriodSeconds: 30
        tolerations:
        - effect: NoSchedule
          key: cattle.io/os
          operator: Equal
          value: linux
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "1"
      meta.helm.sh/release-name: system-upgrade-controller
      meta.helm.sh/release-namespace: cattle-system
    creationTimestamp: "2025-02-05T05:35:18Z"
    generation: 1
    labels:
      pod-template-hash: 5fb67f585d
      upgrade.cattle.io/controller: system-upgrade-controller
    name: system-upgrade-controller-5fb67f585d
    namespace: cattle-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: system-upgrade-controller
      uid: 709a66bf-6f03-4474-8362-d957132dcbcb
    resourceVersion: "196242"
    uid: eb788a2c-eb8b-42a4-9ea4-2f8711f37bf7
  spec:
    replicas: 1
    selector:
      matchLabels:
        pod-template-hash: 5fb67f585d
        upgrade.cattle.io/controller: system-upgrade-controller
    template:
      metadata:
        creationTimestamp: null
        labels:
          pod-template-hash: 5fb67f585d
          upgrade.cattle.io/controller: system-upgrade-controller
      spec:
        affinity:
          nodeAffinity:
            preferredDuringSchedulingIgnoredDuringExecution:
            - preference:
                matchExpressions:
                - key: node-role.kubernetes.io/control-plane
                  operator: In
                  values:
                  - "true"
              weight: 100
            - preference:
                matchExpressions:
                - key: node-role.kubernetes.io/master
                  operator: In
                  values:
                  - "true"
              weight: 100
            requiredDuringSchedulingIgnoredDuringExecution:
              nodeSelectorTerms:
              - matchExpressions:
                - key: kubernetes.io/os
                  operator: NotIn
                  values:
                  - windows
        containers:
        - env:
          - name: SYSTEM_UPGRADE_CONTROLLER_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.labels['upgrade.cattle.io/controller']
          - name: SYSTEM_UPGRADE_CONTROLLER_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          envFrom:
          - configMapRef:
              name: system-upgrade-controller-config
          image: rancher/system-upgrade-controller:v0.14.2
          imagePullPolicy: IfNotPresent
          name: system-upgrade-controller
          resources: {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            runAsGroup: 65534
            runAsNonRoot: true
            runAsUser: 65534
            seccompProfile:
              type: RuntimeDefault
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/ssl
            name: etc-ssl
            readOnly: true
          - mountPath: /etc/pki
            name: etc-pki
            readOnly: true
          - mountPath: /etc/ca-certificates
            name: etc-ca-certificates
            readOnly: true
          - mountPath: /tmp
            name: tmp
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: system-upgrade-controller
        serviceAccountName: system-upgrade-controller
        terminationGracePeriodSeconds: 30
        tolerations:
        - operator: Exists
        volumes:
        - hostPath:
            path: /etc/ssl
            type: DirectoryOrCreate
          name: etc-ssl
        - hostPath:
            path: /etc/pki
            type: DirectoryOrCreate
          name: etc-pki
        - hostPath:
            path: /etc/ca-certificates
            type: DirectoryOrCreate
          name: etc-ca-certificates
        - emptyDir: {}
          name: tmp
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "1"
      meta.helm.sh/release-name: cert-manager
      meta.helm.sh/release-namespace: cert-manager
    creationTimestamp: "2025-02-05T04:55:46Z"
    generation: 1
    labels:
      app: cert-manager
      app.kubernetes.io/component: controller
      app.kubernetes.io/instance: cert-manager
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: cert-manager
      app.kubernetes.io/version: v1.17.0
      helm.sh/chart: cert-manager-v1.17.0
      pod-template-hash: 6846f9d58c
    name: cert-manager-6846f9d58c
    namespace: cert-manager
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: cert-manager
      uid: 05281867-d295-40d0-9c7b-d8c22a054fd9
    resourceVersion: "196227"
    uid: d9803be7-33b4-489c-ab8f-8f2d794f2adb
  spec:
    replicas: 1
    selector:
      matchLabels:
        app.kubernetes.io/component: controller
        app.kubernetes.io/instance: cert-manager
        app.kubernetes.io/name: cert-manager
        pod-template-hash: 6846f9d58c
    template:
      metadata:
        annotations:
          prometheus.io/path: /metrics
          prometheus.io/port: "9402"
          prometheus.io/scrape: "true"
        creationTimestamp: null
        labels:
          app: cert-manager
          app.kubernetes.io/component: controller
          app.kubernetes.io/instance: cert-manager
          app.kubernetes.io/managed-by: Helm
          app.kubernetes.io/name: cert-manager
          app.kubernetes.io/version: v1.17.0
          helm.sh/chart: cert-manager-v1.17.0
          pod-template-hash: 6846f9d58c
      spec:
        containers:
        - args:
          - --v=2
          - --cluster-resource-namespace=$(POD_NAMESPACE)
          - --leader-election-namespace=kube-system
          - --acme-http01-solver-image=quay.io/jetstack/cert-manager-acmesolver:v1.17.0
          - --max-concurrent-challenges=60
          env:
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          image: quay.io/jetstack/cert-manager-controller:v1.17.0
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 8
            httpGet:
              path: /livez
              port: http-healthz
              scheme: HTTP
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 15
          name: cert-manager-controller
          ports:
          - containerPort: 9402
            name: http-metrics
            protocol: TCP
          - containerPort: 9403
            name: http-healthz
            protocol: TCP
          resources: {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        enableServiceLinks: false
        nodeSelector:
          kubernetes.io/os: linux
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          runAsNonRoot: true
          seccompProfile:
            type: RuntimeDefault
        serviceAccount: cert-manager
        serviceAccountName: cert-manager
        terminationGracePeriodSeconds: 30
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "1"
      meta.helm.sh/release-name: cert-manager
      meta.helm.sh/release-namespace: cert-manager
    creationTimestamp: "2025-02-05T04:55:46Z"
    generation: 1
    labels:
      app: cainjector
      app.kubernetes.io/component: cainjector
      app.kubernetes.io/instance: cert-manager
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: cainjector
      app.kubernetes.io/version: v1.17.0
      helm.sh/chart: cert-manager-v1.17.0
      pod-template-hash: 6fc95694fb
    name: cert-manager-cainjector-6fc95694fb
    namespace: cert-manager
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: cert-manager-cainjector
      uid: 20cddda3-c743-4ac7-80e6-048092358b97
    resourceVersion: "196278"
    uid: 6d3904c6-d088-4b28-8e8f-725bacc5b49d
  spec:
    replicas: 1
    selector:
      matchLabels:
        app.kubernetes.io/component: cainjector
        app.kubernetes.io/instance: cert-manager
        app.kubernetes.io/name: cainjector
        pod-template-hash: 6fc95694fb
    template:
      metadata:
        annotations:
          prometheus.io/path: /metrics
          prometheus.io/port: "9402"
          prometheus.io/scrape: "true"
        creationTimestamp: null
        labels:
          app: cainjector
          app.kubernetes.io/component: cainjector
          app.kubernetes.io/instance: cert-manager
          app.kubernetes.io/managed-by: Helm
          app.kubernetes.io/name: cainjector
          app.kubernetes.io/version: v1.17.0
          helm.sh/chart: cert-manager-v1.17.0
          pod-template-hash: 6fc95694fb
      spec:
        containers:
        - args:
          - --v=2
          - --leader-election-namespace=kube-system
          env:
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          image: quay.io/jetstack/cert-manager-cainjector:v1.17.0
          imagePullPolicy: IfNotPresent
          name: cert-manager-cainjector
          ports:
          - containerPort: 9402
            name: http-metrics
            protocol: TCP
          resources: {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        enableServiceLinks: false
        nodeSelector:
          kubernetes.io/os: linux
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          runAsNonRoot: true
          seccompProfile:
            type: RuntimeDefault
        serviceAccount: cert-manager-cainjector
        serviceAccountName: cert-manager-cainjector
        terminationGracePeriodSeconds: 30
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "1"
      meta.helm.sh/release-name: cert-manager
      meta.helm.sh/release-namespace: cert-manager
    creationTimestamp: "2025-02-05T04:55:46Z"
    generation: 1
    labels:
      app: webhook
      app.kubernetes.io/component: webhook
      app.kubernetes.io/instance: cert-manager
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: webhook
      app.kubernetes.io/version: v1.17.0
      helm.sh/chart: cert-manager-v1.17.0
      pod-template-hash: 66f876d88c
    name: cert-manager-webhook-66f876d88c
    namespace: cert-manager
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: cert-manager-webhook
      uid: d35fbb5f-5cbf-4274-8003-b02c57c407cb
    resourceVersion: "196222"
    uid: 4700a818-3bd4-4c8f-b843-f99c1d1125a7
  spec:
    replicas: 1
    selector:
      matchLabels:
        app.kubernetes.io/component: webhook
        app.kubernetes.io/instance: cert-manager
        app.kubernetes.io/name: webhook
        pod-template-hash: 66f876d88c
    template:
      metadata:
        annotations:
          prometheus.io/path: /metrics
          prometheus.io/port: "9402"
          prometheus.io/scrape: "true"
        creationTimestamp: null
        labels:
          app: webhook
          app.kubernetes.io/component: webhook
          app.kubernetes.io/instance: cert-manager
          app.kubernetes.io/managed-by: Helm
          app.kubernetes.io/name: webhook
          app.kubernetes.io/version: v1.17.0
          helm.sh/chart: cert-manager-v1.17.0
          pod-template-hash: 66f876d88c
      spec:
        containers:
        - args:
          - --v=2
          - --secure-port=10250
          - --dynamic-serving-ca-secret-namespace=$(POD_NAMESPACE)
          - --dynamic-serving-ca-secret-name=cert-manager-webhook-ca
          - --dynamic-serving-dns-names=cert-manager-webhook
          - --dynamic-serving-dns-names=cert-manager-webhook.$(POD_NAMESPACE)
          - --dynamic-serving-dns-names=cert-manager-webhook.$(POD_NAMESPACE).svc
          env:
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          image: quay.io/jetstack/cert-manager-webhook:v1.17.0
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /livez
              port: 6080
              scheme: HTTP
            initialDelaySeconds: 60
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          name: cert-manager-webhook
          ports:
          - containerPort: 10250
            name: https
            protocol: TCP
          - containerPort: 6080
            name: healthcheck
            protocol: TCP
          - containerPort: 9402
            name: http-metrics
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /healthz
              port: 6080
              scheme: HTTP
            initialDelaySeconds: 5
            periodSeconds: 5
            successThreshold: 1
            timeoutSeconds: 1
          resources: {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        enableServiceLinks: false
        nodeSelector:
          kubernetes.io/os: linux
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          runAsNonRoot: true
          seccompProfile:
            type: RuntimeDefault
        serviceAccount: cert-manager-webhook
        serviceAccountName: cert-manager-webhook
        terminationGracePeriodSeconds: 30
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "1"
      field.cattle.io/publicEndpoints: '[{"port":32131,"protocol":"TCP","serviceName":"default:flask-lstm-service","allNodes":true}]'
    creationTimestamp: "2025-02-27T12:30:53Z"
    generation: 1
    labels:
      app: flask-lstm
      pod-template-hash: 6f8bc77f4f
    name: flask-lstm-6f8bc77f4f
    namespace: default
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: flask-lstm
      uid: c0c3952a-5854-46cf-bf14-b753db5ed606
    resourceVersion: "196134"
    uid: 73423626-c514-413c-968b-36a624ec0414
  spec:
    replicas: 1
    selector:
      matchLabels:
        app: flask-lstm
        pod-template-hash: 6f8bc77f4f
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: flask-lstm
          pod-template-hash: 6f8bc77f4f
      spec:
        containers:
        - image: gnh374/lstm-model
          imagePullPolicy: Always
          name: flask-lstm
          ports:
          - containerPort: 5000
            protocol: TCP
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "2"
      objectset.rio.cattle.io/applied: H4sIAAAAAAAA/6xVQW/bOBP9Kx/mLMVW0jaugO/QjbPboq3XqJNeiqCgqZHFNcXhkiMnRqD/vhhJduw2TdrFniyTb4ZvHucN70F58xlDNOQgB+V9HG0ySGBtXAE5TNFb2tboGBKokVWhWEF+D8o5YsWGXJS/tPwLNUfkk2DoRCtmiyeGRkaSQPLDfbp1GNLVZg05rM/iwc4mS/733rji/2+KgtyzKZyqEXLQFLBw8afg0SstMetmiWncRsYa2gSsWqLtilpPYqq830H6vPIZHDJGyTYce0EBp7PFE8dWKlaQw1Lj6eTs9PVkkmXnL87U+GzySi1fZuPytHx1juX5i9MXY/3yXIh8V9ITpKNHLZQDbozc5VsTmcL2g6kNQz5OIKJFzRQEVCvW1YenymwlJQfFuNp2acla41bXvlCMfYq7a6c2yli1tAh51ibAWy/MPh1hZR1rb3dxBy30pNDtQVGaHCvjMETIv9yDCiv5gFSTKyGBEbIeDSqN5CZKYxFuEjC1WgmjoJyuMIxqE4LA0gG8+82zk+z0ZAxDxLyxdk7W6C3k8K6cEc8Dxt4C1mzQYYzzQMuuoFIZ2wS8qgLGimwB+VkCFbP/A1n2vWK591GFynIFCXgKDPlkPJFL0RV2d/z26mouUhln2Cg7Rau2C9Tkigj5q3ECHoOhYr+USXCjNcZ4cHKWAJsaqeEH4GN9JBR6KffKzjtWL8/26AEZiEmThRyup8LwmZCUtT8Ou7p4NOx1dhBYIwej4yOBNwkEVIX5V5JL5PZB8WyS/azi3wt++gt6B4zUBI1da1txYOxbv6YgLZWdjz8a6IB/Nxj7Xe0b2RqP627QDtAeKVZA3QTD2wtyjHddmcpaup0HszEWV3gZtbLdPIa8VDZiAlp5tTTWsOmpqKIQ28wur77+9m42/bq4/PT53cWlOKUI5GVPWQs3bS/6n85uPxHx78biMGhyDg22CWzINjV+pMYNfVTL53zQ/cCOcNB9rjSrtI+EhxN2OX+cY6SbyFQfpOr+p89kvJHmKVzcO3mKpWqsmNhRgYuDeXg80ilCDta45k7uyAdDnfBWxTjrCfRqpNo2kTGkOhg2WlmQawobo/GN1lLM7FvjMVkMu0fzyz2sUYhdDPHdQxe7EhIgL0jhB5d3RppENMKyRM2Qw4wWusKisVJ5n0aqSgNZPDmuR5wXyKbeKof/aeZaSf2Pp7yRaj1ZWm0XXq7mgpy8KGbXMt30X/zyq1Sru8Uab3vzDQe871gec6soctcvCdxW6K5dVGxiafrnCqY0I94XKmz7PtqPxdKsPiovRAxjfXRduxcm2U2a/YoI2YNmVOBbEiX2qIclOe6bodz+wCjD6HxgcxyX7r1BXtpK2b1HnzJLe9O2bftPAAAA//8kyNqv/AkAAA
      objectset.rio.cattle.io/id: ""
      objectset.rio.cattle.io/owner-gvk: k3s.cattle.io/v1, Kind=Addon
      objectset.rio.cattle.io/owner-name: coredns
      objectset.rio.cattle.io/owner-namespace: kube-system
    creationTimestamp: "2025-02-05T06:42:43Z"
    generation: 1
    labels:
      k8s-app: kube-dns
      pod-template-hash: f59c78b45
    name: coredns-f59c78b45
    namespace: kube-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: coredns
      uid: cbf9da15-e940-4b11-bda3-3f28d45d8409
    resourceVersion: "167542"
    uid: bbae2dd5-bb82-4268-b04e-bc8d721a9452
  spec:
    replicas: 1
    selector:
      matchLabels:
        k8s-app: kube-dns
        pod-template-hash: f59c78b45
    template:
      metadata:
        annotations:
          kubectl.kubernetes.io/restartedAt: "2025-02-05T06:42:43Z"
        creationTimestamp: null
        labels:
          k8s-app: kube-dns
          pod-template-hash: f59c78b45
      spec:
        containers:
        - args:
          - -conf
          - /etc/coredns/Corefile
          image: rancher/mirrored-coredns-coredns:1.12.0
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /health
              port: 8080
              scheme: HTTP
            initialDelaySeconds: 60
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          name: coredns
          ports:
          - containerPort: 53
            name: dns
            protocol: UDP
          - containerPort: 53
            name: dns-tcp
            protocol: TCP
          - containerPort: 9153
            name: metrics
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /ready
              port: 8181
              scheme: HTTP
            periodSeconds: 2
            successThreshold: 1
            timeoutSeconds: 1
          resources:
            limits:
              memory: 170Mi
            requests:
              cpu: 100m
              memory: 70Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              add:
              - NET_BIND_SERVICE
              drop:
              - all
            readOnlyRootFilesystem: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/coredns
            name: config-volume
            readOnly: true
          - mountPath: /etc/coredns/custom
            name: custom-config-volume
            readOnly: true
        dnsPolicy: Default
        nodeSelector:
          kubernetes.io/os: linux
        priorityClassName: system-cluster-critical
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: coredns
        serviceAccountName: coredns
        terminationGracePeriodSeconds: 30
        tolerations:
        - key: CriticalAddonsOnly
          operator: Exists
        - effect: NoSchedule
          key: node-role.kubernetes.io/control-plane
          operator: Exists
        - effect: NoSchedule
          key: node-role.kubernetes.io/master
          operator: Exists
        topologySpreadConstraints:
        - labelSelector:
            matchLabels:
              k8s-app: kube-dns
          maxSkew: 1
          topologyKey: kubernetes.io/hostname
          whenUnsatisfiable: DoNotSchedule
        volumes:
        - configMap:
            defaultMode: 420
            items:
            - key: Corefile
              path: Corefile
            - key: NodeHosts
              path: NodeHosts
            name: coredns
          name: config-volume
        - configMap:
            defaultMode: 420
            name: coredns-custom
            optional: true
          name: custom-config-volume
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "1"
      objectset.rio.cattle.io/applied: H4sIAAAAAAAA/6xUXWvrRhD9K2WeJdmuE2MEfTBJSksTxySkL8GU8Wpkb7zaXXbHaoTRf7+MJeeDGyf3wn3SfsycPTPnjPaAXv9LIWpnIQf0Pg7qESSw1baAHC7JG9dUZBkSqIixQEbI94DWOkbWzkbZutUTKY7EWdAuU8hsKNNuoAUEkpP37n9LIV3XW8hhO45vbupR8ts/2hZ/zIrC2S8hLFYEORin0KSRXcA1/VBS9Kgkc7tbURqbyFRBm4DBFZlPS9tg3EAOo+m4HJ+ryXlZrtR4ODmbDMfl2bgcnU+HxVRNpvh7gaviTEDfkfTIm9QHV2tpPgXo7k/wiZ6UsAnUxf+lpcjmWleaIR8mEMmQYhckqEJWm+uXCtD706+2As4BmdbN4QFnjLbrB18gUwf2/GCxRm1wZQjyUZsAN1443r2LlXOqvDnmvXGL+QkufaHKWUZtKUTIH2VbVSiWfDzdvsgYxKdpqpwt9RoSGBCrQbfrP9lTdBaWCZCtD8i9KIvby//ms5ur+8Xs4goSqNHs6M/gKiFTajLFHZUv6wWyiH+sMXtVrm3bZQK6Ev/lENCqDYXBx5zzepgNs/EQ+oTFzpiFM1o1kMPf5dzxIlDshu8r79TO7Cq6cTvLXccqWfY837bhFas7SLtMaJdC3AftgubmwmCM8y6uc2FqXUGpCpq1QiPtplBrRTOl5KX5Z/zSPjbFLhgSYGcoHH8gj3vYkhR90cMfhj7eWtPIEHuJFGvD1bOOHKFN9kBlSYohh7m7VxsqdkYGvoM5UA3OUCZjFCwxRZlZMVVwJvUGLf1S5AojH3T4AHJ5VOdoZWn7DXpx0/ey9t5tT8vUtu23AAAA///s6eu+uAUAAA
      objectset.rio.cattle.io/id: ""
      objectset.rio.cattle.io/owner-gvk: k3s.cattle.io/v1, Kind=Addon
      objectset.rio.cattle.io/owner-name: local-storage
      objectset.rio.cattle.io/owner-namespace: kube-system
    creationTimestamp: "2025-02-05T04:54:34Z"
    generation: 1
    labels:
      app: local-path-provisioner
      pod-template-hash: 5cf85fd84d
    name: local-path-provisioner-5cf85fd84d
    namespace: kube-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: local-path-provisioner
      uid: ac77729a-8823-4097-818a-621e5aa7f967
    resourceVersion: "162567"
    uid: 1709685b-a763-4213-b0a9-9a4153d2969d
  spec:
    replicas: 1
    selector:
      matchLabels:
        app: local-path-provisioner
        pod-template-hash: 5cf85fd84d
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: local-path-provisioner
          pod-template-hash: 5cf85fd84d
      spec:
        containers:
        - command:
          - local-path-provisioner
          - start
          - --config
          - /etc/config/config.json
          env:
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          image: rancher/local-path-provisioner:v0.0.30
          imagePullPolicy: IfNotPresent
          name: local-path-provisioner
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/config/
            name: config-volume
        dnsPolicy: ClusterFirst
        priorityClassName: system-node-critical
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: local-path-provisioner-service-account
        serviceAccountName: local-path-provisioner-service-account
        terminationGracePeriodSeconds: 30
        tolerations:
        - key: CriticalAddonsOnly
          operator: Exists
        - effect: NoSchedule
          key: node-role.kubernetes.io/control-plane
          operator: Exists
        - effect: NoSchedule
          key: node-role.kubernetes.io/master
          operator: Exists
        volumes:
        - configMap:
            defaultMode: 420
            name: local-path-config
          name: config-volume
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "1"
      objectset.rio.cattle.io/applied: H4sIAAAAAAAA/6xV4W8auRP9V36az7vJLoHQrsQHRPiV6tIUFXqnUxUh4x3Ah9f2eWZpuIj//TS7SUovpelV9wUt9vPzmzfjmXtQwfyKkYx3UIAKgc53OSSwNa6EAq4wWL+v0DEkUCGrUrGC4h6Uc54VG+9I/vrlH6iZkM+i8WdaMVs8M/7cCAkkJ/f9Z4cxXe+2UMD2go52dnnyv1+MKwfDsvTuRQqnKoRCJEajKSWMO4xpeSz/ZQIKSgvLtl5iSntirOCQgFVLtE2Y21eUqhCeXfQd9o2iDRSAeYadbg/zC+xeqn6np1+XZVl2V69x2bl4vcq7/X6/uyrlvm/GAu36CYkUUIvAiDsjuZwYYh/316YyDEWWAKFFzT4KqFKsN9cvB3UQYo6Kcb1vyL21xq0/hlIxtkR3H53aKWPV0iIU+SEB3gfR9+ErrKxjFezjuaNC+gFzT1pyFLj2jpVxGAmKT/eg4lo+IE01Rk5LEwfnXAVIIE0JdR0xDT7yIM86vaxZFUMtchoirjBGLFNVlhGJUomIBm8dY3TKvp0m47unz4knbrQdU9SEqfMlpsSKa2puagCt/DQieVvL2xnkPWp22FKqTdhgTKk2jDSYX88W49HVZCy/s+Hit7fzyWI4ni06vcvFm9G7xWwyvHjVTb7gPvwQ6h9seefVI67TuzzFdhJ1xDaaDEeTYSdbTN9f/55fZL1vkT0DwW0CplJryW5UTm8wnlcmRi8Z+DrdxS476591IAFrduiQaBr9simolTK2jjjfRKSNtyUUFwlsmMMbZNkPiuURnsvBvyCBJiNFgxD/SW+wqa/JfD6dSVkZZ9goe4VW7WeovSsJisssgYDR+PJpKZenVWuNREeX5wmwqdDX/AX4nXctatqyfariaSOwqc6nc49qQ/TstbdQwHw0hcNtAhFVaX7KETm5/3lLnjvS+ReGyEOoo0ZqW9efNRI33zrUUECeZVUzdiof91BAP3tn2qYkL9jwfuQd410Tj7LWf55GszMW1zgmrWwznaBYKUvYWvTe2f0H7/n/xuJD7yw41rJbuyHdeCe7X619JIySiCw7JLDztq7wna/dQ74q+Zw+WNn2l4dkcRWk68DhVvITovGNYKuIblpEK6BtFDoaNlpZMR7jzmgcai3cNydKhr3F+Dh+P93DFsWg0QNNMzJJopXBFAQpnR/Gd0YMPiT3gKsVakn4jZ/pDZa1lR7W0jSSord4Jh0tOmQkGWVSndHbNFjl8D9lrhRxO0WfU94++t5GilXg/ZWRQXb4ltuHw+HvAAAA//9PFN5y1QgAAA
      objectset.rio.cattle.io/id: ""
      objectset.rio.cattle.io/owner-gvk: k3s.cattle.io/v1, Kind=Addon
      objectset.rio.cattle.io/owner-name: metrics-server-deployment
      objectset.rio.cattle.io/owner-namespace: kube-system
    creationTimestamp: "2025-02-05T04:54:34Z"
    generation: 1
    labels:
      k8s-app: metrics-server
      pod-template-hash: 5985cbc9d7
    name: metrics-server-5985cbc9d7
    namespace: kube-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: metrics-server
      uid: 1fcca16f-9222-4390-8ab2-a13cd57356c9
    resourceVersion: "196248"
    uid: e21cfe77-b639-4272-9a53-a05be6a85625
  spec:
    replicas: 1
    selector:
      matchLabels:
        k8s-app: metrics-server
        pod-template-hash: 5985cbc9d7
    template:
      metadata:
        creationTimestamp: null
        labels:
          k8s-app: metrics-server
          pod-template-hash: 5985cbc9d7
        name: metrics-server
      spec:
        containers:
        - args:
          - --cert-dir=/tmp
          - --secure-port=10250
          - --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname
          - --kubelet-use-node-status-port
          - --metric-resolution=15s
          - --tls-cipher-suites=TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305
          image: rancher/mirrored-metrics-server:v0.7.2
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /livez
              port: https
              scheme: HTTPS
            initialDelaySeconds: 60
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          name: metrics-server
          ports:
          - containerPort: 10250
            name: https
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /readyz
              port: https
              scheme: HTTPS
            periodSeconds: 2
            successThreshold: 1
            timeoutSeconds: 1
          resources:
            requests:
              cpu: 100m
              memory: 70Mi
          securityContext:
            allowPrivilegeEscalation: false
            readOnlyRootFilesystem: true
            runAsNonRoot: true
            runAsUser: 1000
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /tmp
            name: tmp-dir
        dnsPolicy: ClusterFirst
        priorityClassName: system-node-critical
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: metrics-server
        serviceAccountName: metrics-server
        terminationGracePeriodSeconds: 30
        tolerations:
        - key: CriticalAddonsOnly
          operator: Exists
        - effect: NoSchedule
          key: node-role.kubernetes.io/control-plane
          operator: Exists
        - effect: NoSchedule
          key: node-role.kubernetes.io/master
          operator: Exists
        volumes:
        - emptyDir: {}
          name: tmp-dir
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "1"
      field.cattle.io/publicEndpoints: '[{"addresses":["172.31.17.126"],"port":80,"protocol":"TCP","serviceName":"kube-system:traefik","allNodes":false},{"addresses":["172.31.17.126"],"port":443,"protocol":"TCP","serviceName":"kube-system:traefik","allNodes":false}]'
      meta.helm.sh/release-name: traefik
      meta.helm.sh/release-namespace: kube-system
    creationTimestamp: "2025-02-05T04:54:52Z"
    generation: 1
    labels:
      app.kubernetes.io/instance: traefik-kube-system
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: traefik
      helm.sh/chart: traefik-27.0.201_up27.0.2
      pod-template-hash: 5d45fc8cc9
    name: traefik-5d45fc8cc9
    namespace: kube-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: traefik
      uid: 5da96425-00d5-45f4-b331-92d96315e0a7
    resourceVersion: "196185"
    uid: 360ee329-814b-46b4-9a96-61839bfd3820
  spec:
    replicas: 1
    selector:
      matchLabels:
        app.kubernetes.io/instance: traefik-kube-system
        app.kubernetes.io/name: traefik
        pod-template-hash: 5d45fc8cc9
    template:
      metadata:
        annotations:
          prometheus.io/path: /metrics
          prometheus.io/port: "9100"
          prometheus.io/scrape: "true"
        creationTimestamp: null
        labels:
          app.kubernetes.io/instance: traefik-kube-system
          app.kubernetes.io/managed-by: Helm
          app.kubernetes.io/name: traefik
          helm.sh/chart: traefik-27.0.201_up27.0.2
          pod-template-hash: 5d45fc8cc9
      spec:
        containers:
        - args:
          - --global.checknewversion
          - --global.sendanonymoususage
          - --entrypoints.metrics.address=:9100/tcp
          - --entrypoints.traefik.address=:9000/tcp
          - --entrypoints.web.address=:8000/tcp
          - --entrypoints.websecure.address=:8443/tcp
          - --api.dashboard=true
          - --ping=true
          - --metrics.prometheus=true
          - --metrics.prometheus.entrypoint=metrics
          - --providers.kubernetescrd
          - --providers.kubernetesingress
          - --providers.kubernetesingress.ingressendpoint.publishedservice=kube-system/traefik
          - --entrypoints.websecure.http.tls=true
          env:
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          image: rancher/mirrored-library-traefik:2.11.18
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /ping
              port: 9000
              scheme: HTTP
            initialDelaySeconds: 2
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 2
          name: traefik
          ports:
          - containerPort: 9100
            name: metrics
            protocol: TCP
          - containerPort: 9000
            name: traefik
            protocol: TCP
          - containerPort: 8000
            name: web
            protocol: TCP
          - containerPort: 8443
            name: websecure
            protocol: TCP
          readinessProbe:
            failureThreshold: 1
            httpGet:
              path: /ping
              port: 9000
              scheme: HTTP
            initialDelaySeconds: 2
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 2
          resources: {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /data
            name: data
          - mountPath: /tmp
            name: tmp
        dnsPolicy: ClusterFirst
        priorityClassName: system-cluster-critical
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroupChangePolicy: OnRootMismatch
          runAsGroup: 65532
          runAsNonRoot: true
          runAsUser: 65532
        serviceAccount: traefik
        serviceAccountName: traefik
        terminationGracePeriodSeconds: 60
        tolerations:
        - key: CriticalAddonsOnly
          operator: Exists
        - effect: NoSchedule
          key: node-role.kubernetes.io/control-plane
          operator: Exists
        - effect: NoSchedule
          key: node-role.kubernetes.io/master
          operator: Exists
        volumes:
        - emptyDir: {}
          name: data
        - emptyDir: {}
          name: tmp
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "1"
    creationTimestamp: "2025-03-02T08:12:05Z"
    generation: 1
    labels:
      app: flask-lstm
      pod-template-hash: 6f8bc77f4f
    name: flask-lstm-6f8bc77f4f
    namespace: monitoring
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: flask-lstm
      uid: a69f9991-0dc9-4231-8979-080af48ca49f
    resourceVersion: "172691"
    uid: 06759e8e-e00a-409e-b5a7-3ddd8ffe2ae5
  spec:
    replicas: 1
    selector:
      matchLabels:
        app: flask-lstm
        pod-template-hash: 6f8bc77f4f
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: flask-lstm
          pod-template-hash: 6f8bc77f4f
      spec:
        containers:
        - image: gnh374/lstm-model
          imagePullPolicy: Always
          name: flask-lstm
          ports:
          - containerPort: 5000
            protocol: TCP
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "27"
      meta.helm.sh/release-name: prometheus
      meta.helm.sh/release-namespace: monitoring
    creationTimestamp: "2025-02-26T10:32:53Z"
    generation: 1
    labels:
      app.kubernetes.io/component: metrics
      app.kubernetes.io/instance: prometheus
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: kube-state-metrics
      app.kubernetes.io/part-of: kube-state-metrics
      app.kubernetes.io/version: 2.14.0
      helm.sh/chart: kube-state-metrics-5.28.1
      pod-template-hash: 5687c4787f
      velero.io/backup-name: monitoring-backup
      velero.io/restore-name: monitoring-backup-20250226103249
    name: prometheus-kube-state-metrics-5687c4787f
    namespace: monitoring
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: prometheus-kube-state-metrics
      uid: b10b918b-2a9c-43b7-ab4d-a96a9c8326d4
    resourceVersion: "124704"
    uid: fc0756b5-e5fe-4319-952a-9af554a621b8
  spec:
    replicas: 0
    selector:
      matchLabels:
        app.kubernetes.io/instance: prometheus
        app.kubernetes.io/name: kube-state-metrics
        pod-template-hash: 5687c4787f
    template:
      metadata:
        annotations:
          kubectl.kubernetes.io/restartedAt: "2025-02-12T13:04:13Z"
        creationTimestamp: null
        labels:
          app.kubernetes.io/component: metrics
          app.kubernetes.io/instance: prometheus
          app.kubernetes.io/managed-by: Helm
          app.kubernetes.io/name: kube-state-metrics
          app.kubernetes.io/part-of: kube-state-metrics
          app.kubernetes.io/version: 2.14.0
          helm.sh/chart: kube-state-metrics-5.28.1
          pod-template-hash: 5687c4787f
      spec:
        automountServiceAccountToken: true
        containers:
        - args:
          - --port=8080
          - --resources=certificatesigningrequests,configmaps,cronjobs,daemonsets,deployments,endpoints,horizontalpodautoscalers,ingresses,jobs,leases,limitranges,mutatingwebhookconfigurations,namespaces,networkpolicies,nodes,persistentvolumeclaims,persistentvolumes,poddisruptionbudgets,pods,replicasets,replicationcontrollers,resourcequotas,secrets,services,statefulsets,storageclasses,validatingwebhookconfigurations,volumeattachments
          image: registry.k8s.io/kube-state-metrics/kube-state-metrics:v2.14.0
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /livez
              port: 8080
              scheme: HTTP
            initialDelaySeconds: 5
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          name: kube-state-metrics
          ports:
          - containerPort: 8080
            name: http
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /readyz
              port: 8081
              scheme: HTTP
            initialDelaySeconds: 5
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          resources: {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 65534
          runAsGroup: 65534
          runAsNonRoot: true
          runAsUser: 65534
          seccompProfile:
            type: RuntimeDefault
        serviceAccount: prometheus-kube-state-metrics
        serviceAccountName: prometheus-kube-state-metrics
        terminationGracePeriodSeconds: 30
  status:
    observedGeneration: 1
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "18"
      meta.helm.sh/release-name: prometheus
      meta.helm.sh/release-namespace: monitoring
    creationTimestamp: "2025-02-26T10:32:53Z"
    generation: 1
    labels:
      app.kubernetes.io/component: metrics
      app.kubernetes.io/instance: prometheus
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: kube-state-metrics
      app.kubernetes.io/part-of: kube-state-metrics
      app.kubernetes.io/version: 2.14.0
      helm.sh/chart: kube-state-metrics-5.28.1
      pod-template-hash: 5c8cf787f9
      velero.io/backup-name: monitoring-backup
      velero.io/restore-name: monitoring-backup-20250226103249
    name: prometheus-kube-state-metrics-5c8cf787f9
    namespace: monitoring
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: prometheus-kube-state-metrics
      uid: b10b918b-2a9c-43b7-ab4d-a96a9c8326d4
    resourceVersion: "124697"
    uid: eb8fcacd-1f9a-47c7-a5e6-f5c623aa9978
  spec:
    replicas: 0
    selector:
      matchLabels:
        app.kubernetes.io/instance: prometheus
        app.kubernetes.io/name: kube-state-metrics
        pod-template-hash: 5c8cf787f9
    template:
      metadata:
        annotations:
          kubectl.kubernetes.io/restartedAt: "2025-02-12T11:33:17Z"
        creationTimestamp: null
        labels:
          app.kubernetes.io/component: metrics
          app.kubernetes.io/instance: prometheus
          app.kubernetes.io/managed-by: Helm
          app.kubernetes.io/name: kube-state-metrics
          app.kubernetes.io/part-of: kube-state-metrics
          app.kubernetes.io/version: 2.14.0
          helm.sh/chart: kube-state-metrics-5.28.1
          pod-template-hash: 5c8cf787f9
      spec:
        automountServiceAccountToken: true
        containers:
        - args:
          - --port=8080
          - --resources=certificatesigningrequests,configmaps,cronjobs,daemonsets,deployments,endpoints,horizontalpodautoscalers,ingresses,jobs,leases,limitranges,mutatingwebhookconfigurations,namespaces,networkpolicies,nodes,persistentvolumeclaims,persistentvolumes,poddisruptionbudgets,pods,replicasets,replicationcontrollers,resourcequotas,secrets,services,statefulsets,storageclasses,validatingwebhookconfigurations,volumeattachments
          image: registry.k8s.io/kube-state-metrics/kube-state-metrics:v2.14.0
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /livez
              port: 8080
              scheme: HTTP
            initialDelaySeconds: 5
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          name: kube-state-metrics
          ports:
          - containerPort: 8080
            name: http
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /readyz
              port: 8081
              scheme: HTTP
            initialDelaySeconds: 5
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          resources: {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 65534
          runAsGroup: 65534
          runAsNonRoot: true
          runAsUser: 65534
          seccompProfile:
            type: RuntimeDefault
        serviceAccount: prometheus-kube-state-metrics
        serviceAccountName: prometheus-kube-state-metrics
        terminationGracePeriodSeconds: 30
  status:
    observedGeneration: 1
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "21"
      meta.helm.sh/release-name: prometheus
      meta.helm.sh/release-namespace: monitoring
    creationTimestamp: "2025-02-26T10:32:53Z"
    generation: 1
    labels:
      app.kubernetes.io/component: metrics
      app.kubernetes.io/instance: prometheus
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: kube-state-metrics
      app.kubernetes.io/part-of: kube-state-metrics
      app.kubernetes.io/version: 2.14.0
      helm.sh/chart: kube-state-metrics-5.28.1
      pod-template-hash: 64d9984b7f
      velero.io/backup-name: monitoring-backup
      velero.io/restore-name: monitoring-backup-20250226103249
    name: prometheus-kube-state-metrics-64d9984b7f
    namespace: monitoring
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: prometheus-kube-state-metrics
      uid: b10b918b-2a9c-43b7-ab4d-a96a9c8326d4
    resourceVersion: "124701"
    uid: 4a0d666b-a205-44e7-b38f-51acc4e0cdac
  spec:
    replicas: 0
    selector:
      matchLabels:
        app.kubernetes.io/instance: prometheus
        app.kubernetes.io/name: kube-state-metrics
        pod-template-hash: 64d9984b7f
    template:
      metadata:
        annotations:
          kubectl.kubernetes.io/restartedAt: "2025-02-12T11:51:13Z"
        creationTimestamp: null
        labels:
          app.kubernetes.io/component: metrics
          app.kubernetes.io/instance: prometheus
          app.kubernetes.io/managed-by: Helm
          app.kubernetes.io/name: kube-state-metrics
          app.kubernetes.io/part-of: kube-state-metrics
          app.kubernetes.io/version: 2.14.0
          helm.sh/chart: kube-state-metrics-5.28.1
          pod-template-hash: 64d9984b7f
      spec:
        automountServiceAccountToken: true
        containers:
        - args:
          - --port=8080
          - --resources=certificatesigningrequests,configmaps,cronjobs,daemonsets,deployments,endpoints,horizontalpodautoscalers,ingresses,jobs,leases,limitranges,mutatingwebhookconfigurations,namespaces,networkpolicies,nodes,persistentvolumeclaims,persistentvolumes,poddisruptionbudgets,pods,replicasets,replicationcontrollers,resourcequotas,secrets,services,statefulsets,storageclasses,validatingwebhookconfigurations,volumeattachments
          image: registry.k8s.io/kube-state-metrics/kube-state-metrics:v2.14.0
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /livez
              port: 8080
              scheme: HTTP
            initialDelaySeconds: 5
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          name: kube-state-metrics
          ports:
          - containerPort: 8080
            name: http
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /readyz
              port: 8081
              scheme: HTTP
            initialDelaySeconds: 5
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          resources: {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 65534
          runAsGroup: 65534
          runAsNonRoot: true
          runAsUser: 65534
          seccompProfile:
            type: RuntimeDefault
        serviceAccount: prometheus-kube-state-metrics
        serviceAccountName: prometheus-kube-state-metrics
        terminationGracePeriodSeconds: 30
  status:
    observedGeneration: 1
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "20"
      meta.helm.sh/release-name: prometheus
      meta.helm.sh/release-namespace: monitoring
    creationTimestamp: "2025-02-26T10:32:53Z"
    generation: 1
    labels:
      app.kubernetes.io/component: metrics
      app.kubernetes.io/instance: prometheus
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: kube-state-metrics
      app.kubernetes.io/part-of: kube-state-metrics
      app.kubernetes.io/version: 2.14.0
      helm.sh/chart: kube-state-metrics-5.28.1
      pod-template-hash: 677d675b67
      velero.io/backup-name: monitoring-backup
      velero.io/restore-name: monitoring-backup-20250226103249
    name: prometheus-kube-state-metrics-677d675b67
    namespace: monitoring
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: prometheus-kube-state-metrics
      uid: b10b918b-2a9c-43b7-ab4d-a96a9c8326d4
    resourceVersion: "124683"
    uid: 0f7b5591-bbc4-484a-8c0b-d0bc80d171b1
  spec:
    replicas: 0
    selector:
      matchLabels:
        app.kubernetes.io/instance: prometheus
        app.kubernetes.io/name: kube-state-metrics
        pod-template-hash: 677d675b67
    template:
      metadata:
        annotations:
          kubectl.kubernetes.io/restartedAt: "2025-02-12T11:48:06Z"
        creationTimestamp: null
        labels:
          app.kubernetes.io/component: metrics
          app.kubernetes.io/instance: prometheus
          app.kubernetes.io/managed-by: Helm
          app.kubernetes.io/name: kube-state-metrics
          app.kubernetes.io/part-of: kube-state-metrics
          app.kubernetes.io/version: 2.14.0
          helm.sh/chart: kube-state-metrics-5.28.1
          pod-template-hash: 677d675b67
      spec:
        automountServiceAccountToken: true
        containers:
        - args:
          - --port=8080
          - --resources=certificatesigningrequests,configmaps,cronjobs,daemonsets,deployments,endpoints,horizontalpodautoscalers,ingresses,jobs,leases,limitranges,mutatingwebhookconfigurations,namespaces,networkpolicies,nodes,persistentvolumeclaims,persistentvolumes,poddisruptionbudgets,pods,replicasets,replicationcontrollers,resourcequotas,secrets,services,statefulsets,storageclasses,validatingwebhookconfigurations,volumeattachments
          image: registry.k8s.io/kube-state-metrics/kube-state-metrics:v2.14.0
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /livez
              port: 8080
              scheme: HTTP
            initialDelaySeconds: 5
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          name: kube-state-metrics
          ports:
          - containerPort: 8080
            name: http
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /readyz
              port: 8081
              scheme: HTTP
            initialDelaySeconds: 5
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          resources: {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 65534
          runAsGroup: 65534
          runAsNonRoot: true
          runAsUser: 65534
          seccompProfile:
            type: RuntimeDefault
        serviceAccount: prometheus-kube-state-metrics
        serviceAccountName: prometheus-kube-state-metrics
        terminationGracePeriodSeconds: 30
  status:
    observedGeneration: 1
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "26"
      meta.helm.sh/release-name: prometheus
      meta.helm.sh/release-namespace: monitoring
    creationTimestamp: "2025-02-26T10:32:54Z"
    generation: 1
    labels:
      app.kubernetes.io/component: metrics
      app.kubernetes.io/instance: prometheus
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: kube-state-metrics
      app.kubernetes.io/part-of: kube-state-metrics
      app.kubernetes.io/version: 2.14.0
      helm.sh/chart: kube-state-metrics-5.28.1
      pod-template-hash: 6d5c6557f5
      velero.io/backup-name: monitoring-backup
      velero.io/restore-name: monitoring-backup-20250226103249
    name: prometheus-kube-state-metrics-6d5c6557f5
    namespace: monitoring
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: prometheus-kube-state-metrics
      uid: b10b918b-2a9c-43b7-ab4d-a96a9c8326d4
    resourceVersion: "124688"
    uid: 7692ae61-e488-4e3e-9460-b7d57ebf0480
  spec:
    replicas: 0
    selector:
      matchLabels:
        app.kubernetes.io/instance: prometheus
        app.kubernetes.io/name: kube-state-metrics
        pod-template-hash: 6d5c6557f5
    template:
      metadata:
        annotations:
          kubectl.kubernetes.io/restartedAt: "2025-02-12T12:41:42Z"
        creationTimestamp: null
        labels:
          app.kubernetes.io/component: metrics
          app.kubernetes.io/instance: prometheus
          app.kubernetes.io/managed-by: Helm
          app.kubernetes.io/name: kube-state-metrics
          app.kubernetes.io/part-of: kube-state-metrics
          app.kubernetes.io/version: 2.14.0
          helm.sh/chart: kube-state-metrics-5.28.1
          pod-template-hash: 6d5c6557f5
      spec:
        automountServiceAccountToken: true
        containers:
        - args:
          - --port=8080
          - --resources=certificatesigningrequests,configmaps,cronjobs,daemonsets,deployments,endpoints,horizontalpodautoscalers,ingresses,jobs,leases,limitranges,mutatingwebhookconfigurations,namespaces,networkpolicies,nodes,persistentvolumeclaims,persistentvolumes,poddisruptionbudgets,pods,replicasets,replicationcontrollers,resourcequotas,secrets,services,statefulsets,storageclasses,validatingwebhookconfigurations,volumeattachments
          image: registry.k8s.io/kube-state-metrics/kube-state-metrics:v2.14.0
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /livez
              port: 8080
              scheme: HTTP
            initialDelaySeconds: 5
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          name: kube-state-metrics
          ports:
          - containerPort: 8080
            name: http
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /readyz
              port: 8081
              scheme: HTTP
            initialDelaySeconds: 5
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          resources: {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 65534
          runAsGroup: 65534
          runAsNonRoot: true
          runAsUser: 65534
          seccompProfile:
            type: RuntimeDefault
        serviceAccount: prometheus-kube-state-metrics
        serviceAccountName: prometheus-kube-state-metrics
        terminationGracePeriodSeconds: 30
  status:
    observedGeneration: 1
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "28"
      meta.helm.sh/release-name: prometheus
      meta.helm.sh/release-namespace: monitoring
    creationTimestamp: "2025-02-26T10:32:54Z"
    generation: 1
    labels:
      app.kubernetes.io/component: metrics
      app.kubernetes.io/instance: prometheus
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: kube-state-metrics
      app.kubernetes.io/part-of: kube-state-metrics
      app.kubernetes.io/version: 2.14.0
      helm.sh/chart: kube-state-metrics-5.28.1
      pod-template-hash: 75b5b65f7f
      velero.io/backup-name: monitoring-backup
      velero.io/restore-name: monitoring-backup-20250226103249
    name: prometheus-kube-state-metrics-75b5b65f7f
    namespace: monitoring
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: prometheus-kube-state-metrics
      uid: b10b918b-2a9c-43b7-ab4d-a96a9c8326d4
    resourceVersion: "196212"
    uid: 4ff56e3b-ab3e-4869-a50d-dfc7c573b099
  spec:
    replicas: 1
    selector:
      matchLabels:
        app.kubernetes.io/instance: prometheus
        app.kubernetes.io/name: kube-state-metrics
        pod-template-hash: 75b5b65f7f
    template:
      metadata:
        annotations:
          kubectl.kubernetes.io/restartedAt: "2025-02-13T12:06:57Z"
        creationTimestamp: null
        labels:
          app.kubernetes.io/component: metrics
          app.kubernetes.io/instance: prometheus
          app.kubernetes.io/managed-by: Helm
          app.kubernetes.io/name: kube-state-metrics
          app.kubernetes.io/part-of: kube-state-metrics
          app.kubernetes.io/version: 2.14.0
          helm.sh/chart: kube-state-metrics-5.28.1
          pod-template-hash: 75b5b65f7f
      spec:
        automountServiceAccountToken: true
        containers:
        - args:
          - --port=8080
          - --resources=certificatesigningrequests,configmaps,cronjobs,daemonsets,deployments,endpoints,horizontalpodautoscalers,ingresses,jobs,leases,limitranges,mutatingwebhookconfigurations,namespaces,networkpolicies,nodes,persistentvolumeclaims,persistentvolumes,poddisruptionbudgets,pods,replicasets,replicationcontrollers,resourcequotas,secrets,services,statefulsets,storageclasses,validatingwebhookconfigurations,volumeattachments
          image: registry.k8s.io/kube-state-metrics/kube-state-metrics:v2.14.0
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /livez
              port: 8080
              scheme: HTTP
            initialDelaySeconds: 5
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          name: kube-state-metrics
          ports:
          - containerPort: 8080
            name: http
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /readyz
              port: 8081
              scheme: HTTP
            initialDelaySeconds: 5
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          resources: {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 65534
          runAsGroup: 65534
          runAsNonRoot: true
          runAsUser: 65534
          seccompProfile:
            type: RuntimeDefault
        serviceAccount: prometheus-kube-state-metrics
        serviceAccountName: prometheus-kube-state-metrics
        terminationGracePeriodSeconds: 30
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "23"
      meta.helm.sh/release-name: prometheus
      meta.helm.sh/release-namespace: monitoring
    creationTimestamp: "2025-02-26T10:32:54Z"
    generation: 1
    labels:
      app.kubernetes.io/component: metrics
      app.kubernetes.io/instance: prometheus
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: kube-state-metrics
      app.kubernetes.io/part-of: kube-state-metrics
      app.kubernetes.io/version: 2.14.0
      helm.sh/chart: kube-state-metrics-5.28.1
      pod-template-hash: 7f9469bdf4
      velero.io/backup-name: monitoring-backup
      velero.io/restore-name: monitoring-backup-20250226103249
    name: prometheus-kube-state-metrics-7f9469bdf4
    namespace: monitoring
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: prometheus-kube-state-metrics
      uid: b10b918b-2a9c-43b7-ab4d-a96a9c8326d4
    resourceVersion: "124692"
    uid: b3e82ffe-5833-482a-9d4d-a2300aa463df
  spec:
    replicas: 0
    selector:
      matchLabels:
        app.kubernetes.io/instance: prometheus
        app.kubernetes.io/name: kube-state-metrics
        pod-template-hash: 7f9469bdf4
    template:
      metadata:
        annotations:
          kubectl.kubernetes.io/restartedAt: "2025-02-12T12:09:01Z"
        creationTimestamp: null
        labels:
          app.kubernetes.io/component: metrics
          app.kubernetes.io/instance: prometheus
          app.kubernetes.io/managed-by: Helm
          app.kubernetes.io/name: kube-state-metrics
          app.kubernetes.io/part-of: kube-state-metrics
          app.kubernetes.io/version: 2.14.0
          helm.sh/chart: kube-state-metrics-5.28.1
          pod-template-hash: 7f9469bdf4
      spec:
        automountServiceAccountToken: true
        containers:
        - args:
          - --port=8080
          - --resources=certificatesigningrequests,configmaps,cronjobs,daemonsets,deployments,endpoints,horizontalpodautoscalers,ingresses,jobs,leases,limitranges,mutatingwebhookconfigurations,namespaces,networkpolicies,nodes,persistentvolumeclaims,persistentvolumes,poddisruptionbudgets,pods,replicasets,replicationcontrollers,resourcequotas,secrets,services,statefulsets,storageclasses,validatingwebhookconfigurations,volumeattachments
          image: registry.k8s.io/kube-state-metrics/kube-state-metrics:v2.14.0
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /livez
              port: 8080
              scheme: HTTP
            initialDelaySeconds: 5
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          name: kube-state-metrics
          ports:
          - containerPort: 8080
            name: http
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /readyz
              port: 8081
              scheme: HTTP
            initialDelaySeconds: 5
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          resources: {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 65534
          runAsGroup: 65534
          runAsNonRoot: true
          runAsUser: 65534
          seccompProfile:
            type: RuntimeDefault
        serviceAccount: prometheus-kube-state-metrics
        serviceAccountName: prometheus-kube-state-metrics
        terminationGracePeriodSeconds: 30
  status:
    observedGeneration: 1
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "25"
      meta.helm.sh/release-name: prometheus
      meta.helm.sh/release-namespace: monitoring
    creationTimestamp: "2025-02-26T10:32:54Z"
    generation: 1
    labels:
      app.kubernetes.io/component: metrics
      app.kubernetes.io/instance: prometheus
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: kube-state-metrics
      app.kubernetes.io/part-of: kube-state-metrics
      app.kubernetes.io/version: 2.14.0
      helm.sh/chart: kube-state-metrics-5.28.1
      pod-template-hash: 8446c7dbb4
      velero.io/backup-name: monitoring-backup
      velero.io/restore-name: monitoring-backup-20250226103249
    name: prometheus-kube-state-metrics-8446c7dbb4
    namespace: monitoring
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: prometheus-kube-state-metrics
      uid: b10b918b-2a9c-43b7-ab4d-a96a9c8326d4
    resourceVersion: "124670"
    uid: 49dde954-3644-4444-9d99-66cfd52f9d45
  spec:
    replicas: 0
    selector:
      matchLabels:
        app.kubernetes.io/instance: prometheus
        app.kubernetes.io/name: kube-state-metrics
        pod-template-hash: 8446c7dbb4
    template:
      metadata:
        annotations:
          kubectl.kubernetes.io/restartedAt: "2025-02-12T12:39:17Z"
        creationTimestamp: null
        labels:
          app.kubernetes.io/component: metrics
          app.kubernetes.io/instance: prometheus
          app.kubernetes.io/managed-by: Helm
          app.kubernetes.io/name: kube-state-metrics
          app.kubernetes.io/part-of: kube-state-metrics
          app.kubernetes.io/version: 2.14.0
          helm.sh/chart: kube-state-metrics-5.28.1
          pod-template-hash: 8446c7dbb4
      spec:
        automountServiceAccountToken: true
        containers:
        - args:
          - --port=8080
          - --resources=certificatesigningrequests,configmaps,cronjobs,daemonsets,deployments,endpoints,horizontalpodautoscalers,ingresses,jobs,leases,limitranges,mutatingwebhookconfigurations,namespaces,networkpolicies,nodes,persistentvolumeclaims,persistentvolumes,poddisruptionbudgets,pods,replicasets,replicationcontrollers,resourcequotas,secrets,services,statefulsets,storageclasses,validatingwebhookconfigurations,volumeattachments
          image: registry.k8s.io/kube-state-metrics/kube-state-metrics:v2.14.0
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /livez
              port: 8080
              scheme: HTTP
            initialDelaySeconds: 5
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          name: kube-state-metrics
          ports:
          - containerPort: 8080
            name: http
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /readyz
              port: 8081
              scheme: HTTP
            initialDelaySeconds: 5
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          resources: {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 65534
          runAsGroup: 65534
          runAsNonRoot: true
          runAsUser: 65534
          seccompProfile:
            type: RuntimeDefault
        serviceAccount: prometheus-kube-state-metrics
        serviceAccountName: prometheus-kube-state-metrics
        terminationGracePeriodSeconds: 30
  status:
    observedGeneration: 1
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "24"
      meta.helm.sh/release-name: prometheus
      meta.helm.sh/release-namespace: monitoring
    creationTimestamp: "2025-02-26T10:32:55Z"
    generation: 1
    labels:
      app.kubernetes.io/component: metrics
      app.kubernetes.io/instance: prometheus
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: kube-state-metrics
      app.kubernetes.io/part-of: kube-state-metrics
      app.kubernetes.io/version: 2.14.0
      helm.sh/chart: kube-state-metrics-5.28.1
      pod-template-hash: 8476b5ddb8
      velero.io/backup-name: monitoring-backup
      velero.io/restore-name: monitoring-backup-20250226103249
    name: prometheus-kube-state-metrics-8476b5ddb8
    namespace: monitoring
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: prometheus-kube-state-metrics
      uid: b10b918b-2a9c-43b7-ab4d-a96a9c8326d4
    resourceVersion: "124694"
    uid: 6f4b7fad-df52-49ee-ba3a-22e48a99691f
  spec:
    replicas: 0
    selector:
      matchLabels:
        app.kubernetes.io/instance: prometheus
        app.kubernetes.io/name: kube-state-metrics
        pod-template-hash: 8476b5ddb8
    template:
      metadata:
        annotations:
          kubectl.kubernetes.io/restartedAt: "2025-02-12T12:34:18Z"
        creationTimestamp: null
        labels:
          app.kubernetes.io/component: metrics
          app.kubernetes.io/instance: prometheus
          app.kubernetes.io/managed-by: Helm
          app.kubernetes.io/name: kube-state-metrics
          app.kubernetes.io/part-of: kube-state-metrics
          app.kubernetes.io/version: 2.14.0
          helm.sh/chart: kube-state-metrics-5.28.1
          pod-template-hash: 8476b5ddb8
      spec:
        automountServiceAccountToken: true
        containers:
        - args:
          - --port=8080
          - --resources=certificatesigningrequests,configmaps,cronjobs,daemonsets,deployments,endpoints,horizontalpodautoscalers,ingresses,jobs,leases,limitranges,mutatingwebhookconfigurations,namespaces,networkpolicies,nodes,persistentvolumeclaims,persistentvolumes,poddisruptionbudgets,pods,replicasets,replicationcontrollers,resourcequotas,secrets,services,statefulsets,storageclasses,validatingwebhookconfigurations,volumeattachments
          image: registry.k8s.io/kube-state-metrics/kube-state-metrics:v2.14.0
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /livez
              port: 8080
              scheme: HTTP
            initialDelaySeconds: 5
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          name: kube-state-metrics
          ports:
          - containerPort: 8080
            name: http
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /readyz
              port: 8081
              scheme: HTTP
            initialDelaySeconds: 5
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          resources: {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 65534
          runAsGroup: 65534
          runAsNonRoot: true
          runAsUser: 65534
          seccompProfile:
            type: RuntimeDefault
        serviceAccount: prometheus-kube-state-metrics
        serviceAccountName: prometheus-kube-state-metrics
        terminationGracePeriodSeconds: 30
  status:
    observedGeneration: 1
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "19"
      meta.helm.sh/release-name: prometheus
      meta.helm.sh/release-namespace: monitoring
    creationTimestamp: "2025-02-26T10:32:55Z"
    generation: 1
    labels:
      app.kubernetes.io/component: metrics
      app.kubernetes.io/instance: prometheus
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: kube-state-metrics
      app.kubernetes.io/part-of: kube-state-metrics
      app.kubernetes.io/version: 2.14.0
      helm.sh/chart: kube-state-metrics-5.28.1
      pod-template-hash: 85b556d4b9
      velero.io/backup-name: monitoring-backup
      velero.io/restore-name: monitoring-backup-20250226103249
    name: prometheus-kube-state-metrics-85b556d4b9
    namespace: monitoring
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: prometheus-kube-state-metrics
      uid: b10b918b-2a9c-43b7-ab4d-a96a9c8326d4
    resourceVersion: "124709"
    uid: ccafdb01-c81d-4956-a9fd-22e7b0a80aff
  spec:
    replicas: 0
    selector:
      matchLabels:
        app.kubernetes.io/instance: prometheus
        app.kubernetes.io/name: kube-state-metrics
        pod-template-hash: 85b556d4b9
    template:
      metadata:
        annotations:
          kubectl.kubernetes.io/restartedAt: "2025-02-12T11:47:52Z"
        creationTimestamp: null
        labels:
          app.kubernetes.io/component: metrics
          app.kubernetes.io/instance: prometheus
          app.kubernetes.io/managed-by: Helm
          app.kubernetes.io/name: kube-state-metrics
          app.kubernetes.io/part-of: kube-state-metrics
          app.kubernetes.io/version: 2.14.0
          helm.sh/chart: kube-state-metrics-5.28.1
          pod-template-hash: 85b556d4b9
      spec:
        automountServiceAccountToken: true
        containers:
        - args:
          - --port=8080
          - --resources=certificatesigningrequests,configmaps,cronjobs,daemonsets,deployments,endpoints,horizontalpodautoscalers,ingresses,jobs,leases,limitranges,mutatingwebhookconfigurations,namespaces,networkpolicies,nodes,persistentvolumeclaims,persistentvolumes,poddisruptionbudgets,pods,replicasets,replicationcontrollers,resourcequotas,secrets,services,statefulsets,storageclasses,validatingwebhookconfigurations,volumeattachments
          image: registry.k8s.io/kube-state-metrics/kube-state-metrics:v2.14.0
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /livez
              port: 8080
              scheme: HTTP
            initialDelaySeconds: 5
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          name: kube-state-metrics
          ports:
          - containerPort: 8080
            name: http
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /readyz
              port: 8081
              scheme: HTTP
            initialDelaySeconds: 5
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          resources: {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 65534
          runAsGroup: 65534
          runAsNonRoot: true
          runAsUser: 65534
          seccompProfile:
            type: RuntimeDefault
        serviceAccount: prometheus-kube-state-metrics
        serviceAccountName: prometheus-kube-state-metrics
        terminationGracePeriodSeconds: 30
  status:
    observedGeneration: 1
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "22"
      meta.helm.sh/release-name: prometheus
      meta.helm.sh/release-namespace: monitoring
    creationTimestamp: "2025-02-26T10:32:55Z"
    generation: 1
    labels:
      app.kubernetes.io/component: metrics
      app.kubernetes.io/instance: prometheus
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: kube-state-metrics
      app.kubernetes.io/part-of: kube-state-metrics
      app.kubernetes.io/version: 2.14.0
      helm.sh/chart: kube-state-metrics-5.28.1
      pod-template-hash: c9959c888
      velero.io/backup-name: monitoring-backup
      velero.io/restore-name: monitoring-backup-20250226103249
    name: prometheus-kube-state-metrics-c9959c888
    namespace: monitoring
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: prometheus-kube-state-metrics
      uid: b10b918b-2a9c-43b7-ab4d-a96a9c8326d4
    resourceVersion: "124677"
    uid: 967627f8-19a1-4c03-9a44-30fbc4a3ea46
  spec:
    replicas: 0
    selector:
      matchLabels:
        app.kubernetes.io/instance: prometheus
        app.kubernetes.io/name: kube-state-metrics
        pod-template-hash: c9959c888
    template:
      metadata:
        annotations:
          kubectl.kubernetes.io/restartedAt: "2025-02-12T11:51:16Z"
        creationTimestamp: null
        labels:
          app.kubernetes.io/component: metrics
          app.kubernetes.io/instance: prometheus
          app.kubernetes.io/managed-by: Helm
          app.kubernetes.io/name: kube-state-metrics
          app.kubernetes.io/part-of: kube-state-metrics
          app.kubernetes.io/version: 2.14.0
          helm.sh/chart: kube-state-metrics-5.28.1
          pod-template-hash: c9959c888
      spec:
        automountServiceAccountToken: true
        containers:
        - args:
          - --port=8080
          - --resources=certificatesigningrequests,configmaps,cronjobs,daemonsets,deployments,endpoints,horizontalpodautoscalers,ingresses,jobs,leases,limitranges,mutatingwebhookconfigurations,namespaces,networkpolicies,nodes,persistentvolumeclaims,persistentvolumes,poddisruptionbudgets,pods,replicasets,replicationcontrollers,resourcequotas,secrets,services,statefulsets,storageclasses,validatingwebhookconfigurations,volumeattachments
          image: registry.k8s.io/kube-state-metrics/kube-state-metrics:v2.14.0
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /livez
              port: 8080
              scheme: HTTP
            initialDelaySeconds: 5
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          name: kube-state-metrics
          ports:
          - containerPort: 8080
            name: http
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /readyz
              port: 8081
              scheme: HTTP
            initialDelaySeconds: 5
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          resources: {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 65534
          runAsGroup: 65534
          runAsNonRoot: true
          runAsUser: 65534
          seccompProfile:
            type: RuntimeDefault
        serviceAccount: prometheus-kube-state-metrics
        serviceAccountName: prometheus-kube-state-metrics
        terminationGracePeriodSeconds: 30
  status:
    observedGeneration: 1
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "1"
      deployment.kubernetes.io/revision: "23"
      meta.helm.sh/release-name: prometheus
      meta.helm.sh/release-namespace: monitoring
    creationTimestamp: "2025-02-26T10:32:55Z"
    generation: 1
    labels:
      app.kubernetes.io/instance: prometheus
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: prometheus-pushgateway
      app.kubernetes.io/version: v1.11.0
      helm.sh/chart: prometheus-pushgateway-2.17.0
      pod-template-hash: "5677849558"
      velero.io/backup-name: monitoring-backup
      velero.io/restore-name: monitoring-backup-20250226103249
    name: prometheus-prometheus-pushgateway-5677849558
    namespace: monitoring
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: prometheus-prometheus-pushgateway
      uid: d0c6bed6-b65d-4d9b-bac8-04261c7bb2cd
    resourceVersion: "124719"
    uid: fcfb0360-aeb7-457d-9dd7-b8fe713ac983
  spec:
    replicas: 0
    selector:
      matchLabels:
        app.kubernetes.io/instance: prometheus
        app.kubernetes.io/name: prometheus-pushgateway
        pod-template-hash: "5677849558"
    template:
      metadata:
        annotations:
          kubectl.kubernetes.io/restartedAt: "2025-02-12T12:09:01Z"
        creationTimestamp: null
        labels:
          app.kubernetes.io/instance: prometheus
          app.kubernetes.io/managed-by: Helm
          app.kubernetes.io/name: prometheus-pushgateway
          app.kubernetes.io/version: v1.11.0
          helm.sh/chart: prometheus-pushgateway-2.17.0
          pod-template-hash: "5677849558"
      spec:
        automountServiceAccountToken: true
        containers:
        - image: quay.io/prometheus/pushgateway:v1.11.0
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /-/healthy
              port: 9091
              scheme: HTTP
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 10
          name: pushgateway
          ports:
          - containerPort: 9091
            name: metrics
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /-/ready
              port: 9091
              scheme: HTTP
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 10
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /data
            name: storage-volume
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 65534
          runAsNonRoot: true
          runAsUser: 65534
        serviceAccount: prometheus-prometheus-pushgateway
        serviceAccountName: prometheus-prometheus-pushgateway
        terminationGracePeriodSeconds: 30
        volumes:
        - emptyDir: {}
          name: storage-volume
  status:
    observedGeneration: 1
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "1"
      deployment.kubernetes.io/revision: "18"
      meta.helm.sh/release-name: prometheus
      meta.helm.sh/release-namespace: monitoring
    creationTimestamp: "2025-02-26T10:32:55Z"
    generation: 1
    labels:
      app.kubernetes.io/instance: prometheus
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: prometheus-pushgateway
      app.kubernetes.io/version: v1.11.0
      helm.sh/chart: prometheus-pushgateway-2.17.0
      pod-template-hash: 5876485fc
      velero.io/backup-name: monitoring-backup
      velero.io/restore-name: monitoring-backup-20250226103249
    name: prometheus-prometheus-pushgateway-5876485fc
    namespace: monitoring
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: prometheus-prometheus-pushgateway
      uid: d0c6bed6-b65d-4d9b-bac8-04261c7bb2cd
    resourceVersion: "124693"
    uid: 3b667a44-38cd-43de-b516-8f51574c012f
  spec:
    replicas: 0
    selector:
      matchLabels:
        app.kubernetes.io/instance: prometheus
        app.kubernetes.io/name: prometheus-pushgateway
        pod-template-hash: 5876485fc
    template:
      metadata:
        annotations:
          kubectl.kubernetes.io/restartedAt: "2025-02-12T11:33:17Z"
        creationTimestamp: null
        labels:
          app.kubernetes.io/instance: prometheus
          app.kubernetes.io/managed-by: Helm
          app.kubernetes.io/name: prometheus-pushgateway
          app.kubernetes.io/version: v1.11.0
          helm.sh/chart: prometheus-pushgateway-2.17.0
          pod-template-hash: 5876485fc
      spec:
        automountServiceAccountToken: true
        containers:
        - image: quay.io/prometheus/pushgateway:v1.11.0
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /-/healthy
              port: 9091
              scheme: HTTP
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 10
          name: pushgateway
          ports:
          - containerPort: 9091
            name: metrics
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /-/ready
              port: 9091
              scheme: HTTP
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 10
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /data
            name: storage-volume
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 65534
          runAsNonRoot: true
          runAsUser: 65534
        serviceAccount: prometheus-prometheus-pushgateway
        serviceAccountName: prometheus-prometheus-pushgateway
        terminationGracePeriodSeconds: 30
        volumes:
        - emptyDir: {}
          name: storage-volume
  status:
    observedGeneration: 1
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "1"
      deployment.kubernetes.io/revision: "20"
      meta.helm.sh/release-name: prometheus
      meta.helm.sh/release-namespace: monitoring
    creationTimestamp: "2025-02-26T10:32:56Z"
    generation: 1
    labels:
      app.kubernetes.io/instance: prometheus
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: prometheus-pushgateway
      app.kubernetes.io/version: v1.11.0
      helm.sh/chart: prometheus-pushgateway-2.17.0
      pod-template-hash: 5bbf4fd989
      velero.io/backup-name: monitoring-backup
      velero.io/restore-name: monitoring-backup-20250226103249
    name: prometheus-prometheus-pushgateway-5bbf4fd989
    namespace: monitoring
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: prometheus-prometheus-pushgateway
      uid: d0c6bed6-b65d-4d9b-bac8-04261c7bb2cd
    resourceVersion: "124699"
    uid: 67b0079d-2a12-4e6a-a73c-368dabdff8df
  spec:
    replicas: 0
    selector:
      matchLabels:
        app.kubernetes.io/instance: prometheus
        app.kubernetes.io/name: prometheus-pushgateway
        pod-template-hash: 5bbf4fd989
    template:
      metadata:
        annotations:
          kubectl.kubernetes.io/restartedAt: "2025-02-12T11:48:06Z"
        creationTimestamp: null
        labels:
          app.kubernetes.io/instance: prometheus
          app.kubernetes.io/managed-by: Helm
          app.kubernetes.io/name: prometheus-pushgateway
          app.kubernetes.io/version: v1.11.0
          helm.sh/chart: prometheus-pushgateway-2.17.0
          pod-template-hash: 5bbf4fd989
      spec:
        automountServiceAccountToken: true
        containers:
        - image: quay.io/prometheus/pushgateway:v1.11.0
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /-/healthy
              port: 9091
              scheme: HTTP
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 10
          name: pushgateway
          ports:
          - containerPort: 9091
            name: metrics
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /-/ready
              port: 9091
              scheme: HTTP
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 10
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /data
            name: storage-volume
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 65534
          runAsNonRoot: true
          runAsUser: 65534
        serviceAccount: prometheus-prometheus-pushgateway
        serviceAccountName: prometheus-prometheus-pushgateway
        terminationGracePeriodSeconds: 30
        volumes:
        - emptyDir: {}
          name: storage-volume
  status:
    observedGeneration: 1
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "1"
      deployment.kubernetes.io/revision: "27"
      meta.helm.sh/release-name: prometheus
      meta.helm.sh/release-namespace: monitoring
    creationTimestamp: "2025-02-26T10:32:56Z"
    generation: 1
    labels:
      app.kubernetes.io/instance: prometheus
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: prometheus-pushgateway
      app.kubernetes.io/version: v1.11.0
      helm.sh/chart: prometheus-pushgateway-2.17.0
      pod-template-hash: 5cb89857d9
      velero.io/backup-name: monitoring-backup
      velero.io/restore-name: monitoring-backup-20250226103249
    name: prometheus-prometheus-pushgateway-5cb89857d9
    namespace: monitoring
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: prometheus-prometheus-pushgateway
      uid: d0c6bed6-b65d-4d9b-bac8-04261c7bb2cd
    resourceVersion: "124714"
    uid: c6c01f08-b6f8-4895-b17c-7a19d2f61aaf
  spec:
    replicas: 0
    selector:
      matchLabels:
        app.kubernetes.io/instance: prometheus
        app.kubernetes.io/name: prometheus-pushgateway
        pod-template-hash: 5cb89857d9
    template:
      metadata:
        annotations:
          kubectl.kubernetes.io/restartedAt: "2025-02-12T13:04:13Z"
        creationTimestamp: null
        labels:
          app.kubernetes.io/instance: prometheus
          app.kubernetes.io/managed-by: Helm
          app.kubernetes.io/name: prometheus-pushgateway
          app.kubernetes.io/version: v1.11.0
          helm.sh/chart: prometheus-pushgateway-2.17.0
          pod-template-hash: 5cb89857d9
      spec:
        automountServiceAccountToken: true
        containers:
        - image: quay.io/prometheus/pushgateway:v1.11.0
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /-/healthy
              port: 9091
              scheme: HTTP
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 10
          name: pushgateway
          ports:
          - containerPort: 9091
            name: metrics
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /-/ready
              port: 9091
              scheme: HTTP
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 10
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /data
            name: storage-volume
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 65534
          runAsNonRoot: true
          runAsUser: 65534
        serviceAccount: prometheus-prometheus-pushgateway
        serviceAccountName: prometheus-prometheus-pushgateway
        terminationGracePeriodSeconds: 30
        volumes:
        - emptyDir: {}
          name: storage-volume
  status:
    observedGeneration: 1
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "1"
      deployment.kubernetes.io/revision: "22"
      meta.helm.sh/release-name: prometheus
      meta.helm.sh/release-namespace: monitoring
    creationTimestamp: "2025-02-26T10:32:56Z"
    generation: 1
    labels:
      app.kubernetes.io/instance: prometheus
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: prometheus-pushgateway
      app.kubernetes.io/version: v1.11.0
      helm.sh/chart: prometheus-pushgateway-2.17.0
      pod-template-hash: 6cb6b55444
      velero.io/backup-name: monitoring-backup
      velero.io/restore-name: monitoring-backup-20250226103249
    name: prometheus-prometheus-pushgateway-6cb6b55444
    namespace: monitoring
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: prometheus-prometheus-pushgateway
      uid: d0c6bed6-b65d-4d9b-bac8-04261c7bb2cd
    resourceVersion: "124696"
    uid: 9a1a7440-28c2-4396-8645-d21d5dd3e3d0
  spec:
    replicas: 0
    selector:
      matchLabels:
        app.kubernetes.io/instance: prometheus
        app.kubernetes.io/name: prometheus-pushgateway
        pod-template-hash: 6cb6b55444
    template:
      metadata:
        annotations:
          kubectl.kubernetes.io/restartedAt: "2025-02-12T11:51:16Z"
        creationTimestamp: null
        labels:
          app.kubernetes.io/instance: prometheus
          app.kubernetes.io/managed-by: Helm
          app.kubernetes.io/name: prometheus-pushgateway
          app.kubernetes.io/version: v1.11.0
          helm.sh/chart: prometheus-pushgateway-2.17.0
          pod-template-hash: 6cb6b55444
      spec:
        automountServiceAccountToken: true
        containers:
        - image: quay.io/prometheus/pushgateway:v1.11.0
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /-/healthy
              port: 9091
              scheme: HTTP
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 10
          name: pushgateway
          ports:
          - containerPort: 9091
            name: metrics
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /-/ready
              port: 9091
              scheme: HTTP
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 10
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /data
            name: storage-volume
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 65534
          runAsNonRoot: true
          runAsUser: 65534
        serviceAccount: prometheus-prometheus-pushgateway
        serviceAccountName: prometheus-prometheus-pushgateway
        terminationGracePeriodSeconds: 30
        volumes:
        - emptyDir: {}
          name: storage-volume
  status:
    observedGeneration: 1
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "1"
      deployment.kubernetes.io/revision: "25"
      meta.helm.sh/release-name: prometheus
      meta.helm.sh/release-namespace: monitoring
    creationTimestamp: "2025-02-26T10:32:56Z"
    generation: 1
    labels:
      app.kubernetes.io/instance: prometheus
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: prometheus-pushgateway
      app.kubernetes.io/version: v1.11.0
      helm.sh/chart: prometheus-pushgateway-2.17.0
      pod-template-hash: 6cc6b8849b
      velero.io/backup-name: monitoring-backup
      velero.io/restore-name: monitoring-backup-20250226103249
    name: prometheus-prometheus-pushgateway-6cc6b8849b
    namespace: monitoring
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: prometheus-prometheus-pushgateway
      uid: d0c6bed6-b65d-4d9b-bac8-04261c7bb2cd
    resourceVersion: "124724"
    uid: 5c70be4b-5c9b-4e9a-a463-14e05ae55e2c
  spec:
    replicas: 0
    selector:
      matchLabels:
        app.kubernetes.io/instance: prometheus
        app.kubernetes.io/name: prometheus-pushgateway
        pod-template-hash: 6cc6b8849b
    template:
      metadata:
        annotations:
          kubectl.kubernetes.io/restartedAt: "2025-02-12T12:39:17Z"
        creationTimestamp: null
        labels:
          app.kubernetes.io/instance: prometheus
          app.kubernetes.io/managed-by: Helm
          app.kubernetes.io/name: prometheus-pushgateway
          app.kubernetes.io/version: v1.11.0
          helm.sh/chart: prometheus-pushgateway-2.17.0
          pod-template-hash: 6cc6b8849b
      spec:
        automountServiceAccountToken: true
        containers:
        - image: quay.io/prometheus/pushgateway:v1.11.0
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /-/healthy
              port: 9091
              scheme: HTTP
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 10
          name: pushgateway
          ports:
          - containerPort: 9091
            name: metrics
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /-/ready
              port: 9091
              scheme: HTTP
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 10
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /data
            name: storage-volume
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 65534
          runAsNonRoot: true
          runAsUser: 65534
        serviceAccount: prometheus-prometheus-pushgateway
        serviceAccountName: prometheus-prometheus-pushgateway
        terminationGracePeriodSeconds: 30
        volumes:
        - emptyDir: {}
          name: storage-volume
  status:
    observedGeneration: 1
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "1"
      deployment.kubernetes.io/revision: "24"
      meta.helm.sh/release-name: prometheus
      meta.helm.sh/release-namespace: monitoring
    creationTimestamp: "2025-02-26T10:32:56Z"
    generation: 1
    labels:
      app.kubernetes.io/instance: prometheus
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: prometheus-pushgateway
      app.kubernetes.io/version: v1.11.0
      helm.sh/chart: prometheus-pushgateway-2.17.0
      pod-template-hash: 75789bb496
      velero.io/backup-name: monitoring-backup
      velero.io/restore-name: monitoring-backup-20250226103249
    name: prometheus-prometheus-pushgateway-75789bb496
    namespace: monitoring
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: prometheus-prometheus-pushgateway
      uid: d0c6bed6-b65d-4d9b-bac8-04261c7bb2cd
    resourceVersion: "124702"
    uid: b94b2b20-27cf-4d19-b629-55e8c76436c8
  spec:
    replicas: 0
    selector:
      matchLabels:
        app.kubernetes.io/instance: prometheus
        app.kubernetes.io/name: prometheus-pushgateway
        pod-template-hash: 75789bb496
    template:
      metadata:
        annotations:
          kubectl.kubernetes.io/restartedAt: "2025-02-12T12:34:18Z"
        creationTimestamp: null
        labels:
          app.kubernetes.io/instance: prometheus
          app.kubernetes.io/managed-by: Helm
          app.kubernetes.io/name: prometheus-pushgateway
          app.kubernetes.io/version: v1.11.0
          helm.sh/chart: prometheus-pushgateway-2.17.0
          pod-template-hash: 75789bb496
      spec:
        automountServiceAccountToken: true
        containers:
        - image: quay.io/prometheus/pushgateway:v1.11.0
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /-/healthy
              port: 9091
              scheme: HTTP
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 10
          name: pushgateway
          ports:
          - containerPort: 9091
            name: metrics
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /-/ready
              port: 9091
              scheme: HTTP
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 10
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /data
            name: storage-volume
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 65534
          runAsNonRoot: true
          runAsUser: 65534
        serviceAccount: prometheus-prometheus-pushgateway
        serviceAccountName: prometheus-prometheus-pushgateway
        terminationGracePeriodSeconds: 30
        volumes:
        - emptyDir: {}
          name: storage-volume
  status:
    observedGeneration: 1
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "1"
      deployment.kubernetes.io/revision: "19"
      meta.helm.sh/release-name: prometheus
      meta.helm.sh/release-namespace: monitoring
    creationTimestamp: "2025-02-26T10:32:57Z"
    generation: 1
    labels:
      app.kubernetes.io/instance: prometheus
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: prometheus-pushgateway
      app.kubernetes.io/version: v1.11.0
      helm.sh/chart: prometheus-pushgateway-2.17.0
      pod-template-hash: 789bf9ddc
      velero.io/backup-name: monitoring-backup
      velero.io/restore-name: monitoring-backup-20250226103249
    name: prometheus-prometheus-pushgateway-789bf9ddc
    namespace: monitoring
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: prometheus-prometheus-pushgateway
      uid: d0c6bed6-b65d-4d9b-bac8-04261c7bb2cd
    resourceVersion: "124711"
    uid: 6695a97a-5808-42ea-b33f-ccb8e927f27a
  spec:
    replicas: 0
    selector:
      matchLabels:
        app.kubernetes.io/instance: prometheus
        app.kubernetes.io/name: prometheus-pushgateway
        pod-template-hash: 789bf9ddc
    template:
      metadata:
        annotations:
          kubectl.kubernetes.io/restartedAt: "2025-02-12T11:47:52Z"
        creationTimestamp: null
        labels:
          app.kubernetes.io/instance: prometheus
          app.kubernetes.io/managed-by: Helm
          app.kubernetes.io/name: prometheus-pushgateway
          app.kubernetes.io/version: v1.11.0
          helm.sh/chart: prometheus-pushgateway-2.17.0
          pod-template-hash: 789bf9ddc
      spec:
        automountServiceAccountToken: true
        containers:
        - image: quay.io/prometheus/pushgateway:v1.11.0
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /-/healthy
              port: 9091
              scheme: HTTP
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 10
          name: pushgateway
          ports:
          - containerPort: 9091
            name: metrics
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /-/ready
              port: 9091
              scheme: HTTP
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 10
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /data
            name: storage-volume
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 65534
          runAsNonRoot: true
          runAsUser: 65534
        serviceAccount: prometheus-prometheus-pushgateway
        serviceAccountName: prometheus-prometheus-pushgateway
        terminationGracePeriodSeconds: 30
        volumes:
        - emptyDir: {}
          name: storage-volume
  status:
    observedGeneration: 1
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "1"
      deployment.kubernetes.io/revision: "26"
      meta.helm.sh/release-name: prometheus
      meta.helm.sh/release-namespace: monitoring
    creationTimestamp: "2025-02-26T10:32:57Z"
    generation: 1
    labels:
      app.kubernetes.io/instance: prometheus
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: prometheus-pushgateway
      app.kubernetes.io/version: v1.11.0
      helm.sh/chart: prometheus-pushgateway-2.17.0
      pod-template-hash: 85b65fc8fc
      velero.io/backup-name: monitoring-backup
      velero.io/restore-name: monitoring-backup-20250226103249
    name: prometheus-prometheus-pushgateway-85b65fc8fc
    namespace: monitoring
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: prometheus-prometheus-pushgateway
      uid: d0c6bed6-b65d-4d9b-bac8-04261c7bb2cd
    resourceVersion: "124689"
    uid: 5b52186f-2dc7-4a35-af1d-893dce269c68
  spec:
    replicas: 0
    selector:
      matchLabels:
        app.kubernetes.io/instance: prometheus
        app.kubernetes.io/name: prometheus-pushgateway
        pod-template-hash: 85b65fc8fc
    template:
      metadata:
        annotations:
          kubectl.kubernetes.io/restartedAt: "2025-02-12T12:41:42Z"
        creationTimestamp: null
        labels:
          app.kubernetes.io/instance: prometheus
          app.kubernetes.io/managed-by: Helm
          app.kubernetes.io/name: prometheus-pushgateway
          app.kubernetes.io/version: v1.11.0
          helm.sh/chart: prometheus-pushgateway-2.17.0
          pod-template-hash: 85b65fc8fc
      spec:
        automountServiceAccountToken: true
        containers:
        - image: quay.io/prometheus/pushgateway:v1.11.0
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /-/healthy
              port: 9091
              scheme: HTTP
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 10
          name: pushgateway
          ports:
          - containerPort: 9091
            name: metrics
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /-/ready
              port: 9091
              scheme: HTTP
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 10
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /data
            name: storage-volume
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 65534
          runAsNonRoot: true
          runAsUser: 65534
        serviceAccount: prometheus-prometheus-pushgateway
        serviceAccountName: prometheus-prometheus-pushgateway
        terminationGracePeriodSeconds: 30
        volumes:
        - emptyDir: {}
          name: storage-volume
  status:
    observedGeneration: 1
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "1"
      deployment.kubernetes.io/revision: "21"
      meta.helm.sh/release-name: prometheus
      meta.helm.sh/release-namespace: monitoring
    creationTimestamp: "2025-02-26T10:32:57Z"
    generation: 1
    labels:
      app.kubernetes.io/instance: prometheus
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: prometheus-pushgateway
      app.kubernetes.io/version: v1.11.0
      helm.sh/chart: prometheus-pushgateway-2.17.0
      pod-template-hash: 88b76f557
      velero.io/backup-name: monitoring-backup
      velero.io/restore-name: monitoring-backup-20250226103249
    name: prometheus-prometheus-pushgateway-88b76f557
    namespace: monitoring
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: prometheus-prometheus-pushgateway
      uid: d0c6bed6-b65d-4d9b-bac8-04261c7bb2cd
    resourceVersion: "124706"
    uid: 1a83facc-9e8d-47e8-b623-e3432b0f5fea
  spec:
    replicas: 0
    selector:
      matchLabels:
        app.kubernetes.io/instance: prometheus
        app.kubernetes.io/name: prometheus-pushgateway
        pod-template-hash: 88b76f557
    template:
      metadata:
        annotations:
          kubectl.kubernetes.io/restartedAt: "2025-02-12T11:51:13Z"
        creationTimestamp: null
        labels:
          app.kubernetes.io/instance: prometheus
          app.kubernetes.io/managed-by: Helm
          app.kubernetes.io/name: prometheus-pushgateway
          app.kubernetes.io/version: v1.11.0
          helm.sh/chart: prometheus-pushgateway-2.17.0
          pod-template-hash: 88b76f557
      spec:
        automountServiceAccountToken: true
        containers:
        - image: quay.io/prometheus/pushgateway:v1.11.0
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /-/healthy
              port: 9091
              scheme: HTTP
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 10
          name: pushgateway
          ports:
          - containerPort: 9091
            name: metrics
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /-/ready
              port: 9091
              scheme: HTTP
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 10
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /data
            name: storage-volume
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 65534
          runAsNonRoot: true
          runAsUser: 65534
        serviceAccount: prometheus-prometheus-pushgateway
        serviceAccountName: prometheus-prometheus-pushgateway
        terminationGracePeriodSeconds: 30
        volumes:
        - emptyDir: {}
          name: storage-volume
  status:
    observedGeneration: 1
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "1"
      deployment.kubernetes.io/revision: "28"
      meta.helm.sh/release-name: prometheus
      meta.helm.sh/release-namespace: monitoring
    creationTimestamp: "2025-02-26T10:32:57Z"
    generation: 1
    labels:
      app.kubernetes.io/instance: prometheus
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: prometheus-pushgateway
      app.kubernetes.io/version: v1.11.0
      helm.sh/chart: prometheus-pushgateway-2.17.0
      pod-template-hash: d5964d9df
      velero.io/backup-name: monitoring-backup
      velero.io/restore-name: monitoring-backup-20250226103249
    name: prometheus-prometheus-pushgateway-d5964d9df
    namespace: monitoring
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: prometheus-prometheus-pushgateway
      uid: d0c6bed6-b65d-4d9b-bac8-04261c7bb2cd
    resourceVersion: "196236"
    uid: 48d32cf6-583c-45d5-a29b-695c16f1bedf
  spec:
    replicas: 1
    selector:
      matchLabels:
        app.kubernetes.io/instance: prometheus
        app.kubernetes.io/name: prometheus-pushgateway
        pod-template-hash: d5964d9df
    template:
      metadata:
        annotations:
          kubectl.kubernetes.io/restartedAt: "2025-02-13T12:06:57Z"
        creationTimestamp: null
        labels:
          app.kubernetes.io/instance: prometheus
          app.kubernetes.io/managed-by: Helm
          app.kubernetes.io/name: prometheus-pushgateway
          app.kubernetes.io/version: v1.11.0
          helm.sh/chart: prometheus-pushgateway-2.17.0
          pod-template-hash: d5964d9df
      spec:
        automountServiceAccountToken: true
        containers:
        - image: quay.io/prometheus/pushgateway:v1.11.0
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /-/healthy
              port: 9091
              scheme: HTTP
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 10
          name: pushgateway
          ports:
          - containerPort: 9091
            name: metrics
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /-/ready
              port: 9091
              scheme: HTTP
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 10
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /data
            name: storage-volume
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 65534
          runAsNonRoot: true
          runAsUser: 65534
        serviceAccount: prometheus-prometheus-pushgateway
        serviceAccountName: prometheus-prometheus-pushgateway
        terminationGracePeriodSeconds: 30
        volumes:
        - emptyDir: {}
          name: storage-volume
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "1"
      deployment.kubernetes.io/revision: "27"
      field.cattle.io/publicEndpoints: "null"
      meta.helm.sh/release-name: prometheus
      meta.helm.sh/release-namespace: monitoring
    creationTimestamp: "2025-02-26T10:32:57Z"
    generation: 1
    labels:
      app.kubernetes.io/component: server
      app.kubernetes.io/instance: prometheus
      app.kubernetes.io/name: prometheus
      app.kubernetes.io/part-of: prometheus
      app.kubernetes.io/version: v3.1.0
      helm.sh/chart: prometheus-27.3.0
      pod-template-hash: 57f6cffb75
      velero.io/backup-name: monitoring-backup
      velero.io/restore-name: monitoring-backup-20250226103249
    name: prometheus-server-57f6cffb75
    namespace: monitoring
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: prometheus-server
      uid: 9811a1b1-4c9b-43d9-82a8-2f566b280ff0
    resourceVersion: "124731"
    uid: 5e886e71-eaa0-4c20-b013-6924597d926a
  spec:
    replicas: 0
    selector:
      matchLabels:
        app.kubernetes.io/component: server
        app.kubernetes.io/instance: prometheus
        app.kubernetes.io/name: prometheus
        pod-template-hash: 57f6cffb75
    template:
      metadata:
        annotations:
          kubectl.kubernetes.io/restartedAt: "2025-02-12T12:41:42Z"
        creationTimestamp: null
        labels:
          app.kubernetes.io/component: server
          app.kubernetes.io/instance: prometheus
          app.kubernetes.io/name: prometheus
          app.kubernetes.io/part-of: prometheus
          app.kubernetes.io/version: v3.1.0
          helm.sh/chart: prometheus-27.3.0
          pod-template-hash: 57f6cffb75
      spec:
        containers:
        - args:
          - --watched-dir=/etc/config
          - --listen-address=0.0.0.0:8080
          - --reload-url=http://127.0.0.1:9090/-/reload
          image: quay.io/prometheus-operator/prometheus-config-reloader:v0.79.2
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /healthz
              port: metrics
              scheme: HTTP
            initialDelaySeconds: 2
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          name: prometheus-server-configmap-reload
          ports:
          - containerPort: 8080
            name: metrics
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /healthz
              port: metrics
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/config
            name: config-volume
            readOnly: true
        - args:
          - --storage.tsdb.retention.time=15d
          - --config.file=/etc/config/prometheus.yml
          - --storage.tsdb.path=/data
          - --web.console.libraries=/etc/prometheus/console_libraries
          - --web.console.templates=/etc/prometheus/consoles
          - --web.enable-lifecycle
          image: quay.io/prometheus/prometheus:v3.1.0
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /-/healthy
              port: 9090
              scheme: HTTP
            initialDelaySeconds: 30
            periodSeconds: 15
            successThreshold: 1
            timeoutSeconds: 10
          name: prometheus-server
          ports:
          - containerPort: 9090
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /-/ready
              port: 9090
              scheme: HTTP
            initialDelaySeconds: 30
            periodSeconds: 5
            successThreshold: 1
            timeoutSeconds: 4
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/config
            name: config-volume
          - mountPath: /data
            name: storage-volume
        dnsPolicy: ClusterFirst
        enableServiceLinks: true
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 65534
          runAsGroup: 65534
          runAsNonRoot: true
          runAsUser: 65534
        serviceAccount: prometheus-server
        serviceAccountName: prometheus-server
        terminationGracePeriodSeconds: 300
        volumes:
        - configMap:
            defaultMode: 420
            name: prometheus-server
          name: config-volume
        - name: storage-volume
          persistentVolumeClaim:
            claimName: prometheus-server
  status:
    observedGeneration: 1
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "1"
      deployment.kubernetes.io/revision: "26"
      field.cattle.io/publicEndpoints: "null"
      meta.helm.sh/release-name: prometheus
      meta.helm.sh/release-namespace: monitoring
    creationTimestamp: "2025-02-26T10:32:57Z"
    generation: 1
    labels:
      app.kubernetes.io/component: server
      app.kubernetes.io/instance: prometheus
      app.kubernetes.io/name: prometheus
      app.kubernetes.io/part-of: prometheus
      app.kubernetes.io/version: v3.1.0
      helm.sh/chart: prometheus-27.3.0
      pod-template-hash: 58c7b7f7d8
      velero.io/backup-name: monitoring-backup
      velero.io/restore-name: monitoring-backup-20250226103249
    name: prometheus-server-58c7b7f7d8
    namespace: monitoring
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: prometheus-server
      uid: 9811a1b1-4c9b-43d9-82a8-2f566b280ff0
    resourceVersion: "124708"
    uid: b6989e34-67ca-4439-ae09-7ed868c7588f
  spec:
    replicas: 0
    selector:
      matchLabels:
        app.kubernetes.io/component: server
        app.kubernetes.io/instance: prometheus
        app.kubernetes.io/name: prometheus
        pod-template-hash: 58c7b7f7d8
    template:
      metadata:
        annotations:
          kubectl.kubernetes.io/restartedAt: "2025-02-12T12:39:17Z"
        creationTimestamp: null
        labels:
          app.kubernetes.io/component: server
          app.kubernetes.io/instance: prometheus
          app.kubernetes.io/name: prometheus
          app.kubernetes.io/part-of: prometheus
          app.kubernetes.io/version: v3.1.0
          helm.sh/chart: prometheus-27.3.0
          pod-template-hash: 58c7b7f7d8
      spec:
        containers:
        - args:
          - --watched-dir=/etc/config
          - --listen-address=0.0.0.0:8080
          - --reload-url=http://127.0.0.1:9090/-/reload
          image: quay.io/prometheus-operator/prometheus-config-reloader:v0.79.2
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /healthz
              port: metrics
              scheme: HTTP
            initialDelaySeconds: 2
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          name: prometheus-server-configmap-reload
          ports:
          - containerPort: 8080
            name: metrics
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /healthz
              port: metrics
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/config
            name: config-volume
            readOnly: true
        - args:
          - --storage.tsdb.retention.time=15d
          - --config.file=/etc/config/prometheus.yml
          - --storage.tsdb.path=/data
          - --web.console.libraries=/etc/prometheus/console_libraries
          - --web.console.templates=/etc/prometheus/consoles
          - --web.enable-lifecycle
          image: quay.io/prometheus/prometheus:v3.1.0
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /-/healthy
              port: 9090
              scheme: HTTP
            initialDelaySeconds: 30
            periodSeconds: 15
            successThreshold: 1
            timeoutSeconds: 10
          name: prometheus-server
          ports:
          - containerPort: 9090
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /-/ready
              port: 9090
              scheme: HTTP
            initialDelaySeconds: 30
            periodSeconds: 5
            successThreshold: 1
            timeoutSeconds: 4
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/config
            name: config-volume
          - mountPath: /data
            name: storage-volume
        dnsPolicy: ClusterFirst
        enableServiceLinks: true
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 65534
          runAsGroup: 65534
          runAsNonRoot: true
          runAsUser: 65534
        serviceAccount: prometheus-server
        serviceAccountName: prometheus-server
        terminationGracePeriodSeconds: 300
        volumes:
        - configMap:
            defaultMode: 420
            name: prometheus-server
          name: config-volume
        - name: storage-volume
          persistentVolumeClaim:
            claimName: prometheus-server
  status:
    observedGeneration: 1
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "1"
      deployment.kubernetes.io/revision: "19"
      field.cattle.io/publicEndpoints: "null"
      meta.helm.sh/release-name: prometheus
      meta.helm.sh/release-namespace: monitoring
    creationTimestamp: "2025-02-26T10:32:57Z"
    generation: 1
    labels:
      app.kubernetes.io/component: server
      app.kubernetes.io/instance: prometheus
      app.kubernetes.io/name: prometheus
      app.kubernetes.io/part-of: prometheus
      app.kubernetes.io/version: v3.1.0
      helm.sh/chart: prometheus-27.3.0
      pod-template-hash: 5b6ff76495
      velero.io/backup-name: monitoring-backup
      velero.io/restore-name: monitoring-backup-20250226103249
    name: prometheus-server-5b6ff76495
    namespace: monitoring
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: prometheus-server
      uid: 9811a1b1-4c9b-43d9-82a8-2f566b280ff0
    resourceVersion: "124705"
    uid: 878d2f37-3a5b-48ee-bcc8-80910efededd
  spec:
    replicas: 0
    selector:
      matchLabels:
        app.kubernetes.io/component: server
        app.kubernetes.io/instance: prometheus
        app.kubernetes.io/name: prometheus
        pod-template-hash: 5b6ff76495
    template:
      metadata:
        annotations:
          kubectl.kubernetes.io/restartedAt: "2025-02-12T11:33:17Z"
        creationTimestamp: null
        labels:
          app.kubernetes.io/component: server
          app.kubernetes.io/instance: prometheus
          app.kubernetes.io/name: prometheus
          app.kubernetes.io/part-of: prometheus
          app.kubernetes.io/version: v3.1.0
          helm.sh/chart: prometheus-27.3.0
          pod-template-hash: 5b6ff76495
      spec:
        containers:
        - args:
          - --watched-dir=/etc/config
          - --listen-address=0.0.0.0:8080
          - --reload-url=http://127.0.0.1:9090/-/reload
          image: quay.io/prometheus-operator/prometheus-config-reloader:v0.79.2
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /healthz
              port: metrics
              scheme: HTTP
            initialDelaySeconds: 2
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          name: prometheus-server-configmap-reload
          ports:
          - containerPort: 8080
            name: metrics
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /healthz
              port: metrics
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/config
            name: config-volume
            readOnly: true
        - args:
          - --storage.tsdb.retention.time=15d
          - --config.file=/etc/config/prometheus.yml
          - --storage.tsdb.path=/data
          - --web.console.libraries=/etc/prometheus/console_libraries
          - --web.console.templates=/etc/prometheus/consoles
          - --web.enable-lifecycle
          image: quay.io/prometheus/prometheus:v3.1.0
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /-/healthy
              port: 9090
              scheme: HTTP
            initialDelaySeconds: 30
            periodSeconds: 15
            successThreshold: 1
            timeoutSeconds: 10
          name: prometheus-server
          ports:
          - containerPort: 9090
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /-/ready
              port: 9090
              scheme: HTTP
            initialDelaySeconds: 30
            periodSeconds: 5
            successThreshold: 1
            timeoutSeconds: 4
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/config
            name: config-volume
          - mountPath: /data
            name: storage-volume
        dnsPolicy: ClusterFirst
        enableServiceLinks: true
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 65534
          runAsGroup: 65534
          runAsNonRoot: true
          runAsUser: 65534
        serviceAccount: prometheus-server
        serviceAccountName: prometheus-server
        terminationGracePeriodSeconds: 300
        volumes:
        - configMap:
            defaultMode: 420
            name: prometheus-server
          name: config-volume
        - name: storage-volume
          persistentVolumeClaim:
            claimName: prometheus-server
  status:
    observedGeneration: 1
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "1"
      deployment.kubernetes.io/revision: "20"
      field.cattle.io/publicEndpoints: "null"
      meta.helm.sh/release-name: prometheus
      meta.helm.sh/release-namespace: monitoring
    creationTimestamp: "2025-02-26T10:32:57Z"
    generation: 1
    labels:
      app.kubernetes.io/component: server
      app.kubernetes.io/instance: prometheus
      app.kubernetes.io/name: prometheus
      app.kubernetes.io/part-of: prometheus
      app.kubernetes.io/version: v3.1.0
      helm.sh/chart: prometheus-27.3.0
      pod-template-hash: 64d6c77c5c
      velero.io/backup-name: monitoring-backup
      velero.io/restore-name: monitoring-backup-20250226103249
    name: prometheus-server-64d6c77c5c
    namespace: monitoring
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: prometheus-server
      uid: 9811a1b1-4c9b-43d9-82a8-2f566b280ff0
    resourceVersion: "124738"
    uid: 0078dfb5-8347-4735-b9db-5de53b763ef4
  spec:
    replicas: 0
    selector:
      matchLabels:
        app.kubernetes.io/component: server
        app.kubernetes.io/instance: prometheus
        app.kubernetes.io/name: prometheus
        pod-template-hash: 64d6c77c5c
    template:
      metadata:
        annotations:
          kubectl.kubernetes.io/restartedAt: "2025-02-12T11:47:52Z"
        creationTimestamp: null
        labels:
          app.kubernetes.io/component: server
          app.kubernetes.io/instance: prometheus
          app.kubernetes.io/name: prometheus
          app.kubernetes.io/part-of: prometheus
          app.kubernetes.io/version: v3.1.0
          helm.sh/chart: prometheus-27.3.0
          pod-template-hash: 64d6c77c5c
      spec:
        containers:
        - args:
          - --watched-dir=/etc/config
          - --listen-address=0.0.0.0:8080
          - --reload-url=http://127.0.0.1:9090/-/reload
          image: quay.io/prometheus-operator/prometheus-config-reloader:v0.79.2
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /healthz
              port: metrics
              scheme: HTTP
            initialDelaySeconds: 2
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          name: prometheus-server-configmap-reload
          ports:
          - containerPort: 8080
            name: metrics
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /healthz
              port: metrics
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/config
            name: config-volume
            readOnly: true
        - args:
          - --storage.tsdb.retention.time=15d
          - --config.file=/etc/config/prometheus.yml
          - --storage.tsdb.path=/data
          - --web.console.libraries=/etc/prometheus/console_libraries
          - --web.console.templates=/etc/prometheus/consoles
          - --web.enable-lifecycle
          image: quay.io/prometheus/prometheus:v3.1.0
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /-/healthy
              port: 9090
              scheme: HTTP
            initialDelaySeconds: 30
            periodSeconds: 15
            successThreshold: 1
            timeoutSeconds: 10
          name: prometheus-server
          ports:
          - containerPort: 9090
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /-/ready
              port: 9090
              scheme: HTTP
            initialDelaySeconds: 30
            periodSeconds: 5
            successThreshold: 1
            timeoutSeconds: 4
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/config
            name: config-volume
          - mountPath: /data
            name: storage-volume
        dnsPolicy: ClusterFirst
        enableServiceLinks: true
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 65534
          runAsGroup: 65534
          runAsNonRoot: true
          runAsUser: 65534
        serviceAccount: prometheus-server
        serviceAccountName: prometheus-server
        terminationGracePeriodSeconds: 300
        volumes:
        - configMap:
            defaultMode: 420
            name: prometheus-server
          name: config-volume
        - name: storage-volume
          persistentVolumeClaim:
            claimName: prometheus-server
  status:
    observedGeneration: 1
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "1"
      deployment.kubernetes.io/revision: "22"
      field.cattle.io/publicEndpoints: "null"
      meta.helm.sh/release-name: prometheus
      meta.helm.sh/release-namespace: monitoring
    creationTimestamp: "2025-02-26T10:32:57Z"
    generation: 1
    labels:
      app.kubernetes.io/component: server
      app.kubernetes.io/instance: prometheus
      app.kubernetes.io/name: prometheus
      app.kubernetes.io/part-of: prometheus
      app.kubernetes.io/version: v3.1.0
      helm.sh/chart: prometheus-27.3.0
      pod-template-hash: 698d7b9887
      velero.io/backup-name: monitoring-backup
      velero.io/restore-name: monitoring-backup-20250226103249
    name: prometheus-server-698d7b9887
    namespace: monitoring
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: prometheus-server
      uid: 9811a1b1-4c9b-43d9-82a8-2f566b280ff0
    resourceVersion: "124716"
    uid: 6829e7f2-1245-4536-bfea-c409c55f17d2
  spec:
    replicas: 0
    selector:
      matchLabels:
        app.kubernetes.io/component: server
        app.kubernetes.io/instance: prometheus
        app.kubernetes.io/name: prometheus
        pod-template-hash: 698d7b9887
    template:
      metadata:
        annotations:
          kubectl.kubernetes.io/restartedAt: "2025-02-12T11:51:13Z"
        creationTimestamp: null
        labels:
          app.kubernetes.io/component: server
          app.kubernetes.io/instance: prometheus
          app.kubernetes.io/name: prometheus
          app.kubernetes.io/part-of: prometheus
          app.kubernetes.io/version: v3.1.0
          helm.sh/chart: prometheus-27.3.0
          pod-template-hash: 698d7b9887
      spec:
        containers:
        - args:
          - --watched-dir=/etc/config
          - --listen-address=0.0.0.0:8080
          - --reload-url=http://127.0.0.1:9090/-/reload
          image: quay.io/prometheus-operator/prometheus-config-reloader:v0.79.2
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /healthz
              port: metrics
              scheme: HTTP
            initialDelaySeconds: 2
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          name: prometheus-server-configmap-reload
          ports:
          - containerPort: 8080
            name: metrics
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /healthz
              port: metrics
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/config
            name: config-volume
            readOnly: true
        - args:
          - --storage.tsdb.retention.time=15d
          - --config.file=/etc/config/prometheus.yml
          - --storage.tsdb.path=/data
          - --web.console.libraries=/etc/prometheus/console_libraries
          - --web.console.templates=/etc/prometheus/consoles
          - --web.enable-lifecycle
          image: quay.io/prometheus/prometheus:v3.1.0
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /-/healthy
              port: 9090
              scheme: HTTP
            initialDelaySeconds: 30
            periodSeconds: 15
            successThreshold: 1
            timeoutSeconds: 10
          name: prometheus-server
          ports:
          - containerPort: 9090
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /-/ready
              port: 9090
              scheme: HTTP
            initialDelaySeconds: 30
            periodSeconds: 5
            successThreshold: 1
            timeoutSeconds: 4
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/config
            name: config-volume
          - mountPath: /data
            name: storage-volume
        dnsPolicy: ClusterFirst
        enableServiceLinks: true
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 65534
          runAsGroup: 65534
          runAsNonRoot: true
          runAsUser: 65534
        serviceAccount: prometheus-server
        serviceAccountName: prometheus-server
        terminationGracePeriodSeconds: 300
        volumes:
        - configMap:
            defaultMode: 420
            name: prometheus-server
          name: config-volume
        - name: storage-volume
          persistentVolumeClaim:
            claimName: prometheus-server
  status:
    observedGeneration: 1
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "1"
      deployment.kubernetes.io/revision: "21"
      field.cattle.io/publicEndpoints: "null"
      meta.helm.sh/release-name: prometheus
      meta.helm.sh/release-namespace: monitoring
    creationTimestamp: "2025-02-26T10:32:58Z"
    generation: 1
    labels:
      app.kubernetes.io/component: server
      app.kubernetes.io/instance: prometheus
      app.kubernetes.io/name: prometheus
      app.kubernetes.io/part-of: prometheus
      app.kubernetes.io/version: v3.1.0
      helm.sh/chart: prometheus-27.3.0
      pod-template-hash: 6c89695999
      velero.io/backup-name: monitoring-backup
      velero.io/restore-name: monitoring-backup-20250226103249
    name: prometheus-server-6c89695999
    namespace: monitoring
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: prometheus-server
      uid: 9811a1b1-4c9b-43d9-82a8-2f566b280ff0
    resourceVersion: "124737"
    uid: ebe3f6bd-2338-430d-9605-3aa6244409ab
  spec:
    replicas: 0
    selector:
      matchLabels:
        app.kubernetes.io/component: server
        app.kubernetes.io/instance: prometheus
        app.kubernetes.io/name: prometheus
        pod-template-hash: 6c89695999
    template:
      metadata:
        annotations:
          kubectl.kubernetes.io/restartedAt: "2025-02-12T11:48:06Z"
        creationTimestamp: null
        labels:
          app.kubernetes.io/component: server
          app.kubernetes.io/instance: prometheus
          app.kubernetes.io/name: prometheus
          app.kubernetes.io/part-of: prometheus
          app.kubernetes.io/version: v3.1.0
          helm.sh/chart: prometheus-27.3.0
          pod-template-hash: 6c89695999
      spec:
        containers:
        - args:
          - --watched-dir=/etc/config
          - --listen-address=0.0.0.0:8080
          - --reload-url=http://127.0.0.1:9090/-/reload
          image: quay.io/prometheus-operator/prometheus-config-reloader:v0.79.2
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /healthz
              port: metrics
              scheme: HTTP
            initialDelaySeconds: 2
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          name: prometheus-server-configmap-reload
          ports:
          - containerPort: 8080
            name: metrics
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /healthz
              port: metrics
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/config
            name: config-volume
            readOnly: true
        - args:
          - --storage.tsdb.retention.time=15d
          - --config.file=/etc/config/prometheus.yml
          - --storage.tsdb.path=/data
          - --web.console.libraries=/etc/prometheus/console_libraries
          - --web.console.templates=/etc/prometheus/consoles
          - --web.enable-lifecycle
          image: quay.io/prometheus/prometheus:v3.1.0
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /-/healthy
              port: 9090
              scheme: HTTP
            initialDelaySeconds: 30
            periodSeconds: 15
            successThreshold: 1
            timeoutSeconds: 10
          name: prometheus-server
          ports:
          - containerPort: 9090
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /-/ready
              port: 9090
              scheme: HTTP
            initialDelaySeconds: 30
            periodSeconds: 5
            successThreshold: 1
            timeoutSeconds: 4
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/config
            name: config-volume
          - mountPath: /data
            name: storage-volume
        dnsPolicy: ClusterFirst
        enableServiceLinks: true
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 65534
          runAsGroup: 65534
          runAsNonRoot: true
          runAsUser: 65534
        serviceAccount: prometheus-server
        serviceAccountName: prometheus-server
        terminationGracePeriodSeconds: 300
        volumes:
        - configMap:
            defaultMode: 420
            name: prometheus-server
          name: config-volume
        - name: storage-volume
          persistentVolumeClaim:
            claimName: prometheus-server
  status:
    observedGeneration: 1
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "1"
      deployment.kubernetes.io/revision: "23"
      field.cattle.io/publicEndpoints: "null"
      meta.helm.sh/release-name: prometheus
      meta.helm.sh/release-namespace: monitoring
    creationTimestamp: "2025-02-26T10:32:58Z"
    generation: 1
    labels:
      app.kubernetes.io/component: server
      app.kubernetes.io/instance: prometheus
      app.kubernetes.io/name: prometheus
      app.kubernetes.io/part-of: prometheus
      app.kubernetes.io/version: v3.1.0
      helm.sh/chart: prometheus-27.3.0
      pod-template-hash: 7b5bc84cbd
      velero.io/backup-name: monitoring-backup
      velero.io/restore-name: monitoring-backup-20250226103249
    name: prometheus-server-7b5bc84cbd
    namespace: monitoring
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: prometheus-server
      uid: 9811a1b1-4c9b-43d9-82a8-2f566b280ff0
    resourceVersion: "124720"
    uid: f068558f-1e9a-4fb8-b2e4-10e997d011eb
  spec:
    replicas: 0
    selector:
      matchLabels:
        app.kubernetes.io/component: server
        app.kubernetes.io/instance: prometheus
        app.kubernetes.io/name: prometheus
        pod-template-hash: 7b5bc84cbd
    template:
      metadata:
        annotations:
          kubectl.kubernetes.io/restartedAt: "2025-02-12T11:51:16Z"
        creationTimestamp: null
        labels:
          app.kubernetes.io/component: server
          app.kubernetes.io/instance: prometheus
          app.kubernetes.io/name: prometheus
          app.kubernetes.io/part-of: prometheus
          app.kubernetes.io/version: v3.1.0
          helm.sh/chart: prometheus-27.3.0
          pod-template-hash: 7b5bc84cbd
      spec:
        containers:
        - args:
          - --watched-dir=/etc/config
          - --listen-address=0.0.0.0:8080
          - --reload-url=http://127.0.0.1:9090/-/reload
          image: quay.io/prometheus-operator/prometheus-config-reloader:v0.79.2
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /healthz
              port: metrics
              scheme: HTTP
            initialDelaySeconds: 2
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          name: prometheus-server-configmap-reload
          ports:
          - containerPort: 8080
            name: metrics
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /healthz
              port: metrics
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/config
            name: config-volume
            readOnly: true
        - args:
          - --storage.tsdb.retention.time=15d
          - --config.file=/etc/config/prometheus.yml
          - --storage.tsdb.path=/data
          - --web.console.libraries=/etc/prometheus/console_libraries
          - --web.console.templates=/etc/prometheus/consoles
          - --web.enable-lifecycle
          image: quay.io/prometheus/prometheus:v3.1.0
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /-/healthy
              port: 9090
              scheme: HTTP
            initialDelaySeconds: 30
            periodSeconds: 15
            successThreshold: 1
            timeoutSeconds: 10
          name: prometheus-server
          ports:
          - containerPort: 9090
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /-/ready
              port: 9090
              scheme: HTTP
            initialDelaySeconds: 30
            periodSeconds: 5
            successThreshold: 1
            timeoutSeconds: 4
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/config
            name: config-volume
          - mountPath: /data
            name: storage-volume
        dnsPolicy: ClusterFirst
        enableServiceLinks: true
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 65534
          runAsGroup: 65534
          runAsNonRoot: true
          runAsUser: 65534
        serviceAccount: prometheus-server
        serviceAccountName: prometheus-server
        terminationGracePeriodSeconds: 300
        volumes:
        - configMap:
            defaultMode: 420
            name: prometheus-server
          name: config-volume
        - name: storage-volume
          persistentVolumeClaim:
            claimName: prometheus-server
  status:
    observedGeneration: 1
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "1"
      deployment.kubernetes.io/revision: "25"
      field.cattle.io/publicEndpoints: "null"
      meta.helm.sh/release-name: prometheus
      meta.helm.sh/release-namespace: monitoring
    creationTimestamp: "2025-02-26T10:32:58Z"
    generation: 1
    labels:
      app.kubernetes.io/component: server
      app.kubernetes.io/instance: prometheus
      app.kubernetes.io/name: prometheus
      app.kubernetes.io/part-of: prometheus
      app.kubernetes.io/version: v3.1.0
      helm.sh/chart: prometheus-27.3.0
      pod-template-hash: 7b7f6fcd44
      velero.io/backup-name: monitoring-backup
      velero.io/restore-name: monitoring-backup-20250226103249
    name: prometheus-server-7b7f6fcd44
    namespace: monitoring
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: prometheus-server
      uid: 9811a1b1-4c9b-43d9-82a8-2f566b280ff0
    resourceVersion: "124713"
    uid: dd099944-a025-4fe5-9dc0-beec7e096dad
  spec:
    replicas: 0
    selector:
      matchLabels:
        app.kubernetes.io/component: server
        app.kubernetes.io/instance: prometheus
        app.kubernetes.io/name: prometheus
        pod-template-hash: 7b7f6fcd44
    template:
      metadata:
        annotations:
          kubectl.kubernetes.io/restartedAt: "2025-02-12T12:34:18Z"
        creationTimestamp: null
        labels:
          app.kubernetes.io/component: server
          app.kubernetes.io/instance: prometheus
          app.kubernetes.io/name: prometheus
          app.kubernetes.io/part-of: prometheus
          app.kubernetes.io/version: v3.1.0
          helm.sh/chart: prometheus-27.3.0
          pod-template-hash: 7b7f6fcd44
      spec:
        containers:
        - args:
          - --watched-dir=/etc/config
          - --listen-address=0.0.0.0:8080
          - --reload-url=http://127.0.0.1:9090/-/reload
          image: quay.io/prometheus-operator/prometheus-config-reloader:v0.79.2
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /healthz
              port: metrics
              scheme: HTTP
            initialDelaySeconds: 2
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          name: prometheus-server-configmap-reload
          ports:
          - containerPort: 8080
            name: metrics
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /healthz
              port: metrics
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/config
            name: config-volume
            readOnly: true
        - args:
          - --storage.tsdb.retention.time=15d
          - --config.file=/etc/config/prometheus.yml
          - --storage.tsdb.path=/data
          - --web.console.libraries=/etc/prometheus/console_libraries
          - --web.console.templates=/etc/prometheus/consoles
          - --web.enable-lifecycle
          image: quay.io/prometheus/prometheus:v3.1.0
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /-/healthy
              port: 9090
              scheme: HTTP
            initialDelaySeconds: 30
            periodSeconds: 15
            successThreshold: 1
            timeoutSeconds: 10
          name: prometheus-server
          ports:
          - containerPort: 9090
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /-/ready
              port: 9090
              scheme: HTTP
            initialDelaySeconds: 30
            periodSeconds: 5
            successThreshold: 1
            timeoutSeconds: 4
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/config
            name: config-volume
          - mountPath: /data
            name: storage-volume
        dnsPolicy: ClusterFirst
        enableServiceLinks: true
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 65534
          runAsGroup: 65534
          runAsNonRoot: true
          runAsUser: 65534
        serviceAccount: prometheus-server
        serviceAccountName: prometheus-server
        terminationGracePeriodSeconds: 300
        volumes:
        - configMap:
            defaultMode: 420
            name: prometheus-server
          name: config-volume
        - name: storage-volume
          persistentVolumeClaim:
            claimName: prometheus-server
  status:
    observedGeneration: 1
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "1"
      deployment.kubernetes.io/revision: "28"
      field.cattle.io/publicEndpoints: "null"
      meta.helm.sh/release-name: prometheus
      meta.helm.sh/release-namespace: monitoring
    creationTimestamp: "2025-02-26T10:32:58Z"
    generation: 1
    labels:
      app.kubernetes.io/component: server
      app.kubernetes.io/instance: prometheus
      app.kubernetes.io/name: prometheus
      app.kubernetes.io/part-of: prometheus
      app.kubernetes.io/version: v3.1.0
      helm.sh/chart: prometheus-27.3.0
      pod-template-hash: 7bcffd9c9f
      velero.io/backup-name: monitoring-backup
      velero.io/restore-name: monitoring-backup-20250226103249
    name: prometheus-server-7bcffd9c9f
    namespace: monitoring
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: prometheus-server
      uid: 9811a1b1-4c9b-43d9-82a8-2f566b280ff0
    resourceVersion: "124698"
    uid: b225f6fa-01aa-4837-ac89-71e952366b0e
  spec:
    replicas: 0
    selector:
      matchLabels:
        app.kubernetes.io/component: server
        app.kubernetes.io/instance: prometheus
        app.kubernetes.io/name: prometheus
        pod-template-hash: 7bcffd9c9f
    template:
      metadata:
        annotations:
          kubectl.kubernetes.io/restartedAt: "2025-02-12T13:04:13Z"
        creationTimestamp: null
        labels:
          app.kubernetes.io/component: server
          app.kubernetes.io/instance: prometheus
          app.kubernetes.io/name: prometheus
          app.kubernetes.io/part-of: prometheus
          app.kubernetes.io/version: v3.1.0
          helm.sh/chart: prometheus-27.3.0
          pod-template-hash: 7bcffd9c9f
      spec:
        containers:
        - args:
          - --watched-dir=/etc/config
          - --listen-address=0.0.0.0:8080
          - --reload-url=http://127.0.0.1:9090/-/reload
          image: quay.io/prometheus-operator/prometheus-config-reloader:v0.79.2
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /healthz
              port: metrics
              scheme: HTTP
            initialDelaySeconds: 2
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          name: prometheus-server-configmap-reload
          ports:
          - containerPort: 8080
            name: metrics
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /healthz
              port: metrics
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/config
            name: config-volume
            readOnly: true
        - args:
          - --storage.tsdb.retention.time=15d
          - --config.file=/etc/config/prometheus.yml
          - --storage.tsdb.path=/data
          - --web.console.libraries=/etc/prometheus/console_libraries
          - --web.console.templates=/etc/prometheus/consoles
          - --web.enable-lifecycle
          image: quay.io/prometheus/prometheus:v3.1.0
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /-/healthy
              port: 9090
              scheme: HTTP
            initialDelaySeconds: 30
            periodSeconds: 15
            successThreshold: 1
            timeoutSeconds: 10
          name: prometheus-server
          ports:
          - containerPort: 9090
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /-/ready
              port: 9090
              scheme: HTTP
            initialDelaySeconds: 30
            periodSeconds: 5
            successThreshold: 1
            timeoutSeconds: 4
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/config
            name: config-volume
          - mountPath: /data
            name: storage-volume
        dnsPolicy: ClusterFirst
        enableServiceLinks: true
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 65534
          runAsGroup: 65534
          runAsNonRoot: true
          runAsUser: 65534
        serviceAccount: prometheus-server
        serviceAccountName: prometheus-server
        terminationGracePeriodSeconds: 300
        volumes:
        - configMap:
            defaultMode: 420
            name: prometheus-server
          name: config-volume
        - name: storage-volume
          persistentVolumeClaim:
            claimName: prometheus-server
  status:
    observedGeneration: 1
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "1"
      deployment.kubernetes.io/revision: "24"
      field.cattle.io/publicEndpoints: "null"
      meta.helm.sh/release-name: prometheus
      meta.helm.sh/release-namespace: monitoring
    creationTimestamp: "2025-02-26T10:32:59Z"
    generation: 1
    labels:
      app.kubernetes.io/component: server
      app.kubernetes.io/instance: prometheus
      app.kubernetes.io/name: prometheus
      app.kubernetes.io/part-of: prometheus
      app.kubernetes.io/version: v3.1.0
      helm.sh/chart: prometheus-27.3.0
      pod-template-hash: 9b658cb66
      velero.io/backup-name: monitoring-backup
      velero.io/restore-name: monitoring-backup-20250226103249
    name: prometheus-server-9b658cb66
    namespace: monitoring
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: prometheus-server
      uid: 9811a1b1-4c9b-43d9-82a8-2f566b280ff0
    resourceVersion: "124700"
    uid: 9865207f-61bc-4ead-aeb0-2d85a38de33d
  spec:
    replicas: 0
    selector:
      matchLabels:
        app.kubernetes.io/component: server
        app.kubernetes.io/instance: prometheus
        app.kubernetes.io/name: prometheus
        pod-template-hash: 9b658cb66
    template:
      metadata:
        annotations:
          kubectl.kubernetes.io/restartedAt: "2025-02-12T12:09:01Z"
        creationTimestamp: null
        labels:
          app.kubernetes.io/component: server
          app.kubernetes.io/instance: prometheus
          app.kubernetes.io/name: prometheus
          app.kubernetes.io/part-of: prometheus
          app.kubernetes.io/version: v3.1.0
          helm.sh/chart: prometheus-27.3.0
          pod-template-hash: 9b658cb66
      spec:
        containers:
        - args:
          - --watched-dir=/etc/config
          - --listen-address=0.0.0.0:8080
          - --reload-url=http://127.0.0.1:9090/-/reload
          image: quay.io/prometheus-operator/prometheus-config-reloader:v0.79.2
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /healthz
              port: metrics
              scheme: HTTP
            initialDelaySeconds: 2
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          name: prometheus-server-configmap-reload
          ports:
          - containerPort: 8080
            name: metrics
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /healthz
              port: metrics
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/config
            name: config-volume
            readOnly: true
        - args:
          - --storage.tsdb.retention.time=15d
          - --config.file=/etc/config/prometheus.yml
          - --storage.tsdb.path=/data
          - --web.console.libraries=/etc/prometheus/console_libraries
          - --web.console.templates=/etc/prometheus/consoles
          - --web.enable-lifecycle
          image: quay.io/prometheus/prometheus:v3.1.0
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /-/healthy
              port: 9090
              scheme: HTTP
            initialDelaySeconds: 30
            periodSeconds: 15
            successThreshold: 1
            timeoutSeconds: 10
          name: prometheus-server
          ports:
          - containerPort: 9090
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /-/ready
              port: 9090
              scheme: HTTP
            initialDelaySeconds: 30
            periodSeconds: 5
            successThreshold: 1
            timeoutSeconds: 4
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/config
            name: config-volume
          - mountPath: /data
            name: storage-volume
        dnsPolicy: ClusterFirst
        enableServiceLinks: true
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 65534
          runAsGroup: 65534
          runAsNonRoot: true
          runAsUser: 65534
        serviceAccount: prometheus-server
        serviceAccountName: prometheus-server
        terminationGracePeriodSeconds: 300
        volumes:
        - configMap:
            defaultMode: 420
            name: prometheus-server
          name: config-volume
        - name: storage-volume
          persistentVolumeClaim:
            claimName: prometheus-server
  status:
    observedGeneration: 1
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "1"
      deployment.kubernetes.io/revision: "29"
      field.cattle.io/publicEndpoints: "null"
      meta.helm.sh/release-name: prometheus
      meta.helm.sh/release-namespace: monitoring
    creationTimestamp: "2025-02-26T10:32:59Z"
    generation: 1
    labels:
      app.kubernetes.io/component: server
      app.kubernetes.io/instance: prometheus
      app.kubernetes.io/name: prometheus
      app.kubernetes.io/part-of: prometheus
      app.kubernetes.io/version: v3.1.0
      helm.sh/chart: prometheus-27.3.0
      pod-template-hash: d7d4585c5
      velero.io/backup-name: monitoring-backup
      velero.io/restore-name: monitoring-backup-20250226103249
    name: prometheus-server-d7d4585c5
    namespace: monitoring
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: prometheus-server
      uid: 9811a1b1-4c9b-43d9-82a8-2f566b280ff0
    resourceVersion: "196328"
    uid: 75cbd753-bfa2-44ae-9e3e-121839b72e0a
  spec:
    replicas: 1
    selector:
      matchLabels:
        app.kubernetes.io/component: server
        app.kubernetes.io/instance: prometheus
        app.kubernetes.io/name: prometheus
        pod-template-hash: d7d4585c5
    template:
      metadata:
        annotations:
          kubectl.kubernetes.io/restartedAt: "2025-02-13T12:06:57Z"
        creationTimestamp: null
        labels:
          app.kubernetes.io/component: server
          app.kubernetes.io/instance: prometheus
          app.kubernetes.io/name: prometheus
          app.kubernetes.io/part-of: prometheus
          app.kubernetes.io/version: v3.1.0
          helm.sh/chart: prometheus-27.3.0
          pod-template-hash: d7d4585c5
      spec:
        containers:
        - args:
          - --watched-dir=/etc/config
          - --listen-address=0.0.0.0:8080
          - --reload-url=http://127.0.0.1:9090/-/reload
          image: quay.io/prometheus-operator/prometheus-config-reloader:v0.79.2
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /healthz
              port: metrics
              scheme: HTTP
            initialDelaySeconds: 2
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          name: prometheus-server-configmap-reload
          ports:
          - containerPort: 8080
            name: metrics
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /healthz
              port: metrics
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/config
            name: config-volume
            readOnly: true
        - args:
          - --storage.tsdb.retention.time=15d
          - --config.file=/etc/config/prometheus.yml
          - --storage.tsdb.path=/data
          - --web.console.libraries=/etc/prometheus/console_libraries
          - --web.console.templates=/etc/prometheus/consoles
          - --web.enable-lifecycle
          image: quay.io/prometheus/prometheus:v3.1.0
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /-/healthy
              port: 9090
              scheme: HTTP
            initialDelaySeconds: 30
            periodSeconds: 15
            successThreshold: 1
            timeoutSeconds: 10
          name: prometheus-server
          ports:
          - containerPort: 9090
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /-/ready
              port: 9090
              scheme: HTTP
            initialDelaySeconds: 30
            periodSeconds: 5
            successThreshold: 1
            timeoutSeconds: 4
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/config
            name: config-volume
          - mountPath: /data
            name: storage-volume
        dnsPolicy: ClusterFirst
        enableServiceLinks: true
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 65534
          runAsGroup: 65534
          runAsNonRoot: true
          runAsUser: 65534
        serviceAccount: prometheus-server
        serviceAccountName: prometheus-server
        terminationGracePeriodSeconds: 300
        volumes:
        - configMap:
            defaultMode: 420
            name: prometheus-server
          name: config-volume
        - name: storage-volume
          persistentVolumeClaim:
            claimName: prometheus-server
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "18"
      field.cattle.io/publicEndpoints: '[{"port":30089,"protocol":"TCP","serviceName":"monitoring:velero-runner","allNodes":true}]'
    creationTimestamp: "2025-02-26T10:32:59Z"
    generation: 1
    labels:
      app: velero-runner
      pod-template-hash: 54974bb85d
      velero.io/backup-name: monitoring-backup
      velero.io/restore-name: monitoring-backup-20250226103249
    name: velero-runner-54974bb85d
    namespace: monitoring
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: velero-runner
      uid: 70edf5cb-849c-44d2-b360-47a942d14fa9
    resourceVersion: "197321"
    uid: 0c1e3709-3326-4c5b-80b2-f6be3901b525
  spec:
    replicas: 1
    selector:
      matchLabels:
        app: velero-runner
        pod-template-hash: 54974bb85d
    template:
      metadata:
        annotations:
          kubectl.kubernetes.io/restartedAt: "2025-02-13T12:06:57Z"
        creationTimestamp: null
        labels:
          app: velero-runner
          pod-template-hash: 54974bb85d
      spec:
        containers:
        - command:
          - python
          - /app/webhook.py
          image: python:3.9
          imagePullPolicy: IfNotPresent
          name: velero-runner
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /app
            name: app
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
        volumes:
        - configMap:
            defaultMode: 420
            name: webhook-config
          name: app
  status:
    fullyLabeledReplicas: 1
    observedGeneration: 1
    replicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "12"
      field.cattle.io/publicEndpoints: "null"
    creationTimestamp: "2025-02-26T10:32:59Z"
    generation: 1
    labels:
      app: velero-runner
      pod-template-hash: 567845dc97
      velero.io/backup-name: monitoring-backup
      velero.io/restore-name: monitoring-backup-20250226103249
    name: velero-runner-567845dc97
    namespace: monitoring
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: velero-runner
      uid: 70edf5cb-849c-44d2-b360-47a942d14fa9
    resourceVersion: "124747"
    uid: cdfefe76-01dd-4644-bee1-da35c73bed63
  spec:
    replicas: 0
    selector:
      matchLabels:
        app: velero-runner
        pod-template-hash: 567845dc97
    template:
      metadata:
        annotations:
          kubectl.kubernetes.io/restartedAt: "2025-02-12T11:51:16Z"
        creationTimestamp: null
        labels:
          app: velero-runner
          pod-template-hash: 567845dc97
      spec:
        containers:
        - command:
          - python
          - /app/webhook.py
          image: python:3.9
          imagePullPolicy: IfNotPresent
          name: velero-runner
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /app
            name: app
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
        volumes:
        - configMap:
            defaultMode: 420
            name: webhook-config
          name: app
  status:
    observedGeneration: 1
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "10"
      field.cattle.io/publicEndpoints: "null"
    creationTimestamp: "2025-02-26T10:32:59Z"
    generation: 1
    labels:
      app: velero-runner
      pod-template-hash: 694547ff5d
      velero.io/backup-name: monitoring-backup
      velero.io/restore-name: monitoring-backup-20250226103249
    name: velero-runner-694547ff5d
    namespace: monitoring
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: velero-runner
      uid: 70edf5cb-849c-44d2-b360-47a942d14fa9
    resourceVersion: "124744"
    uid: b09c5968-b51a-4cfd-b89c-0986a6b072cb
  spec:
    replicas: 0
    selector:
      matchLabels:
        app: velero-runner
        pod-template-hash: 694547ff5d
    template:
      metadata:
        annotations:
          kubectl.kubernetes.io/restartedAt: "2025-02-12T11:48:06Z"
        creationTimestamp: null
        labels:
          app: velero-runner
          pod-template-hash: 694547ff5d
      spec:
        containers:
        - command:
          - python
          - /app/webhook.py
          image: python:3.9
          imagePullPolicy: IfNotPresent
          name: velero-runner
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /app
            name: app
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
        volumes:
        - configMap:
            defaultMode: 420
            name: webhook-config
          name: app
  status:
    observedGeneration: 1
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "17"
      field.cattle.io/publicEndpoints: "null"
    creationTimestamp: "2025-02-26T10:33:00Z"
    generation: 1
    labels:
      app: velero-runner
      pod-template-hash: 695cc5974
      velero.io/backup-name: monitoring-backup
      velero.io/restore-name: monitoring-backup-20250226103249
    name: velero-runner-695cc5974
    namespace: monitoring
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: velero-runner
      uid: 70edf5cb-849c-44d2-b360-47a942d14fa9
    resourceVersion: "124712"
    uid: e53453aa-1408-4ec7-8f05-6b8ff968a8f7
  spec:
    replicas: 0
    selector:
      matchLabels:
        app: velero-runner
        pod-template-hash: 695cc5974
    template:
      metadata:
        annotations:
          kubectl.kubernetes.io/restartedAt: "2025-02-12T13:04:13Z"
        creationTimestamp: null
        labels:
          app: velero-runner
          pod-template-hash: 695cc5974
      spec:
        containers:
        - command:
          - python
          - /app/webhook.py
          image: python:3.9
          imagePullPolicy: IfNotPresent
          name: velero-runner
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /app
            name: app
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
        volumes:
        - configMap:
            defaultMode: 420
            name: webhook-config
          name: app
  status:
    observedGeneration: 1
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "14"
      field.cattle.io/publicEndpoints: "null"
    creationTimestamp: "2025-02-26T10:33:00Z"
    generation: 1
    labels:
      app: velero-runner
      pod-template-hash: 7485fbdc49
      velero.io/backup-name: monitoring-backup
      velero.io/restore-name: monitoring-backup-20250226103249
    name: velero-runner-7485fbdc49
    namespace: monitoring
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: velero-runner
      uid: 70edf5cb-849c-44d2-b360-47a942d14fa9
    resourceVersion: "124732"
    uid: b03b7a33-7032-4ac2-a8c7-25abc8485526
  spec:
    replicas: 0
    selector:
      matchLabels:
        app: velero-runner
        pod-template-hash: 7485fbdc49
    template:
      metadata:
        annotations:
          kubectl.kubernetes.io/restartedAt: "2025-02-12T12:34:18Z"
        creationTimestamp: null
        labels:
          app: velero-runner
          pod-template-hash: 7485fbdc49
      spec:
        containers:
        - command:
          - python
          - /app/webhook.py
          image: python:3.9
          imagePullPolicy: IfNotPresent
          name: velero-runner
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /app
            name: app
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
        volumes:
        - configMap:
            defaultMode: 420
            name: webhook-config
          name: app
  status:
    observedGeneration: 1
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "8"
      field.cattle.io/publicEndpoints: "null"
    creationTimestamp: "2025-02-26T10:33:00Z"
    generation: 1
    labels:
      app: velero-runner
      pod-template-hash: 749d48fc58
      velero.io/backup-name: monitoring-backup
      velero.io/restore-name: monitoring-backup-20250226103249
    name: velero-runner-749d48fc58
    namespace: monitoring
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: velero-runner
      uid: 70edf5cb-849c-44d2-b360-47a942d14fa9
    resourceVersion: "124751"
    uid: 5b503c96-ae8a-46b8-b297-b07bc5b832b9
  spec:
    replicas: 0
    selector:
      matchLabels:
        app: velero-runner
        pod-template-hash: 749d48fc58
    template:
      metadata:
        annotations:
          kubectl.kubernetes.io/restartedAt: "2025-02-12T11:33:17Z"
        creationTimestamp: null
        labels:
          app: velero-runner
          pod-template-hash: 749d48fc58
      spec:
        containers:
        - command:
          - python
          - /app/webhook.py
          image: python:3.9
          imagePullPolicy: IfNotPresent
          name: velero-runner
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /app
            name: app
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
        volumes:
        - configMap:
            defaultMode: 420
            name: webhook-config
          name: app
  status:
    observedGeneration: 1
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "16"
      field.cattle.io/publicEndpoints: "null"
    creationTimestamp: "2025-02-26T10:33:01Z"
    generation: 1
    labels:
      app: velero-runner
      pod-template-hash: 799b9c9dfd
      velero.io/backup-name: monitoring-backup
      velero.io/restore-name: monitoring-backup-20250226103249
    name: velero-runner-799b9c9dfd
    namespace: monitoring
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: velero-runner
      uid: 70edf5cb-849c-44d2-b360-47a942d14fa9
    resourceVersion: "124715"
    uid: 056a9576-5d48-4628-bc28-ee88c43cce11
  spec:
    replicas: 0
    selector:
      matchLabels:
        app: velero-runner
        pod-template-hash: 799b9c9dfd
    template:
      metadata:
        annotations:
          kubectl.kubernetes.io/restartedAt: "2025-02-12T12:41:42Z"
        creationTimestamp: null
        labels:
          app: velero-runner
          pod-template-hash: 799b9c9dfd
      spec:
        containers:
        - command:
          - python
          - /app/webhook.py
          image: python:3.9
          imagePullPolicy: IfNotPresent
          name: velero-runner
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /app
            name: app
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
        volumes:
        - configMap:
            defaultMode: 420
            name: webhook-config
          name: app
  status:
    observedGeneration: 1
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "13"
      field.cattle.io/publicEndpoints: "null"
    creationTimestamp: "2025-02-26T10:33:01Z"
    generation: 1
    labels:
      app: velero-runner
      pod-template-hash: 84ff7cf4c9
      velero.io/backup-name: monitoring-backup
      velero.io/restore-name: monitoring-backup-20250226103249
    name: velero-runner-84ff7cf4c9
    namespace: monitoring
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: velero-runner
      uid: 70edf5cb-849c-44d2-b360-47a942d14fa9
    resourceVersion: "124718"
    uid: 5594a7a3-42a9-4e2c-8158-9d9bedc1eaed
  spec:
    replicas: 0
    selector:
      matchLabels:
        app: velero-runner
        pod-template-hash: 84ff7cf4c9
    template:
      metadata:
        annotations:
          kubectl.kubernetes.io/restartedAt: "2025-02-12T12:09:01Z"
        creationTimestamp: null
        labels:
          app: velero-runner
          pod-template-hash: 84ff7cf4c9
      spec:
        containers:
        - command:
          - python
          - /app/webhook.py
          image: python:3.9
          imagePullPolicy: IfNotPresent
          name: velero-runner
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /app
            name: app
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
        volumes:
        - configMap:
            defaultMode: 420
            name: webhook-config
          name: app
  status:
    observedGeneration: 1
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "9"
      field.cattle.io/publicEndpoints: "null"
    creationTimestamp: "2025-02-26T10:33:01Z"
    generation: 1
    labels:
      app: velero-runner
      pod-template-hash: 85c57f7f64
      velero.io/backup-name: monitoring-backup
      velero.io/restore-name: monitoring-backup-20250226103249
    name: velero-runner-85c57f7f64
    namespace: monitoring
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: velero-runner
      uid: 70edf5cb-849c-44d2-b360-47a942d14fa9
    resourceVersion: "124723"
    uid: 22f19ff3-ce68-49da-8d3f-aa2fa2c7f298
  spec:
    replicas: 0
    selector:
      matchLabels:
        app: velero-runner
        pod-template-hash: 85c57f7f64
    template:
      metadata:
        annotations:
          kubectl.kubernetes.io/restartedAt: "2025-02-12T11:47:52Z"
        creationTimestamp: null
        labels:
          app: velero-runner
          pod-template-hash: 85c57f7f64
      spec:
        containers:
        - command:
          - python
          - /app/webhook.py
          image: python:3.9
          imagePullPolicy: IfNotPresent
          name: velero-runner
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /app
            name: app
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
        volumes:
        - configMap:
            defaultMode: 420
            name: webhook-config
          name: app
  status:
    observedGeneration: 1
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "11"
      field.cattle.io/publicEndpoints: "null"
    creationTimestamp: "2025-02-26T10:33:01Z"
    generation: 1
    labels:
      app: velero-runner
      pod-template-hash: bf4684786
      velero.io/backup-name: monitoring-backup
      velero.io/restore-name: monitoring-backup-20250226103249
    name: velero-runner-bf4684786
    namespace: monitoring
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: velero-runner
      uid: 70edf5cb-849c-44d2-b360-47a942d14fa9
    resourceVersion: "124735"
    uid: 367c87f4-208d-4d56-89b8-f356b78772d1
  spec:
    replicas: 0
    selector:
      matchLabels:
        app: velero-runner
        pod-template-hash: bf4684786
    template:
      metadata:
        annotations:
          kubectl.kubernetes.io/restartedAt: "2025-02-12T11:51:13Z"
        creationTimestamp: null
        labels:
          app: velero-runner
          pod-template-hash: bf4684786
      spec:
        containers:
        - command:
          - python
          - /app/webhook.py
          image: python:3.9
          imagePullPolicy: IfNotPresent
          name: velero-runner
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /app
            name: app
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
        volumes:
        - configMap:
            defaultMode: 420
            name: webhook-config
          name: app
  status:
    observedGeneration: 1
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "15"
      field.cattle.io/publicEndpoints: "null"
    creationTimestamp: "2025-02-26T10:33:01Z"
    generation: 1
    labels:
      app: velero-runner
      pod-template-hash: cd44bcbb9
      velero.io/backup-name: monitoring-backup
      velero.io/restore-name: monitoring-backup-20250226103249
    name: velero-runner-cd44bcbb9
    namespace: monitoring
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: velero-runner
      uid: 70edf5cb-849c-44d2-b360-47a942d14fa9
    resourceVersion: "124740"
    uid: 1fd8e69c-8277-4ee5-8d09-104006bd198a
  spec:
    replicas: 0
    selector:
      matchLabels:
        app: velero-runner
        pod-template-hash: cd44bcbb9
    template:
      metadata:
        annotations:
          kubectl.kubernetes.io/restartedAt: "2025-02-12T12:39:17Z"
        creationTimestamp: null
        labels:
          app: velero-runner
          pod-template-hash: cd44bcbb9
      spec:
        containers:
        - command:
          - python
          - /app/webhook.py
          image: python:3.9
          imagePullPolicy: IfNotPresent
          name: velero-runner
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /app
            name: app
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
        volumes:
        - configMap:
            defaultMode: 420
            name: webhook-config
          name: app
  status:
    observedGeneration: 1
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "30"
      field.cattle.io/publicEndpoints: "null"
    creationTimestamp: "2025-02-26T10:33:01Z"
    generation: 1
    labels:
      app: webhook
      pod-template-hash: 55cf9b97
      velero.io/backup-name: monitoring-backup
      velero.io/restore-name: monitoring-backup-20250226103249
    name: webhook-deployment-55cf9b97
    namespace: monitoring
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: webhook-deployment
      uid: 1ef53948-3f7e-4489-8902-5b431f11b6a1
    resourceVersion: "124746"
    uid: a883db6c-1c9a-4caa-9cdc-c4a95c5dd0a6
  spec:
    replicas: 0
    selector:
      matchLabels:
        app: webhook
        pod-template-hash: 55cf9b97
    template:
      metadata:
        annotations:
          kubectl.kubernetes.io/restartedAt: "2025-02-12T12:41:42Z"
        creationTimestamp: null
        labels:
          app: webhook
          pod-template-hash: 55cf9b97
      spec:
        containers:
        - image: gnh374/webhook-express:latest
          imagePullPolicy: Always
          name: webhook
          ports:
          - containerPort: 3000
            protocol: TCP
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: webhook-sa
        serviceAccountName: webhook-sa
        terminationGracePeriodSeconds: 30
  status:
    observedGeneration: 1
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "25"
      field.cattle.io/publicEndpoints: "null"
    creationTimestamp: "2025-02-26T10:33:02Z"
    generation: 1
    labels:
      app: webhook
      pod-template-hash: 6465ff57c7
      velero.io/backup-name: monitoring-backup
      velero.io/restore-name: monitoring-backup-20250226103249
    name: webhook-deployment-6465ff57c7
    namespace: monitoring
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: webhook-deployment
      uid: 1ef53948-3f7e-4489-8902-5b431f11b6a1
    resourceVersion: "124760"
    uid: 31b7271d-63d9-49d1-a391-790bd3177c21
  spec:
    replicas: 0
    selector:
      matchLabels:
        app: webhook
        pod-template-hash: 6465ff57c7
    template:
      metadata:
        annotations:
          kubectl.kubernetes.io/restartedAt: "2025-02-12T11:51:13Z"
        creationTimestamp: null
        labels:
          app: webhook
          pod-template-hash: 6465ff57c7
      spec:
        containers:
        - image: gnh374/webhook-express:latest
          imagePullPolicy: Always
          name: webhook
          ports:
          - containerPort: 3000
            protocol: TCP
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: webhook-sa
        serviceAccountName: webhook-sa
        terminationGracePeriodSeconds: 30
  status:
    observedGeneration: 1
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "32"
      field.cattle.io/publicEndpoints: '[{"port":30080,"protocol":"TCP","serviceName":"monitoring:webhook-service","allNodes":true}]'
    creationTimestamp: "2025-02-26T10:33:02Z"
    generation: 1
    labels:
      app: webhook
      pod-template-hash: 64d4fb9bcc
      velero.io/backup-name: monitoring-backup
      velero.io/restore-name: monitoring-backup-20250226103249
    name: webhook-deployment-64d4fb9bcc
    namespace: monitoring
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: webhook-deployment
      uid: 1ef53948-3f7e-4489-8902-5b431f11b6a1
    resourceVersion: "196205"
    uid: 0ec5bc5d-b9e5-45c7-9b02-18ec3268c1c7
  spec:
    replicas: 1
    selector:
      matchLabels:
        app: webhook
        pod-template-hash: 64d4fb9bcc
    template:
      metadata:
        annotations:
          kubectl.kubernetes.io/restartedAt: "2025-02-13T12:06:57Z"
        creationTimestamp: null
        labels:
          app: webhook
          pod-template-hash: 64d4fb9bcc
      spec:
        containers:
        - image: gnh374/webhook-express:latest
          imagePullPolicy: Always
          name: webhook
          ports:
          - containerPort: 3000
            protocol: TCP
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: webhook-sa
        serviceAccountName: webhook-sa
        terminationGracePeriodSeconds: 30
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "24"
      field.cattle.io/publicEndpoints: "null"
    creationTimestamp: "2025-02-26T10:33:02Z"
    generation: 1
    labels:
      app: webhook
      pod-template-hash: 674b68f6f7
      velero.io/backup-name: monitoring-backup
      velero.io/restore-name: monitoring-backup-20250226103249
    name: webhook-deployment-674b68f6f7
    namespace: monitoring
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: webhook-deployment
      uid: 1ef53948-3f7e-4489-8902-5b431f11b6a1
    resourceVersion: "124774"
    uid: caa632fa-2afc-4c83-98d5-1a16b6f6a152
  spec:
    replicas: 0
    selector:
      matchLabels:
        app: webhook
        pod-template-hash: 674b68f6f7
    template:
      metadata:
        annotations:
          kubectl.kubernetes.io/restartedAt: "2025-02-12T11:48:06Z"
        creationTimestamp: null
        labels:
          app: webhook
          pod-template-hash: 674b68f6f7
      spec:
        containers:
        - image: gnh374/webhook-express:latest
          imagePullPolicy: Always
          name: webhook
          ports:
          - containerPort: 3000
            protocol: TCP
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: webhook-sa
        serviceAccountName: webhook-sa
        terminationGracePeriodSeconds: 30
  status:
    observedGeneration: 1
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "31"
      field.cattle.io/publicEndpoints: "null"
    creationTimestamp: "2025-02-26T10:33:02Z"
    generation: 1
    labels:
      app: webhook
      pod-template-hash: 699f67647c
      velero.io/backup-name: monitoring-backup
      velero.io/restore-name: monitoring-backup-20250226103249
    name: webhook-deployment-699f67647c
    namespace: monitoring
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: webhook-deployment
      uid: 1ef53948-3f7e-4489-8902-5b431f11b6a1
    resourceVersion: "124743"
    uid: 5c749700-145b-4fcb-bfd8-8d3d02eb9783
  spec:
    replicas: 0
    selector:
      matchLabels:
        app: webhook
        pod-template-hash: 699f67647c
    template:
      metadata:
        annotations:
          kubectl.kubernetes.io/restartedAt: "2025-02-12T13:04:13Z"
        creationTimestamp: null
        labels:
          app: webhook
          pod-template-hash: 699f67647c
      spec:
        containers:
        - image: gnh374/webhook-express:latest
          imagePullPolicy: Always
          name: webhook
          ports:
          - containerPort: 3000
            protocol: TCP
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: webhook-sa
        serviceAccountName: webhook-sa
        terminationGracePeriodSeconds: 30
  status:
    observedGeneration: 1
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "23"
      field.cattle.io/publicEndpoints: "null"
    creationTimestamp: "2025-02-26T10:33:02Z"
    generation: 1
    labels:
      app: webhook
      pod-template-hash: 6bd7777c55
      velero.io/backup-name: monitoring-backup
      velero.io/restore-name: monitoring-backup-20250226103249
    name: webhook-deployment-6bd7777c55
    namespace: monitoring
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: webhook-deployment
      uid: 1ef53948-3f7e-4489-8902-5b431f11b6a1
    resourceVersion: "124734"
    uid: 2d8784fc-b0ff-4e42-8971-96c50ee7ac10
  spec:
    replicas: 0
    selector:
      matchLabels:
        app: webhook
        pod-template-hash: 6bd7777c55
    template:
      metadata:
        annotations:
          kubectl.kubernetes.io/restartedAt: "2025-02-12T11:47:52Z"
        creationTimestamp: null
        labels:
          app: webhook
          pod-template-hash: 6bd7777c55
      spec:
        containers:
        - image: gnh374/webhook-express:latest
          imagePullPolicy: Always
          name: webhook
          ports:
          - containerPort: 3000
            protocol: TCP
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: webhook-sa
        serviceAccountName: webhook-sa
        terminationGracePeriodSeconds: 30
  status:
    observedGeneration: 1
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "22"
      field.cattle.io/publicEndpoints: "null"
    creationTimestamp: "2025-02-26T10:33:02Z"
    generation: 1
    labels:
      app: webhook
      pod-template-hash: 6c67bcdd6f
      velero.io/backup-name: monitoring-backup
      velero.io/restore-name: monitoring-backup-20250226103249
    name: webhook-deployment-6c67bcdd6f
    namespace: monitoring
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: webhook-deployment
      uid: 1ef53948-3f7e-4489-8902-5b431f11b6a1
    resourceVersion: "124739"
    uid: f461dd9a-41c2-46ff-823b-14402dd408fe
  spec:
    replicas: 0
    selector:
      matchLabels:
        app: webhook
        pod-template-hash: 6c67bcdd6f
    template:
      metadata:
        annotations:
          kubectl.kubernetes.io/restartedAt: "2025-02-12T11:33:17Z"
        creationTimestamp: null
        labels:
          app: webhook
          pod-template-hash: 6c67bcdd6f
      spec:
        containers:
        - image: gnh374/webhook-express:latest
          imagePullPolicy: Always
          name: webhook
          ports:
          - containerPort: 3000
            protocol: TCP
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: webhook-sa
        serviceAccountName: webhook-sa
        terminationGracePeriodSeconds: 30
  status:
    observedGeneration: 1
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "29"
      field.cattle.io/publicEndpoints: "null"
    creationTimestamp: "2025-02-26T10:33:02Z"
    generation: 1
    labels:
      app: webhook
      pod-template-hash: 7969db7fc6
      velero.io/backup-name: monitoring-backup
      velero.io/restore-name: monitoring-backup-20250226103249
    name: webhook-deployment-7969db7fc6
    namespace: monitoring
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: webhook-deployment
      uid: 1ef53948-3f7e-4489-8902-5b431f11b6a1
    resourceVersion: "124757"
    uid: ad76fbf9-f61e-4060-8b40-d0940b6e8c95
  spec:
    replicas: 0
    selector:
      matchLabels:
        app: webhook
        pod-template-hash: 7969db7fc6
    template:
      metadata:
        annotations:
          kubectl.kubernetes.io/restartedAt: "2025-02-12T12:39:17Z"
        creationTimestamp: null
        labels:
          app: webhook
          pod-template-hash: 7969db7fc6
      spec:
        containers:
        - image: gnh374/webhook-express:latest
          imagePullPolicy: Always
          name: webhook
          ports:
          - containerPort: 3000
            protocol: TCP
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: webhook-sa
        serviceAccountName: webhook-sa
        terminationGracePeriodSeconds: 30
  status:
    observedGeneration: 1
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "26"
      field.cattle.io/publicEndpoints: "null"
    creationTimestamp: "2025-02-26T10:33:02Z"
    generation: 1
    labels:
      app: webhook
      pod-template-hash: 7fd45cff8d
      velero.io/backup-name: monitoring-backup
      velero.io/restore-name: monitoring-backup-20250226103249
    name: webhook-deployment-7fd45cff8d
    namespace: monitoring
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: webhook-deployment
      uid: 1ef53948-3f7e-4489-8902-5b431f11b6a1
    resourceVersion: "124765"
    uid: 7e82ec98-ac64-49b4-bdbe-6adb17bc9348
  spec:
    replicas: 0
    selector:
      matchLabels:
        app: webhook
        pod-template-hash: 7fd45cff8d
    template:
      metadata:
        annotations:
          kubectl.kubernetes.io/restartedAt: "2025-02-12T11:51:16Z"
        creationTimestamp: null
        labels:
          app: webhook
          pod-template-hash: 7fd45cff8d
      spec:
        containers:
        - image: gnh374/webhook-express:latest
          imagePullPolicy: Always
          name: webhook
          ports:
          - containerPort: 3000
            protocol: TCP
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: webhook-sa
        serviceAccountName: webhook-sa
        terminationGracePeriodSeconds: 30
  status:
    observedGeneration: 1
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "27"
      field.cattle.io/publicEndpoints: "null"
    creationTimestamp: "2025-02-26T10:33:02Z"
    generation: 1
    labels:
      app: webhook
      pod-template-hash: b8bf98bcd
      velero.io/backup-name: monitoring-backup
      velero.io/restore-name: monitoring-backup-20250226103249
    name: webhook-deployment-b8bf98bcd
    namespace: monitoring
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: webhook-deployment
      uid: 1ef53948-3f7e-4489-8902-5b431f11b6a1
    resourceVersion: "124752"
    uid: 1333097b-99ad-43b1-9ee2-a8ce8ef74baf
  spec:
    replicas: 0
    selector:
      matchLabels:
        app: webhook
        pod-template-hash: b8bf98bcd
    template:
      metadata:
        annotations:
          kubectl.kubernetes.io/restartedAt: "2025-02-12T12:09:01Z"
        creationTimestamp: null
        labels:
          app: webhook
          pod-template-hash: b8bf98bcd
      spec:
        containers:
        - image: gnh374/webhook-express:latest
          imagePullPolicy: Always
          name: webhook
          ports:
          - containerPort: 3000
            protocol: TCP
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: webhook-sa
        serviceAccountName: webhook-sa
        terminationGracePeriodSeconds: 30
  status:
    observedGeneration: 1
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "28"
      field.cattle.io/publicEndpoints: "null"
    creationTimestamp: "2025-02-26T10:33:03Z"
    generation: 1
    labels:
      app: webhook
      pod-template-hash: f5fdd76b8
      velero.io/backup-name: monitoring-backup
      velero.io/restore-name: monitoring-backup-20250226103249
    name: webhook-deployment-f5fdd76b8
    namespace: monitoring
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: webhook-deployment
      uid: 1ef53948-3f7e-4489-8902-5b431f11b6a1
    resourceVersion: "124730"
    uid: 37a8203d-5132-4141-88fd-aa7835f3c853
  spec:
    replicas: 0
    selector:
      matchLabels:
        app: webhook
        pod-template-hash: f5fdd76b8
    template:
      metadata:
        annotations:
          kubectl.kubernetes.io/restartedAt: "2025-02-12T12:34:18Z"
        creationTimestamp: null
        labels:
          app: webhook
          pod-template-hash: f5fdd76b8
      spec:
        containers:
        - image: gnh374/webhook-express:latest
          imagePullPolicy: Always
          name: webhook
          ports:
          - containerPort: 3000
            protocol: TCP
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: webhook-sa
        serviceAccountName: webhook-sa
        terminationGracePeriodSeconds: 30
  status:
    observedGeneration: 1
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "1"
    creationTimestamp: "2025-02-06T14:32:37Z"
    generation: 1
    labels:
      component: velero
      deploy: velero
      pod-template-hash: 57fdbd48db
    name: velero-57fdbd48db
    namespace: velero
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: velero
      uid: 958dac77-e6fe-4e0c-a880-b2980d992d7e
    resourceVersion: "196321"
    uid: 1da66ef8-ddcf-49ab-b15e-981515303dd3
  spec:
    replicas: 1
    selector:
      matchLabels:
        deploy: velero
        pod-template-hash: 57fdbd48db
    template:
      metadata:
        annotations:
          prometheus.io/path: /metrics
          prometheus.io/port: "8085"
          prometheus.io/scrape: "true"
        creationTimestamp: null
        labels:
          component: velero
          deploy: velero
          pod-template-hash: 57fdbd48db
      spec:
        containers:
        - args:
          - server
          - --features=
          - --uploader-type=kopia
          command:
          - /velero
          env:
          - name: VELERO_SCRATCH_DIR
            value: /scratch
          - name: VELERO_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          - name: LD_LIBRARY_PATH
            value: /plugins
          image: velero/velero:v1.15.2
          imagePullPolicy: IfNotPresent
          name: velero
          ports:
          - containerPort: 8085
            name: metrics
            protocol: TCP
          resources:
            limits:
              cpu: "1"
              memory: 512Mi
            requests:
              cpu: 500m
              memory: 128Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /plugins
            name: plugins
          - mountPath: /scratch
            name: scratch
        dnsPolicy: ClusterFirst
        initContainers:
        - image: velero/velero-plugin-for-aws:v1.10.0
          imagePullPolicy: IfNotPresent
          name: velero-velero-plugin-for-aws
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /target
            name: plugins
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: velero
        serviceAccountName: velero
        terminationGracePeriodSeconds: 30
        volumes:
        - emptyDir: {}
          name: plugins
        - emptyDir: {}
          name: scratch
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
- apiVersion: apps/v1
  kind: StatefulSet
  metadata:
    annotations:
      objectset.rio.cattle.io/applied: H4sIAAAAAAAA/8xWbW8iNxD+L/68ECBHXvYbhc0VHQEEuVbVCUWDdxbc89pbe8yFRvvfq9nlLdccukqNlG+21/P6PONnn0WOBCkQiPhZgDGWgJQ1nrd2+SdK8khNp2xTApHGprIXKhWxyDQiNWCFhhpLa8mTg6JRX2rUH7WVoBt+6wlzUUZCOqycP6gcPUFeiNgErSOhYYn6bMg1+LWIxeVVp9W5zK67HbzO2rc3t+nN8qrbwVbndpl25FX3BvBafvjA0Qzk+DJPUR/6AiR/OZurL1ByQh41SrKO1zmQXI8OuUJRfOe/ZEN0GyVx/Fp0wrzQQFg5O+n7TzTmB8F2WUKWKaNoy2tjU+yd7AuHGTqH6SA4ZVZzucY0aGVWw5Wxh+PkCWXgHET8ZW+DRuKh7uSpcOh9zY0vz+IrbvcJneC0r9QW6KBqmxgaEYkN6IBsKMgFFItyUUbiG6rVmkTcLhdcjLSGQBl0dQBp8xxMyjZVlNr1IhJoNtWFHcC/fB4PRskgmY4mf9wn44fHWdKfjPvDUTJ7/H0y+5TM5vsERCy6LVFGB9vBbHj3nwzGvftkPu31k/2NO2dz7lGmUKczzA7rKRAzdg9z88i8sjxx2PvIKc/7k2lyEvQMNReRUDms+JYDI9foLk5YEW9azXa7eSl2t6ZB66nVSjJYw2xsaerQH2fhXxR16G1wkrF6rtgsg1O07VtD+EQV17S236ZObZTGFSZegoaaOBloj5GQUMBSaUWq8iJSZwtGsTcaCYa92NumBxOHkE6M3s6spTulcVdszGQpI7GxOuR4b4Ohmhs5L3cdvmh+DUs81lPtmF8/olAkpA6e0HkCCv4VSv3vKPd/TfqfhuPH4fghmf3WG51A3e7mLf+2sDZe1vsuQa4aYBT1f+YRYF8rxSW9HXhvCsgh/XeLBavI/ET7eKqcQULPz7z1IhZamfAkXs/ZBdPzH50NhYjbrVYrqk/G1nDEOs7u7LNHV186qmdPSp7w10XUataWgxBhlqEkBt7utI0fg1qeuIqm1DakhbMblaJrviwkGCadAq3+xvSlbiV/BdAng1oJF8/0uYBHJbT+vLtd9xb7121XTF7QdqBcTYbvXrQyEqFIgXBODghXLO/VX0A910yVDSgNS40zLLSS4EXMrT9uyvKfAAAA//90DHTd8gkAAA
      objectset.rio.cattle.io/id: fleet-agent-bootstrap-cattle-fleet-local-system
    creationTimestamp: "2025-03-03T02:56:46Z"
    generation: 1
    labels:
      objectset.rio.cattle.io/hash: 362023f752e7f1989d8b652e029bd2c658ae7c44
    name: fleet-agent
    namespace: cattle-fleet-local-system
    resourceVersion: "196800"
    uid: 369f846f-3ada-4c9c-971f-f53c7a4dd7b8
  spec:
    persistentVolumeClaimRetentionPolicy:
      whenDeleted: Retain
      whenScaled: Retain
    podManagementPolicy: OrderedReady
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app: fleet-agent
    serviceName: fleet-agent
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: fleet-agent
      spec:
        affinity:
          nodeAffinity:
            preferredDuringSchedulingIgnoredDuringExecution:
            - preference:
                matchExpressions:
                - key: fleet.cattle.io/agent
                  operator: In
                  values:
                  - "true"
              weight: 1
        containers:
        - command:
          - fleetagent
          env:
          - name: BUNDLEDEPLOYMENT_RECONCILER_WORKERS
            value: "50"
          - name: DRIFT_RECONCILER_WORKERS
            value: "50"
          - name: NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          - name: AGENT_SCOPE
            value: cattle-fleet-local-system
          image: rancher/fleet-agent:v0.11.3
          imagePullPolicy: IfNotPresent
          name: fleet-agent
          resources: {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            privileged: false
            readOnlyRootFilesystem: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /.kube
            name: kube
        - command:
          - fleetagent
          - clusterstatus
          env:
          - name: NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          - name: CHECKIN_INTERVAL
            value: 15m0s
          image: rancher/fleet-agent:v0.11.3
          imagePullPolicy: IfNotPresent
          name: fleet-agent-clusterstatus
          resources: {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            privileged: false
            readOnlyRootFilesystem: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        initContainers:
        - command:
          - fleetagent
          - register
          env:
          - name: NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          image: rancher/fleet-agent:v0.11.3
          imagePullPolicy: IfNotPresent
          name: fleet-agent-register
          resources: {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            privileged: false
            readOnlyRootFilesystem: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        nodeSelector:
          kubernetes.io/os: linux
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          runAsGroup: 1000
          runAsNonRoot: true
          runAsUser: 1000
        serviceAccount: fleet-agent
        serviceAccountName: fleet-agent
        terminationGracePeriodSeconds: 30
        tolerations:
        - effect: NoSchedule
          key: node.cloudprovider.kubernetes.io/uninitialized
          operator: Equal
          value: "true"
        - effect: NoSchedule
          key: cattle.io/os
          operator: Equal
          value: linux
        volumes:
        - emptyDir: {}
          name: kube
    updateStrategy:
      rollingUpdate:
        partition: 0
      type: RollingUpdate
  status:
    availableReplicas: 1
    collisionCount: 0
    currentReplicas: 1
    currentRevision: fleet-agent-57565ff5cd
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
    updateRevision: fleet-agent-57565ff5cd
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: StatefulSet
  metadata:
    annotations:
      meta.helm.sh/release-name: prometheus
      meta.helm.sh/release-namespace: monitoring
    creationTimestamp: "2025-02-26T10:33:08Z"
    generation: 1
    labels:
      app.kubernetes.io/instance: prometheus
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: alertmanager
      app.kubernetes.io/version: v0.28.0
      helm.sh/chart: alertmanager-1.14.0
      velero.io/backup-name: monitoring-backup
      velero.io/restore-name: monitoring-backup-20250226103249
    name: prometheus-alertmanager
    namespace: monitoring
    resourceVersion: "196088"
    uid: 0258509c-71b2-4644-967f-2f8dde4e4393
  spec:
    persistentVolumeClaimRetentionPolicy:
      whenDeleted: Retain
      whenScaled: Retain
    podManagementPolicy: OrderedReady
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app.kubernetes.io/instance: prometheus
        app.kubernetes.io/name: alertmanager
    serviceName: prometheus-alertmanager-headless
    template:
      metadata:
        annotations:
          checksum/config: c2171b6682aaa4b8718c39a16c2736cef1b363bd9b893f8f58254180d449f03f
        creationTimestamp: null
        labels:
          app.kubernetes.io/instance: prometheus
          app.kubernetes.io/name: alertmanager
      spec:
        automountServiceAccountToken: true
        containers:
        - args:
          - --storage.path=/alertmanager
          - --config.file=/etc/alertmanager/alertmanager.yml
          env:
          - name: POD_IP
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.podIP
          image: quay.io/prometheus/alertmanager:v0.28.0
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /
              port: http
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          name: alertmanager
          ports:
          - containerPort: 9093
            name: http
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /
              port: http
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          resources: {}
          securityContext:
            runAsGroup: 65534
            runAsNonRoot: true
            runAsUser: 65534
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/alertmanager
            name: config
          - mountPath: /alertmanager
            name: storage
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 65534
          runAsGroup: 65534
          runAsNonRoot: true
          runAsUser: 65534
        serviceAccount: prometheus-alertmanager
        serviceAccountName: prometheus-alertmanager
        terminationGracePeriodSeconds: 30
        volumes:
        - configMap:
            defaultMode: 420
            name: prometheus-alertmanager
          name: config
    updateStrategy:
      rollingUpdate:
        partition: 0
      type: RollingUpdate
    volumeClaimTemplates:
    - apiVersion: v1
      kind: PersistentVolumeClaim
      metadata:
        creationTimestamp: null
        name: storage
      spec:
        accessModes:
        - ReadWriteOnce
        resources:
          requests:
            storage: 2Gi
        volumeMode: Filesystem
      status:
        phase: Pending
  status:
    availableReplicas: 1
    collisionCount: 0
    currentReplicas: 1
    currentRevision: prometheus-alertmanager-657656587f
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
    updateRevision: prometheus-alertmanager-657656587f
    updatedReplicas: 1
- apiVersion: batch/v1
  kind: CronJob
  metadata:
    annotations:
      meta.helm.sh/release-name: fleet
      meta.helm.sh/release-namespace: cattle-fleet-system
    creationTimestamp: "2025-02-05T05:33:55Z"
    generation: 1
    labels:
      app.kubernetes.io/managed-by: Helm
    name: fleet-cleanup-gitrepo-jobs
    namespace: cattle-fleet-system
    resourceVersion: "196363"
    uid: e940bc27-0af8-4f72-ad47-c2df41beff06
  spec:
    concurrencyPolicy: Forbid
    failedJobsHistoryLimit: 1
    jobTemplate:
      metadata:
        creationTimestamp: null
      spec:
        backoffLimit: 1
        template:
          metadata:
            creationTimestamp: null
            labels:
              app: fleet-job
          spec:
            containers:
            - args:
              - cleanup
              - gitjob
              command:
              - fleet
              image: rancher/fleet:v0.11.3
              imagePullPolicy: IfNotPresent
              name: cleanup
              resources: {}
              securityContext:
                allowPrivilegeEscalation: false
                capabilities:
                  drop:
                  - ALL
                privileged: false
                readOnlyRootFilesystem: false
              terminationMessagePath: /dev/termination-log
              terminationMessagePolicy: File
            dnsPolicy: ClusterFirst
            nodeSelector:
              kubernetes.io/os: linux
            restartPolicy: Never
            schedulerName: default-scheduler
            securityContext:
              runAsGroup: 1000
              runAsNonRoot: true
              runAsUser: 1000
            serviceAccount: gitjob
            serviceAccountName: gitjob
            terminationGracePeriodSeconds: 30
            tolerations:
            - effect: NoSchedule
              key: cattle.io/os
              operator: Equal
              value: linux
    schedule: '@daily'
    successfulJobsHistoryLimit: 0
    suspend: false
  status:
    lastScheduleTime: "2025-03-03T00:00:00Z"
    lastSuccessfulTime: "2025-03-03T02:55:40Z"
- apiVersion: batch/v1
  kind: CronJob
  metadata:
    annotations:
      objectset.rio.cattle.io/applied: H4sIAAAAAAAA/5xTW2+bTBD9K9E8fsJAiLExb5Fl6ZNyaZS4famiaFjGZs2yS3YHkibiv1eLXdVNL6kqHtjLnMOZM4dXaIixREbIXwG1NowsjXZ+a4odCXbEoZUmFMisKJQmkiXkYGtKJg2KSmqaCKM3cjsRilB3LQwBCEsj0Vo25BibFnLdKRWAwoLUH+krdBXkUJaLshCbci7OsixLy7OY5hmKYpZimlG6mCWzaZzFif+axobeaPpR0kRYo3emgH2ta1F4wEYR8aSkDXaKPZFrSXhtO1OsqWkVMvntsUm/6ewIXKCozWZzKRvJkJ/GAfA/cwmjGaUm6yD//Apot34BUUWqJRsd2gtdBfcBCNM0qMuxoJA62p+S7kfswaTl+Xp9uXq4W91+Wt1CAD2qzp9XzK3Lo2g6DZMkDuNwlobOKdmG0sAQvMUvzx+W/6+WF3cfr35xu/5wsbo+In/JVDZ/1jrbzZ6fHqfbup7VRZosnvvULp6qYpr1/WM9r/ui1U36osuk5kLAcB+AbHA7Dhe1qMhGh/cEt6Q575PwNA4T+JsMtKaEACw501lBPoNDAL1RXUNXptO8t7jxyxtkn8GDzd/ZD4x7EAz3XqD1o7N8Y5QUXyCHa+pHiCPbS0HnQnjG63flOYRvcvZK9vdX2PogvN+eE1a2fPQ7/CR2GHyyREVlp3xBehKf/OefMfqM3I2mDF8DAAD//55dvRgWBAAA
      objectset.rio.cattle.io/id: rke2-machine-config-cleanup
    creationTimestamp: "2025-02-05T05:32:22Z"
    generation: 2
    labels:
      objectset.rio.cattle.io/hash: dd9dbcfd7c38885d30e78acb65a58e5962640802
    name: rke2-machineconfig-cleanup-cronjob
    namespace: fleet-default
    resourceVersion: "196447"
    uid: d36d6811-c464-4c3a-bc27-5c6050bd87f2
  spec:
    concurrencyPolicy: Allow
    failedJobsHistoryLimit: 1
    jobTemplate:
      metadata:
        creationTimestamp: null
      spec:
        backoffLimit: 10
        template:
          metadata:
            creationTimestamp: null
          spec:
            containers:
            - args:
              - /helper/cleanup.sh
              command:
              - /bin/sh
              env:
              - name: CATTLE_SERVER
                value: https://44.220.0.65.sslip.io
              - name: CATTLE_CA_CHECKSUM
              - name: CATTLE_TOKEN
                value: z8l87xnn8j6xwq4gkk6kb529xv5r9whb48vvqk7kvbpnm5znd2ktbc
              image: rancher/rancher-agent:v2.10.2
              imagePullPolicy: IfNotPresent
              name: rke2-machineconfig-cleanup-pod
              resources: {}
              terminationMessagePath: /dev/termination-log
              terminationMessagePolicy: File
              volumeMounts:
              - mountPath: /helper
                name: config-volume
            dnsPolicy: ClusterFirst
            restartPolicy: Never
            schedulerName: default-scheduler
            securityContext: {}
            serviceAccount: rke2-machineconfig-cleanup-sa
            serviceAccountName: rke2-machineconfig-cleanup-sa
            terminationGracePeriodSeconds: 30
            volumes:
            - configMap:
                defaultMode: 420
                name: rke2-machineconfig-cleanup-script
              name: config-volume
    schedule: 5 0 * * *
    successfulJobsHistoryLimit: 3
    suspend: false
  status:
    lastScheduleTime: "2025-03-03T00:05:00Z"
    lastSuccessfulTime: "2025-03-03T02:55:41Z"
- apiVersion: batch/v1
  kind: Job
  metadata:
    annotations:
      batch.kubernetes.io/cronjob-scheduled-timestamp: "2025-02-27T00:05:00Z"
    creationTimestamp: "2025-02-27T04:33:05Z"
    generation: 1
    labels:
      batch.kubernetes.io/controller-uid: 0d1a231a-721c-4022-a184-4f4563e74802
      batch.kubernetes.io/job-name: rke2-machineconfig-cleanup-cronjob-29010245
      controller-uid: 0d1a231a-721c-4022-a184-4f4563e74802
      job-name: rke2-machineconfig-cleanup-cronjob-29010245
    name: rke2-machineconfig-cleanup-cronjob-29010245
    namespace: fleet-default
    ownerReferences:
    - apiVersion: batch/v1
      blockOwnerDeletion: true
      controller: true
      kind: CronJob
      name: rke2-machineconfig-cleanup-cronjob
      uid: d36d6811-c464-4c3a-bc27-5c6050bd87f2
    resourceVersion: "133943"
    uid: 0d1a231a-721c-4022-a184-4f4563e74802
  spec:
    backoffLimit: 10
    completionMode: NonIndexed
    completions: 1
    manualSelector: false
    parallelism: 1
    podReplacementPolicy: TerminatingOrFailed
    selector:
      matchLabels:
        batch.kubernetes.io/controller-uid: 0d1a231a-721c-4022-a184-4f4563e74802
    suspend: false
    template:
      metadata:
        creationTimestamp: null
        labels:
          batch.kubernetes.io/controller-uid: 0d1a231a-721c-4022-a184-4f4563e74802
          batch.kubernetes.io/job-name: rke2-machineconfig-cleanup-cronjob-29010245
          controller-uid: 0d1a231a-721c-4022-a184-4f4563e74802
          job-name: rke2-machineconfig-cleanup-cronjob-29010245
      spec:
        containers:
        - args:
          - /helper/cleanup.sh
          command:
          - /bin/sh
          env:
          - name: CATTLE_SERVER
            value: https://44.220.0.65.sslip.io
          - name: CATTLE_CA_CHECKSUM
          - name: CATTLE_TOKEN
            value: z8l87xnn8j6xwq4gkk6kb529xv5r9whb48vvqk7kvbpnm5znd2ktbc
          image: rancher/rancher-agent:v2.10.2
          imagePullPolicy: IfNotPresent
          name: rke2-machineconfig-cleanup-pod
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /helper
            name: config-volume
        dnsPolicy: ClusterFirst
        restartPolicy: Never
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: rke2-machineconfig-cleanup-sa
        serviceAccountName: rke2-machineconfig-cleanup-sa
        terminationGracePeriodSeconds: 30
        volumes:
        - configMap:
            defaultMode: 420
            name: rke2-machineconfig-cleanup-script
          name: config-volume
  status:
    completionTime: "2025-02-27T04:33:19Z"
    conditions:
    - lastProbeTime: "2025-02-27T04:33:19Z"
      lastTransitionTime: "2025-02-27T04:33:19Z"
      message: Reached expected number of succeeded pods
      reason: CompletionsReached
      status: "True"
      type: SuccessCriteriaMet
    - lastProbeTime: "2025-02-27T04:33:19Z"
      lastTransitionTime: "2025-02-27T04:33:19Z"
      message: Reached expected number of succeeded pods
      reason: CompletionsReached
      status: "True"
      type: Complete
    ready: 0
    startTime: "2025-02-27T04:33:06Z"
    succeeded: 1
    terminating: 0
    uncountedTerminatedPods: {}
- apiVersion: batch/v1
  kind: Job
  metadata:
    annotations:
      batch.kubernetes.io/cronjob-scheduled-timestamp: "2025-03-02T00:05:00Z"
    creationTimestamp: "2025-03-02T07:30:53Z"
    generation: 1
    labels:
      batch.kubernetes.io/controller-uid: efe74705-8c46-4e66-a2a7-23a4fb54df89
      batch.kubernetes.io/job-name: rke2-machineconfig-cleanup-cronjob-29014565
      controller-uid: efe74705-8c46-4e66-a2a7-23a4fb54df89
      job-name: rke2-machineconfig-cleanup-cronjob-29014565
    name: rke2-machineconfig-cleanup-cronjob-29014565
    namespace: fleet-default
    ownerReferences:
    - apiVersion: batch/v1
      blockOwnerDeletion: true
      controller: true
      kind: CronJob
      name: rke2-machineconfig-cleanup-cronjob
      uid: d36d6811-c464-4c3a-bc27-5c6050bd87f2
    resourceVersion: "167755"
    uid: efe74705-8c46-4e66-a2a7-23a4fb54df89
  spec:
    backoffLimit: 10
    completionMode: NonIndexed
    completions: 1
    manualSelector: false
    parallelism: 1
    podReplacementPolicy: TerminatingOrFailed
    selector:
      matchLabels:
        batch.kubernetes.io/controller-uid: efe74705-8c46-4e66-a2a7-23a4fb54df89
    suspend: false
    template:
      metadata:
        creationTimestamp: null
        labels:
          batch.kubernetes.io/controller-uid: efe74705-8c46-4e66-a2a7-23a4fb54df89
          batch.kubernetes.io/job-name: rke2-machineconfig-cleanup-cronjob-29014565
          controller-uid: efe74705-8c46-4e66-a2a7-23a4fb54df89
          job-name: rke2-machineconfig-cleanup-cronjob-29014565
      spec:
        containers:
        - args:
          - /helper/cleanup.sh
          command:
          - /bin/sh
          env:
          - name: CATTLE_SERVER
            value: https://44.220.0.65.sslip.io
          - name: CATTLE_CA_CHECKSUM
          - name: CATTLE_TOKEN
            value: z8l87xnn8j6xwq4gkk6kb529xv5r9whb48vvqk7kvbpnm5znd2ktbc
          image: rancher/rancher-agent:v2.10.2
          imagePullPolicy: IfNotPresent
          name: rke2-machineconfig-cleanup-pod
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /helper
            name: config-volume
        dnsPolicy: ClusterFirst
        restartPolicy: Never
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: rke2-machineconfig-cleanup-sa
        serviceAccountName: rke2-machineconfig-cleanup-sa
        terminationGracePeriodSeconds: 30
        volumes:
        - configMap:
            defaultMode: 420
            name: rke2-machineconfig-cleanup-script
          name: config-volume
  status:
    completionTime: "2025-03-02T07:31:11Z"
    conditions:
    - lastProbeTime: "2025-03-02T07:31:11Z"
      lastTransitionTime: "2025-03-02T07:31:11Z"
      message: Reached expected number of succeeded pods
      reason: CompletionsReached
      status: "True"
      type: SuccessCriteriaMet
    - lastProbeTime: "2025-03-02T07:31:12Z"
      lastTransitionTime: "2025-03-02T07:31:12Z"
      message: Reached expected number of succeeded pods
      reason: CompletionsReached
      status: "True"
      type: Complete
    ready: 0
    startTime: "2025-03-02T07:30:54Z"
    succeeded: 1
    terminating: 0
    uncountedTerminatedPods: {}
- apiVersion: batch/v1
  kind: Job
  metadata:
    annotations:
      batch.kubernetes.io/cronjob-scheduled-timestamp: "2025-03-03T00:05:00Z"
    creationTimestamp: "2025-03-03T02:55:24Z"
    generation: 1
    labels:
      batch.kubernetes.io/controller-uid: 51e431b0-336c-4b13-a974-2f8fae68aa95
      batch.kubernetes.io/job-name: rke2-machineconfig-cleanup-cronjob-29016005
      controller-uid: 51e431b0-336c-4b13-a974-2f8fae68aa95
      job-name: rke2-machineconfig-cleanup-cronjob-29016005
    name: rke2-machineconfig-cleanup-cronjob-29016005
    namespace: fleet-default
    ownerReferences:
    - apiVersion: batch/v1
      blockOwnerDeletion: true
      controller: true
      kind: CronJob
      name: rke2-machineconfig-cleanup-cronjob
      uid: d36d6811-c464-4c3a-bc27-5c6050bd87f2
    resourceVersion: "196438"
    uid: 51e431b0-336c-4b13-a974-2f8fae68aa95
  spec:
    backoffLimit: 10
    completionMode: NonIndexed
    completions: 1
    manualSelector: false
    parallelism: 1
    podReplacementPolicy: TerminatingOrFailed
    selector:
      matchLabels:
        batch.kubernetes.io/controller-uid: 51e431b0-336c-4b13-a974-2f8fae68aa95
    suspend: false
    template:
      metadata:
        creationTimestamp: null
        labels:
          batch.kubernetes.io/controller-uid: 51e431b0-336c-4b13-a974-2f8fae68aa95
          batch.kubernetes.io/job-name: rke2-machineconfig-cleanup-cronjob-29016005
          controller-uid: 51e431b0-336c-4b13-a974-2f8fae68aa95
          job-name: rke2-machineconfig-cleanup-cronjob-29016005
      spec:
        containers:
        - args:
          - /helper/cleanup.sh
          command:
          - /bin/sh
          env:
          - name: CATTLE_SERVER
            value: https://44.220.0.65.sslip.io
          - name: CATTLE_CA_CHECKSUM
          - name: CATTLE_TOKEN
            value: z8l87xnn8j6xwq4gkk6kb529xv5r9whb48vvqk7kvbpnm5znd2ktbc
          image: rancher/rancher-agent:v2.10.2
          imagePullPolicy: IfNotPresent
          name: rke2-machineconfig-cleanup-pod
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /helper
            name: config-volume
        dnsPolicy: ClusterFirst
        restartPolicy: Never
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: rke2-machineconfig-cleanup-sa
        serviceAccountName: rke2-machineconfig-cleanup-sa
        terminationGracePeriodSeconds: 30
        volumes:
        - configMap:
            defaultMode: 420
            name: rke2-machineconfig-cleanup-script
          name: config-volume
  status:
    completionTime: "2025-03-03T02:55:41Z"
    conditions:
    - lastProbeTime: "2025-03-03T02:55:41Z"
      lastTransitionTime: "2025-03-03T02:55:41Z"
      message: Reached expected number of succeeded pods
      reason: CompletionsReached
      status: "True"
      type: SuccessCriteriaMet
    - lastProbeTime: "2025-03-03T02:55:41Z"
      lastTransitionTime: "2025-03-03T02:55:41Z"
      message: Reached expected number of succeeded pods
      reason: CompletionsReached
      status: "True"
      type: Complete
    ready: 0
    startTime: "2025-03-03T02:55:27Z"
    succeeded: 1
    terminating: 0
    uncountedTerminatedPods: {}
- apiVersion: batch/v1
  kind: Job
  metadata:
    annotations:
      objectset.rio.cattle.io/applied: H4sIAAAAAAAA/6xW34/aOBD+V06W+nRJSPixhEh9yEL24MpCBLS602mFHDMBH44d2Q4tWvG/n+yk3WzZbrfSvdlm5svMfN/M8IhwST+BVFRwFKEMa3LonALkoCPlOxShP0WGHFSAxjusMYoeEeZcaKyp4MpcRfYvEK1Ae5IKj2CtGXhUdKjxPgArXCK4loIxkC45YKldCXuqtLQYyPkhgvjMQbr707EBav10CpzfPlC+ez8FVowN6E9xOC4ARUhLDDk9vslclZgYn2OVgavOSkOBLg4iEmzoG1qA0rgoUcQrxhzEcAbMFsWEa3NV3neR29c3xXHA6oAiRPx8NAQ8xN0B9HZAhsFoMCRD3Ov2A9wL8x4McgyD3ETW5GirTrnSmDH36UM/SslBNuUV5CCBE1Ao+uc7XVyVHzkoY4Icl8ZzAgwsl1GOmQIHPTH+7amRU5uvK0YqqxkIdwM/92/cAYQjt98Pu252Mxi6Wa8/6vvZIA/DHbo8XBykSiCm2hkmR5Hnc1pQjaLA930HaShKhjWY319R7ytECZ7T/bQmYT2Nu4Ob98ndzSAMwtE4DG7CoB/44+B2mHTD8e3ozvd7fjD0R73wdjSaBOPJMBmPkr5/G3a7w/9dNpdW+qbamHKQDXFybw6oEQBykOsq0K7SkvI9ctCeiQwzr2Z/AjmumF7VLXl+jx4cBPxkkRqCFvF9ghx0wqxq83Vxvll8Slbr2XLRflol6bJ9nybz++1kNfuUrFpYCogE3bYbT+PVZms+uU7jcfu7z7vwuUPL7KB1qaJO593jh4+3yWqRbJL1Nk5nl3cdZYgndS1Vp8nD7Q493+v6we9VaY5XQb+Q3CZe/ZH8UpTxx810m8br9Xa8SibJYjOL5+uWm+2StsNssU7GH1fJdv1hlm4387WJY3b392s+6TyeLbbTzSZ9zWqx3Kar5V9tJE+diOMRVikN0mOCYOYEvtfver7nd4Ibe+k1lzbWXTybmyDT5Xw2biNK+Cq/y4ODaIH39hVzcgDZOTJaliBdI/Lo5Hsjr+dmFWW7rt/tB74fosYnrRhLBaPkbEqSL4ROJSjgrfFhMJCDJChRSTu8Hk1vAKkk1eex4Bq+aNv5jInPqaQnymAPiSKY4edTC5c4o4xqalHQTorSNFI8nyMzbiTg3ZKz80oIfUcZNCxHWlZwcdBJsKqAe1FxXTdiYY4p1maAdA6igGd5d7wm8iaP9m+2xD9zJ5gc4Nq/fn4TgJ1wLyDU71cQuihbU7sory2+R7RqUC/YPd8AZoAZTi8PRixc7GANDIgW0tBgOkpy0KDsdlYoQozy6guylCiNpf4mkSW/w5RV0tTlBQnIisdqIbhhsObNmhFRlKkUOWV2XehzaSdYxTUtoBmQ9cQFeaIEYkJMNovWtn1aYrUMagVAUerzhMp6Ce1oVaAI3UMh5Lm1rq+Y/zW3J8J/0e+J5rc5fqX8ObnO1yEePTanpi71f73aym3tLbuwcrq/x6Xx4W3rRglP5i9pxDKhsa5sr1/+CwAA///6AkLrwwoAAA
      objectset.rio.cattle.io/id: helm-controller-chart-registration
      objectset.rio.cattle.io/owner-gvk: helm.cattle.io/v1, Kind=HelmChart
      objectset.rio.cattle.io/owner-name: traefik
      objectset.rio.cattle.io/owner-namespace: kube-system
    creationTimestamp: "2025-02-05T04:54:32Z"
    generation: 1
    labels:
      helmcharts.helm.cattle.io/chart: traefik
      objectset.rio.cattle.io/hash: c0f97ea7a25e3dec71957c7a3241a38f3e5fae5f
    name: helm-install-traefik
    namespace: kube-system
    ownerReferences:
    - apiVersion: helm.cattle.io/v1
      blockOwnerDeletion: false
      controller: false
      kind: HelmChart
      name: traefik
      uid: e8d50f06-5e89-4482-b657-b34940b5f88d
    resourceVersion: "649"
    uid: f75d2c0c-a946-47c2-be0b-8877e556c103
  spec:
    backoffLimit: 1000
    completionMode: NonIndexed
    completions: 1
    manualSelector: false
    parallelism: 1
    podReplacementPolicy: TerminatingOrFailed
    selector:
      matchLabels:
        batch.kubernetes.io/controller-uid: f75d2c0c-a946-47c2-be0b-8877e556c103
    suspend: false
    template:
      metadata:
        annotations:
          helmcharts.helm.cattle.io/configHash: SHA256=EF658189C81681410C1B7E28CB9F0030170938B99D1CD7EC9E40B82278BB67A8
        creationTimestamp: null
        labels:
          batch.kubernetes.io/controller-uid: f75d2c0c-a946-47c2-be0b-8877e556c103
          batch.kubernetes.io/job-name: helm-install-traefik
          controller-uid: f75d2c0c-a946-47c2-be0b-8877e556c103
          helmcharts.helm.cattle.io/chart: traefik
          job-name: helm-install-traefik
      spec:
        containers:
        - args:
          - install
          - --set-string
          - global.systemDefaultRegistry=
          env:
          - name: NAME
            value: traefik
          - name: VERSION
          - name: REPO
          - name: HELM_DRIVER
            value: secret
          - name: CHART_NAMESPACE
            value: kube-system
          - name: CHART
            value: https://%{KUBERNETES_API}%/static/charts/traefik-27.0.201+up27.0.2.tgz
          - name: HELM_VERSION
          - name: TARGET_NAMESPACE
            value: kube-system
          - name: AUTH_PASS_CREDENTIALS
            value: "false"
          - name: INSECURE_SKIP_TLS_VERIFY
            value: "false"
          - name: PLAIN_HTTP
            value: "false"
          - name: NO_PROXY
            value: .svc,.cluster.local,10.42.0.0/16,10.43.0.0/16
          - name: FAILURE_POLICY
            value: reinstall
          image: rancher/klipper-helm:v0.9.3-build20241008
          imagePullPolicy: IfNotPresent
          name: helm
          resources: {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /home/klipper-helm/.helm
            name: klipper-helm
          - mountPath: /home/klipper-helm/.cache
            name: klipper-cache
          - mountPath: /home/klipper-helm/.config
            name: klipper-config
          - mountPath: /tmp
            name: tmp
          - mountPath: /config
            name: values
          - mountPath: /chart
            name: content
        dnsPolicy: ClusterFirst
        nodeSelector:
          kubernetes.io/os: linux
        restartPolicy: OnFailure
        schedulerName: default-scheduler
        securityContext:
          runAsNonRoot: true
          seccompProfile:
            type: RuntimeDefault
        serviceAccount: helm-traefik
        serviceAccountName: helm-traefik
        terminationGracePeriodSeconds: 30
        volumes:
        - emptyDir:
            medium: Memory
          name: klipper-helm
        - emptyDir:
            medium: Memory
          name: klipper-cache
        - emptyDir:
            medium: Memory
          name: klipper-config
        - emptyDir:
            medium: Memory
          name: tmp
        - name: values
          secret:
            defaultMode: 420
            secretName: chart-values-traefik
        - configMap:
            defaultMode: 420
            name: chart-content-traefik
          name: content
  status:
    completionTime: "2025-02-05T04:54:54Z"
    conditions:
    - lastProbeTime: "2025-02-05T04:54:54Z"
      lastTransitionTime: "2025-02-05T04:54:54Z"
      message: Reached expected number of succeeded pods
      reason: CompletionsReached
      status: "True"
      type: SuccessCriteriaMet
    - lastProbeTime: "2025-02-05T04:54:54Z"
      lastTransitionTime: "2025-02-05T04:54:54Z"
      message: Reached expected number of succeeded pods
      reason: CompletionsReached
      status: "True"
      type: Complete
    ready: 0
    startTime: "2025-02-05T04:54:33Z"
    succeeded: 1
    terminating: 0
    uncountedTerminatedPods: {}
- apiVersion: batch/v1
  kind: Job
  metadata:
    annotations:
      objectset.rio.cattle.io/applied: H4sIAAAAAAAA/7RWUY/aOBD+KydLfbokJCEsm0h9yLLhlisLEdDqTqcVMs4EfDh2ZDu0aMV/P9mkbdjdbrcP90ac+b7MzPd5hkeEa/oJpKKCowRtsCa73iFADtpTXqAE/Sk2yEEVaFxgjVHyiDDnQmNNBVfmUWz+BaIVaE9S4RGsNQOPih416B2wyiWCaykYA+mSHZbalbClSkvLgZwfMojPHKS7Pexbos6rQ+D89oHy4v0dsGpkSH/Kw3EFKEFaYijp3iWyeBNE1ZgY3L7ZgKuOSkOFTg4iEmz6K1qB0riqUcIbxhzE8AaYbYxJ2darvCfZ29M357LDaocSFF2XZb8YkH4QDDf9YViSguCBHw/jQRmX4dAPcekP4tBk19Zqu0+50pgx9/JjPyrNQbb0BZQggRNQKPnniUeeSYEctGGC7OcGeQsMrK5JiZkCB31X/9tRa62udi+q01gPhRhHV6TYuNfXwcCNov6VG5Nh4ZbDMhxAMYxgAOj0cHKQqoGYzm8w2YuynNKKapQEvu87SENVM6zBvH/Fza+IJnhJt3dnMZZ3aTi4ep/1b/xRFIXx9XgUjIIoTsc342h0HcdX45s4jMJhmkVBFl1F8U3cj0ZpFA/iOLgZ/i8WOnVaYLqOKQfZCii35gdqzYAeHAT8YF+1nZ+l9xly0AGz5qkQJ+db1KdssZzMZ92jRZbPu8932fR+fbuYfMoWHT4FRILuxo3u0sVqbT67zNNR99uXV+0S0AnbaV2rpNd79/jh4022mGWrbLlO88npXU8ZRcm5SarXqcUNh57vhX7we1M/S/qF4lbp4o/sl7JMP67u1nm6XK5Hi+w2m60m6XTZgdkr0AVMZsts9HGRrZcfJvl6NV2aPCbjv1/D5NN0MlvfrVb5a1Gz+TpfzP/qMnnqQByPsEZpkB4TBDMn8L0o9HzP7wVX9qHfPnS5xulkapLM59PJqMso4aunTg8OohXe2lPMyQ5kb89oXYN0jXuTg+/FXt/dNJQVoR9Gge9foxaTN4zlglFyNC0pZ0LnEhTwzmwwHMhBEpRopJ1Mj8bwQBpJ9XEkuIYv2l5pxsTnXNIDZbCFTBHM8OVIwjXeUEY1tSyokKI2tyOdTpGZIxJwMefsuBBCjymDVuVEywZODjoI1lRwLxquz7erMj9zrM1k6O1EBRd197w287aO7jvb4p/BCSY7eI4/H7+JwI6uFxjO588odFV3RnJVP494ymjdoF6IuxzvZioZTU8PxixcFLAEBkQLaWQwN0py0KDsClYoQYzy5guykiiNpf5mkTkfY8oaafryggVkw1M1E9woeNbNhhFR1bkUJWV2D+hjbSdYwzWt4BZK3DB9HqMgD5RASoipZtZZp5cb6myFswugqvXxlsrzhiloU6EE3UMl5LGzk5+p/2uw76L/Iu671G8DfpX9UmDn6yBPHttfbW/Of+zOUe6TpWS3UUm397g2ON5FtI64hLzkF6uKxrqx9/70XwAAAP//uABTo7gKAAA
      objectset.rio.cattle.io/id: helm-controller-chart-registration
      objectset.rio.cattle.io/owner-gvk: helm.cattle.io/v1, Kind=HelmChart
      objectset.rio.cattle.io/owner-name: traefik-crd
      objectset.rio.cattle.io/owner-namespace: kube-system
    creationTimestamp: "2025-02-05T04:54:32Z"
    generation: 1
    labels:
      helmcharts.helm.cattle.io/chart: traefik-crd
      objectset.rio.cattle.io/hash: 48ff3d5c3117b372fcdca509795f9f2702af0592
    name: helm-install-traefik-crd
    namespace: kube-system
    ownerReferences:
    - apiVersion: helm.cattle.io/v1
      blockOwnerDeletion: false
      controller: false
      kind: HelmChart
      name: traefik-crd
      uid: 2aa46cdb-8815-4436-9c7d-f7f25ed74e5e
    resourceVersion: "632"
    uid: 627f7bc5-598d-43a7-bc02-d83fb5a0e297
  spec:
    backoffLimit: 1000
    completionMode: NonIndexed
    completions: 1
    manualSelector: false
    parallelism: 1
    podReplacementPolicy: TerminatingOrFailed
    selector:
      matchLabels:
        batch.kubernetes.io/controller-uid: 627f7bc5-598d-43a7-bc02-d83fb5a0e297
    suspend: false
    template:
      metadata:
        annotations:
          helmcharts.helm.cattle.io/configHash: SHA256=E3B0C44298FC1C149AFBF4C8996FB92427AE41E4649B934CA495991B7852B855
        creationTimestamp: null
        labels:
          batch.kubernetes.io/controller-uid: 627f7bc5-598d-43a7-bc02-d83fb5a0e297
          batch.kubernetes.io/job-name: helm-install-traefik-crd
          controller-uid: 627f7bc5-598d-43a7-bc02-d83fb5a0e297
          helmcharts.helm.cattle.io/chart: traefik-crd
          job-name: helm-install-traefik-crd
      spec:
        containers:
        - args:
          - install
          env:
          - name: NAME
            value: traefik-crd
          - name: VERSION
          - name: REPO
          - name: HELM_DRIVER
            value: secret
          - name: CHART_NAMESPACE
            value: kube-system
          - name: CHART
            value: https://%{KUBERNETES_API}%/static/charts/traefik-crd-27.0.201+up27.0.2.tgz
          - name: HELM_VERSION
          - name: TARGET_NAMESPACE
            value: kube-system
          - name: AUTH_PASS_CREDENTIALS
            value: "false"
          - name: INSECURE_SKIP_TLS_VERIFY
            value: "false"
          - name: PLAIN_HTTP
            value: "false"
          - name: NO_PROXY
            value: .svc,.cluster.local,10.42.0.0/16,10.43.0.0/16
          - name: FAILURE_POLICY
            value: reinstall
          image: rancher/klipper-helm:v0.9.3-build20241008
          imagePullPolicy: IfNotPresent
          name: helm
          resources: {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /home/klipper-helm/.helm
            name: klipper-helm
          - mountPath: /home/klipper-helm/.cache
            name: klipper-cache
          - mountPath: /home/klipper-helm/.config
            name: klipper-config
          - mountPath: /tmp
            name: tmp
          - mountPath: /config
            name: values
          - mountPath: /chart
            name: content
        dnsPolicy: ClusterFirst
        nodeSelector:
          kubernetes.io/os: linux
        restartPolicy: OnFailure
        schedulerName: default-scheduler
        securityContext:
          runAsNonRoot: true
          seccompProfile:
            type: RuntimeDefault
        serviceAccount: helm-traefik-crd
        serviceAccountName: helm-traefik-crd
        terminationGracePeriodSeconds: 30
        volumes:
        - emptyDir:
            medium: Memory
          name: klipper-helm
        - emptyDir:
            medium: Memory
          name: klipper-cache
        - emptyDir:
            medium: Memory
          name: klipper-config
        - emptyDir:
            medium: Memory
          name: tmp
        - name: values
          secret:
            defaultMode: 420
            secretName: chart-values-traefik-crd
        - configMap:
            defaultMode: 420
            name: chart-content-traefik-crd
          name: content
  status:
    completionTime: "2025-02-05T04:54:53Z"
    conditions:
    - lastProbeTime: "2025-02-05T04:54:53Z"
      lastTransitionTime: "2025-02-05T04:54:53Z"
      message: Reached expected number of succeeded pods
      reason: CompletionsReached
      status: "True"
      type: SuccessCriteriaMet
    - lastProbeTime: "2025-02-05T04:54:53Z"
      lastTransitionTime: "2025-02-05T04:54:53Z"
      message: Reached expected number of succeeded pods
      reason: CompletionsReached
      status: "True"
      type: Complete
    ready: 0
    startTime: "2025-02-05T04:54:33Z"
    succeeded: 1
    terminating: 0
    uncountedTerminatedPods: {}
kind: List
metadata:
  resourceVersion: ""
